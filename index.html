<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Sunder Ali Khowaja </title> <meta name="author" content="Sunder Ali Khowaja"> <meta name="description" content="Dr. Sunder Ali Khowaja (Ph.D.) is IEEE Senior Member and a distinguished researcher in the field of Computer Vision, Deep Learning, Privacy Preservation Machine Learning, and Model Inversion Attacks. "> <meta name="keywords" content="Khowaja Sunder, Sunder Ali Khowaja, Sander Ali Khowaja, dr.Sunder Ali Khowaja, doctor sunder khowaja, sunder ali, sander ali, dr sunder ali contact number, IEEE Senior Member, sunder ali usindh, sander ali usindh, deep-learning, computer-vision, AI-researcher, machine-learning, model inversion attacks, medical-imaging, private ai, neural-networks, academic-publications, research-portfolio"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img//favicon.png?5db4cf704bc2caffc10b1d53e8fa8ffd"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://sander-ali.github.io/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">About <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">Blogs </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">Teaching </a> </li> <li class="nav-item "> <a class="nav-link" href="/updates/">News </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/pic11111-480.webp 480w,/assets/img/pic11111-800.webp 800w,/assets/img/pic11111-1400.webp 1400w," type="image/webp" sizes="95vw"> <img src="/assets/img/pic11111.jpg" class="img-fluid z-depth-1 rounded" width="100%" height="auto" alt="pic11111.jpg" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <h1 class="post-title"> Sunder Ali Khowaja </h1> <p class="desc">Assistant Professor at Dublin City University, Dublin Ireland | IEEE Senior Member</p> <div class="social"> <div class="contact-icons"> <a href="mailto:%73%75%6E%64%65%72.%61%6C%69@%69%65%65%65.%6F%72%67" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="https://github.com/sander-ali" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-github"></i></a> <a href="https://ieeexplore.ieee.org/author//" title="IEEE Xplore" rel="external nofollow noopener" target="_blank"><i class="ai ai-ieee"></i></a> <a href="https://www.linkedin.com/in/sanderali" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-linkedin"></i></a> <a href="https://medium.com/@https://sandar-ali.medium.com/" title="Medium" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-medium"></i></a> <a href="https://orcid.org/0000-0002-4586-4131" title="ORCID" rel="external nofollow noopener" target="_blank"><i class="ai ai-orcid"></i></a> <a href="https://scholar.google.com/citations?user=JG9e8TsAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://www.dcu.ie/computing/people/sunder-ali-khowaja" title="Work" rel="external nofollow noopener" target="_blank"><i class="fa-solid fa-briefcase"></i></a> </div> <div class="contact-note">Best way to reach me is via email. </div> <br> </div> </header> <article> <div class="clearfix"> <p>I bring a rich blend of academic and industrial experience to the computer vision, deep learning, and privacy preservation domain. I am a senior member of IEEE along with an Associate Investigator at CONNECT Centre and Academic Collaborator at ADAPT Centre, Ireland. My educational journey began with a Ph.D. in Industrial and Information Systems Engineering from Hankuk University of Foreign Studiesin Yongin, Republic of Korea, where I focused on Ambient Intelligence and Affective Computing Methods using Machine Learning, Wearable Sensors, and Imaging techniques under the guidance of Prof. Seok-Lyong Lee. I further honed my skills with a Postdoctoral Research Fellowship at Tech University of Korea under the guidance of Prof. Ik Hyun Lee.</p> <p>I am currently associated with Dublin City University as an Assistant Professor. Formerly, I worked with Technological University Dublin (TU Dublin) as an Assistant Lecturer from Dec 2023 to Dec 2024. I have previously served as an Associate Professor, Assistant Professor, and Lecturer (from 2011 - 2023) at University of Sindh, Pakistan. I have the experience of working with multinational companies as a Network and RF Engineer from 2008 to 2011. I have published more than 70 research articles in national, international journals, and conference proceedings. I am the runner-up recipient for UG2+ Competition (Atmospheric Turbulence Mitigation) at CVPR2022 and second runner-up for UG2+ Competition (Target Coded Restoration Challenge) at CVPR 2023. Furthermore, I have achieved Top ten positions in several of the CVPR challenges from 2022 to 2024. I am currently serving as an Associate Editor for PLoS ONE and Consumer Electronics Letters. In addition, I am serving(ed) as a Guest Editor for special issues in IEEE Transactions on Vehicular Technology, IEEE Transactions on Green Communications and Networks, IEEE Open Journal of the Communication Society, IEEE Transactions on Consumer Electronics, Computers and Electrical Engineering, Human-centric Computing and Information Sciences, Journal of King Saud University: Computer and Information Sciences, and Sustainable Energy Technology and Assessments. I have also served as a TPC member for main tracks and workshops in A* Conferences, such as ECAI, Globecom, Mobicom, and ICDCS. I am a Regular Reviewer of notable journals, including IEEE Transactions, IEEE Journals, Elsevier Journals, and Springer Journals.</p> <p>üí° I am open to new opportunities and exploring exciting roles in my field of expertise. Please feel free to contact me to discuss potential collaborations.</p> </div> <div class="cv"> <a class="anchor" id="education"></a> <div class="card mt-3 p-3"> <h3 class="card-title font-weight-medium">Education</h3> <div> <ul class="card-text font-weight-light list-group list-group-flush"> <li class="list-group-item"> <div class="row"> <div class="col-xs-2 cl-sm-2 col-md-2 text-center date-column"> <table class="table-cv"> <tbody> <tr> <td> <span class="badge font-weight-bold danger-color-dark text-uppercase align-middle" style="min-width: 75px"> 2016.02 - 2019.02 </span> </td> </tr> <tr> <td> <p class="location"> <i class="fa-solid fa-location-dot iconlocation"></i> Yongin, South Korea </p> </td> </tr> </tbody> </table> </div> <div class="col-xs-10 cl-sm-10 col-md-10 mt-2 mt-md-0"> <h6 class="title font-weight-bold ml-1 ml-md-4"> <a href="https://www.hufs.ac.kr" rel="external nofollow noopener" target="_blank">Doctor of Philosophy</a> </h6> <h6 class="ml-1 ml-md-4" style="font-size: 0.95rem">Hankuk University of Foreign Studies</h6> <h6 class="ml-1 ml-md-4" style="font-size: 0.95rem; font-style: italic">Engineering</h6> <ul class="items"> <li> <span class="item">Sematic Image Networks for Human Action Recognition</span> </li> </ul> </div> </div> </li> <li class="list-group-item"> <div class="row"> <div class="col-xs-2 cl-sm-2 col-md-2 text-center date-column"> <table class="table-cv"> <tbody> <tr> <td> <span class="badge font-weight-bold danger-color-dark text-uppercase align-middle" style="min-width: 75px"> 2012.01 - 2014.11 </span> </td> </tr> <tr> <td> <p class="location"> <i class="fa-solid fa-location-dot iconlocation"></i> Jamshoro, Pakistan </p> </td> </tr> </tbody> </table> </div> <div class="col-xs-10 cl-sm-10 col-md-10 mt-2 mt-md-0"> <h6 class="title font-weight-bold ml-1 ml-md-4"> <a href="https://www.muet.edu.pk" rel="external nofollow noopener" target="_blank">Master in Engineering</a> </h6> <h6 class="ml-1 ml-md-4" style="font-size: 0.95rem">Mehran University of Engineering and Technology</h6> <h6 class="ml-1 ml-md-4" style="font-size: 0.95rem; font-style: italic">Communication Systems and Networks</h6> <ul class="items"> </ul> </div> </div> </li> <li class="list-group-item"> <div class="row"> <div class="col-xs-2 cl-sm-2 col-md-2 text-center date-column"> <table class="table-cv"> <tbody> <tr> <td> <span class="badge font-weight-bold danger-color-dark text-uppercase align-middle" style="min-width: 75px"> 2004.01 - 2007.12 </span> </td> </tr> <tr> <td> <p class="location"> <i class="fa-solid fa-location-dot iconlocation"></i> Jamshoro, Pakistan </p> </td> </tr> </tbody> </table> </div> <div class="col-xs-10 cl-sm-10 col-md-10 mt-2 mt-md-0"> <h6 class="title font-weight-bold ml-1 ml-md-4"> <a href="https://www.muet.edu.pk" rel="external nofollow noopener" target="_blank">Bachelor in Engineering</a> </h6> <h6 class="ml-1 ml-md-4" style="font-size: 0.95rem">Mehran University of Engineering and Technology</h6> <h6 class="ml-1 ml-md-4" style="font-size: 0.95rem; font-style: italic">Telecommunication Engineering</h6> <ul class="items"> </ul> </div> </div> </li> </ul> </div> </div> <a class="anchor" id="work"></a> <div class="card mt-3 p-3"> <h3 class="card-title font-weight-medium">Work</h3> <div> <ul class="card-text font-weight-light list-group list-group-flush"> <li class="list-group-item"> <div class="row"> <div class="col-xs-2 cl-sm-2 col-md-2 text-center date-column"> <table class="table-cv"> <tbody> <tr> <td> <span class="badge font-weight-bold danger-color-dark text-uppercase align-middle" style="min-width: 75px"> 2024.12 - Present </span> </td> </tr> <tr> <td> <p class="location"> <i class="fa-solid fa-location-dot iconlocation"></i> Dublin, Ireland </p> </td> </tr> </tbody> </table> </div> <div class="col-xs-10 cl-sm-10 col-md-10 mt-2 mt-md-0"> <h6 class="title font-weight-bold ml-1 ml-md-4"> <a href="https://www.dcu.ie" rel="external nofollow noopener" target="_blank">Assistant Professor</a> </h6> <h6 class="ml-1 ml-md-4" style="font-size: 0.95rem">Dublin City University </h6> <h6 class="ml-1 ml-md-4" style="font-size: 0.95rem; font-style: italic">Assistant Professor in School of Computing</h6> <ul class="items"> </ul> </div> </div> </li> <li class="list-group-item"> <div class="row"> <div class="col-xs-2 cl-sm-2 col-md-2 text-center date-column"> <table class="table-cv"> <tbody> <tr> <td> <span class="badge font-weight-bold danger-color-dark text-uppercase align-middle" style="min-width: 75px"> 2023.12 - 2024.12 </span> </td> </tr> <tr> <td> <p class="location"> <i class="fa-solid fa-location-dot iconlocation"></i> Dublin, Ireland </p> </td> </tr> </tbody> </table> </div> <div class="col-xs-10 cl-sm-10 col-md-10 mt-2 mt-md-0"> <h6 class="title font-weight-bold ml-1 ml-md-4"> <a href="https://www.tudublin.ie/" rel="external nofollow noopener" target="_blank">Assistant Lecturer</a> </h6> <h6 class="ml-1 ml-md-4" style="font-size: 0.95rem">Technological University Dublin</h6> <h6 class="ml-1 ml-md-4" style="font-size: 0.95rem; font-style: italic">Teaching Subjects such as Object Oriented Programming, Machine Learning, and Python for Data Science, while Conducting cutting-edge research on deep-learning models and Generative AI for privacy preservation machine learning and trustworthy AI</h6> <ul class="items"> </ul> </div> </div> </li> <li class="list-group-item"> <div class="row"> <div class="col-xs-2 cl-sm-2 col-md-2 text-center date-column"> <table class="table-cv"> <tbody> <tr> <td> <span class="badge font-weight-bold danger-color-dark text-uppercase align-middle" style="min-width: 75px"> 2021.10 - 2022.03 </span> </td> </tr> <tr> <td> <p class="location"> <i class="fa-solid fa-location-dot iconlocation"></i> Siheung, South Korea </p> </td> </tr> </tbody> </table> </div> <div class="col-xs-10 cl-sm-10 col-md-10 mt-2 mt-md-0"> <h6 class="title font-weight-bold ml-1 ml-md-4"> <a href="http://www.tukorea.ac.kr/" rel="external nofollow noopener" target="_blank">Postdoctoral Research Fellow</a> </h6> <h6 class="ml-1 ml-md-4" style="font-size: 0.95rem">Tech University of Korea</h6> <h6 class="ml-1 ml-md-4" style="font-size: 0.95rem; font-style: italic">Developed advanced deep learning techniques for Image Enhancement and Applications revolving around Thermal Images.</h6> <ul class="items"> </ul> </div> </div> </li> <li class="list-group-item"> <div class="row"> <div class="col-xs-2 cl-sm-2 col-md-2 text-center date-column"> <table class="table-cv"> <tbody> <tr> <td> <span class="badge font-weight-bold danger-color-dark text-uppercase align-middle" style="min-width: 75px"> 2018.03 - 2023.11 </span> </td> </tr> <tr> <td> <p class="location"> <i class="fa-solid fa-location-dot iconlocation"></i> Jamshoro, Pakistan </p> </td> </tr> </tbody> </table> </div> <div class="col-xs-10 cl-sm-10 col-md-10 mt-2 mt-md-0"> <h6 class="title font-weight-bold ml-1 ml-md-4"> <a href="http://www.usindh.edu.pk/" rel="external nofollow noopener" target="_blank">Associate Professor and Assistant Professor</a> </h6> <h6 class="ml-1 ml-md-4" style="font-size: 0.95rem">University of Sindh</h6> <h6 class="ml-1 ml-md-4" style="font-size: 0.95rem; font-style: italic">Teaching Subjects such as Data Science, Machine Learning, Computer Vision, and Artificial Intelligence. Applied deep learning and computer vision techniques for image enhancement and privacy preservation machine learning.</h6> <ul class="items"> </ul> </div> </div> </li> <li class="list-group-item"> <div class="row"> <div class="col-xs-2 cl-sm-2 col-md-2 text-center date-column"> <table class="table-cv"> <tbody> <tr> <td> <span class="badge font-weight-bold danger-color-dark text-uppercase align-middle" style="min-width: 75px"> 2011.07 - 2018.03 </span> </td> </tr> <tr> <td> <p class="location"> <i class="fa-solid fa-location-dot iconlocation"></i> Jamshoro, Pakistan </p> </td> </tr> </tbody> </table> </div> <div class="col-xs-10 cl-sm-10 col-md-10 mt-2 mt-md-0"> <h6 class="title font-weight-bold ml-1 ml-md-4"> <a href="http://www.usindh.edu.pk/" rel="external nofollow noopener" target="_blank">Lecturer</a> </h6> <h6 class="ml-1 ml-md-4" style="font-size: 0.95rem">University of Sindh</h6> <h6 class="ml-1 ml-md-4" style="font-size: 0.95rem; font-style: italic">Teaching subjects such as Digital Signal Processing, Computer Vision, and Artificial Intelligence. Conducting research on Action Recognition using computer vision and deep learning techniques.</h6> <ul class="items"> </ul> </div> </div> </li> <li class="list-group-item"> <div class="row"> <div class="col-xs-2 cl-sm-2 col-md-2 text-center date-column"> <table class="table-cv"> <tbody> <tr> <td> <span class="badge font-weight-bold danger-color-dark text-uppercase align-middle" style="min-width: 75px"> 2010.12 - 2011.07 </span> </td> </tr> <tr> <td> <p class="location"> <i class="fa-solid fa-location-dot iconlocation"></i> Karachi, Pakistan </p> </td> </tr> </tbody> </table> </div> <div class="col-xs-10 cl-sm-10 col-md-10 mt-2 mt-md-0"> <h6 class="title font-weight-bold ml-1 ml-md-4"> <a href="www.newhorizonit.net">Network Engineer - Projects</a> </h6> <h6 class="ml-1 ml-md-4" style="font-size: 0.95rem">New Horizon IT Ltd.</h6> <h6 class="ml-1 ml-md-4" style="font-size: 0.95rem; font-style: italic">Installation and Troubleshooting of Network Equipments including Routers, Switches, and Firewalls</h6> <ul class="items"> </ul> </div> </div> </li> <li class="list-group-item"> <div class="row"> <div class="col-xs-2 cl-sm-2 col-md-2 text-center date-column"> <table class="table-cv"> <tbody> <tr> <td> <span class="badge font-weight-bold danger-color-dark text-uppercase align-middle" style="min-width: 75px"> 2008.05 - 2010.09 </span> </td> </tr> <tr> <td> <p class="location"> <i class="fa-solid fa-location-dot iconlocation"></i> Karachi, Pakistan </p> </td> </tr> </tbody> </table> </div> <div class="col-xs-10 cl-sm-10 col-md-10 mt-2 mt-md-0"> <h6 class="title font-weight-bold ml-1 ml-md-4"> <a href="www.comstar.com.pk">System Support Engineer</a> </h6> <h6 class="ml-1 ml-md-4" style="font-size: 0.95rem">Comstar ISA Ltd</h6> <h6 class="ml-1 ml-md-4" style="font-size: 0.95rem; font-style: italic">Installation and Troubleshooting of RF, Network, and Satellite Equipments including Routers, Switches, Firewalls, Line-of-Sight and IDirect Equipment</h6> <ul class="items"> </ul> </div> </div> </li> </ul> </div> </div> </div> <h2> <a href="/publications/" style="color: inherit">Selected Publications</a> </h2> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/BELA_2025-480.webp 480w,/assets/img/publication_preview/BELA_2025-800.webp 800w,/assets/img/publication_preview/BELA_2025-1400.webp 1400w," type="image/webp" sizes="200px"> <img src="/assets/img/publication_preview/BELA_2025.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="BELA_2025.jpg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="BELA2025" class="col-sm-8"> <div class="title">Block Encryption LAyer (BELA): Zero-Trust Defense Against Model Inversion Attacks for Federated Learning in 5G/6G Systems</div> <div class="author"> <em>Sunder Ali Khowaja</em>,¬†Parus Khuwaja,¬†Kapal Dev, and <span class="more-authors" title="click to view 4 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '4 more authors' ? 'Keshav Singh, Xingwang Li, Bartzoudis Nikolaos, Ciprian R Comsa' : '4 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">4 more authors</span> </div> <div class="periodical"> <em>IEEE Open Journal of the Communications Society</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1109/OJCOMS.2025.3526768" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/abstract/document/10829858" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-doi="10.1109/OJCOMS.2025.3526768" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=JG9e8TsAAAAJ&amp;citation_for_view=JG9e8TsAAAAJ:u9iWguZQMMsC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-1-4285F4?logo=googlescholar&amp;labelColor=beige" alt="1 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>Federated Learning (FL) paradigm has been very popular in the implementation of 5G and beyond communication systems as it provides necessary security for the users in terms of data. However, the FL paradigm is still vulnerable to model inversion attacks, which allow malicious attackers to reconstruct data by using the trained model gradients. Such attacks can be carried out using generative adversarial networks (GANs), generative models, or by backtracking the model gradients. A zero-trust mechanism involves securing access and interactions with model gradients under the principle of ‚Äúnever trust, always verify.‚Äù This proactive approach ensures that sensitive information, such as model gradients, is kept private, making it difficult for adversaries to infer the private details of the users. This paper proposes a zero-trust based Block Encryption LAyer (BELA) module that provides defense against the model inversion attacks in FL settings. The BELA module mimics the Batch normalization (BN) layer in the deep neural network architecture that considers the random sequence. The sequence and the parameters are private to each client, which helps in providing defense against the model inversion attacks. We also provide extensive theoretical analysis to show that the proposed module is integratable in a variety of deep neural network architectures. Our experimental analysis on four publicly available datasets and various network architectures show that the BELA module can increase the mean square error (MSE) up to 194% when a reconstruction attempt is performed by an adversary using existing state-of-the-art methods</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">BELA2025</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Block Encryption LAyer (BELA): Zero-Trust Defense Against Model Inversion Attacks for Federated Learning in 5G/6G Systems}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Khowaja, Sunder Ali and Khuwaja, Parus and Dev, Kapal and Singh, Keshav and Li, Xingwang and Nikolaos, Bartzoudis and Comsa, Ciprian R}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Open Journal of the Communications Society}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{6}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{807-819}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/OJCOMS.2025.3526768}</span><span class="p">,</span>
  <span class="na">dimensions</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/turbulence_2025-480.webp 480w,/assets/img/publication_preview/turbulence_2025-800.webp 800w,/assets/img/publication_preview/turbulence_2025-1400.webp 1400w," type="image/webp" sizes="200px"> <img src="/assets/img/publication_preview/turbulence_2025.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="turbulence_2025.jpg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="turb2025" class="col-sm-8"> <div class="title">Image Registration and Deep NeuroFuzzy Networks for Mitigating Atmospheric Turbulence Effects in Consumer-Based Optical Imaging</div> <div class="author"> <em>Sunder Ali Khowaja</em>,¬†Usman Ali,¬†Kapal Dev, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Ik Hyun Lee' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>IEEE Transactions on Consumer Electronics</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1109/TCE.2025.3528544" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/abstract/document/10838578" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-doi="10.1109/TCE.2025.3528544" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=JG9e8TsAAAAJ&amp;citation_for_view=JG9e8TsAAAAJ:XiSMed-E-HIC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-0-4285F4?logo=googlescholar&amp;labelColor=beige" alt="0 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>Consumer-based optical imaging systems are characterized as big data processing systems, which are drastically affected by atmospheric turbulences that add geometric distortions and blur effect to the images when used in outdoor condition. Physics-grounded simulators have been proposed recently to generate synthetic data but the generalization to the real-world turbulent images is not so good. In this paper, we combine the characteristics of image registration, deep neurofuzzy methods, and channel-attention based discriminative learning strategy to propose image registration, neurofuzzy based denoising, and deblurring network (RND2Net). The RND2Net is designed on a principle that it does not require turbulent image pairs (ground truth images) to train the network, which closely resembles the real-world situation used as consumer devices. The registration module focuses on the region-based fusion techniques while the denoising and deblurring module incorporates deep neurofuzzy network along with dense residual blocks and channel attention mechanism to train the network. The RND2Net is also designed to reduce the noise and blur effect from images, while generalizing on the down-stream tasks, such as text recognition. Experimental results show that the RND2Net yields better performance quantitatively as qualitatively on synthetic and real-world datasets in comparison to existing state-of-the-art methods</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">turb2025</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Image Registration and Deep NeuroFuzzy Networks for Mitigating Atmospheric Turbulence Effects in Consumer-Based Optical Imaging}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Khowaja, Sunder Ali and Ali, Usman and Dev, Kapal and Lee, Ik Hyun}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Transactions on Consumer Electronics}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/TCE.2025.3528544}</span><span class="p">,</span>
  <span class="na">dimensions</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/RSMA_NCAA_2024-480.webp 480w,/assets/img/publication_preview/RSMA_NCAA_2024-800.webp 800w,/assets/img/publication_preview/RSMA_NCAA_2024-1400.webp 1400w," type="image/webp" sizes="200px"> <img src="/assets/img/publication_preview/RSMA_NCAA_2024.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="RSMA_NCAA_2024.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="RSMA2024" class="col-sm-8"> <div class="title">RSMA-assisted SHAPTINs: secrecy performance under imperfect hardware and channel estimation errors</div> <div class="author"> Zhou Feng,¬†Kefeng Guo,¬†Cheng Jian, and <span class="more-authors" title="click to view 4 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '4 more authors' ? 'Sunder Ali Khowaja, Kapal Dev, Thippa Reddy Gadekallu, Hussam Al Hamadi' : '4 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">4 more authors</span> </div> <div class="periodical"> <em>Neural Computing and Applications</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1007/s00521-024-10526-2" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://link.springer.com/article/10.1007/s00521-024-10526-2" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-doi="10.1007/s00521-024-10526-2" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=JG9e8TsAAAAJ&amp;citation_for_view=JG9e8TsAAAAJ:p2g8aNsByqUC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-0-4285F4?logo=googlescholar&amp;labelColor=beige" alt="0 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>Satellite high aerial platform terrestrial integrated networks have become the hot topic these years, which have been regarded as the major part of the intelligence of things for future networks. During this work, we investigate the secrecy performance for rate-splitting multiple access-assisted satellite high aerial platform terrestrial integrated networks. Besides, imperfect hardware and channel estimation errors are considered in the secrecy networks. Moreover, to enhance the energy utilization efficiency, rate-splitting multiple access scheme is utilized into the considered network, which is prior to that of the non-orthogonal multiple access scheme. What‚Äôs more, to enhance the satellite transmission, multiple high aerial platforms are used to forward the transmission along with multiple eavesdroppers. In addition, the direct transmission link is not considered in the secrecy networks due to the heavy fading and obstacles. Relied on the former considerations, the exact and asymptotic analysis for the secrecy performance is further obtained to confirm the rightness of the analysis. Finally, some representative Monte Carlo simulations are carried out to validate the obtained results.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">RSMA2024</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{RSMA-assisted SHAPTINs: secrecy performance under imperfect hardware and channel estimation errors}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Feng, Zhou and Guo, Kefeng and Jian, Cheng and Khowaja, Sunder Ali and Dev, Kapal and Gadekallu, Thippa Reddy and Hamadi, Hussam Al}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Neural Computing and Applications}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Springer}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1007/s00521-024-10526-2}</span><span class="p">,</span>
  <span class="na">dimensions</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/SelfFed_ESWA_2024-480.webp 480w,/assets/img/publication_preview/SelfFed_ESWA_2024-800.webp 800w,/assets/img/publication_preview/SelfFed_ESWA_2024-1400.webp 1400w," type="image/webp" sizes="200px"> <img src="/assets/img/publication_preview/SelfFed_ESWA_2024.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="SelfFed_ESWA_2024.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="selffed2024" class="col-sm-8"> <div class="title">SelfFed: Self-supervised federated learning for data heterogeneity and label scarcity in medical images</div> <div class="author"> <em>Sunder Ali Khowaja</em>,¬†Kapal Dev,¬†Syed Muhammad Anwar, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Marius George Linguraru' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>Expert Systems With Applications</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1016/j.eswa.2024.125493" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.sciencedirect.com/science/article/pii/S0957417424023601" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://www.sciencedirect.com/science/article/pii/S0957417424023601" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-doi="10.1016/j.eswa.2024.125493" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=JG9e8TsAAAAJ&amp;citation_for_view=JG9e8TsAAAAJ:u_35RYKgDlwC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-7-4285F4?logo=googlescholar&amp;labelColor=beige" alt="7 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>Self-supervised learning in the federated learning paradigm has been gaining a lot of interest both in industry and research due to the collaborative learning capability on unlabeled yet isolated data. However, self-supervised based federated learning strategies suffer from performance degradation due to label scarcity and diverse data distributions, i.e., data heterogeneity. In this paper, we propose the SelfFed framework for medical images to overcome data heterogeneity and label scarcity issues. The first phase of the SelfFed framework helps to overcome the data heterogeneity issue by leveraging the pre-training paradigm that performs augmentative modeling using Swin Transformer-based encoder in a decentralized manner. The label scarcity issue is addressed by fine-tuning paradigm that introduces a contrastive network and a novel aggregation strategy. We perform our experimental analysis on publicly available medical imaging datasets to show that SelfFed performs better when compared to existing baselines and works. Our method achieves a maximum improvement of 8.8% and 4.1% on Retina and COVID-FL datasets on non-IID datasets. Further, our proposed method outperforms existing baselines even when trained on a few (10%) labeled instances.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">selffed2024</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{SelfFed: Self-supervised federated learning for data heterogeneity and label scarcity in medical images}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Khowaja, Sunder Ali and Dev, Kapal and Anwar, Syed Muhammad and Linguraru, Marius George}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Expert Systems With Applications}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{261}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{125493}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Elsevier}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1016/j.eswa.2024.125493}</span><span class="p">,</span>
  <span class="na">dimensions</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/EACNet_TFS_2024-480.webp 480w,/assets/img/publication_preview/EACNet_TFS_2024-800.webp 800w,/assets/img/publication_preview/EACNet_TFS_2024-1400.webp 1400w," type="image/webp" sizes="200px"> <img src="/assets/img/publication_preview/EACNet_TFS_2024.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="EACNet_TFS_2024.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="EACNet2024" class="col-sm-8"> <div class="title">Depression Detection From Social Media Posts Using Emotion Aware Encoders and Fuzzy Based Contrastive Networks</div> <div class="author"> <em>Sunder Ali Khowaja</em>,¬†Lewis Nkenyereye,¬†Parus Khuwaja, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Hussam Al Hamadi, Kapal Dev' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>IEEE Transactions on Fuzzy Systems</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1109/TFUZZ.2024.3461776" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/document/10681286" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://ieeexplore.ieee.org/document/10681286" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-doi="10.1109/TFUZZ.2024.3461776" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=JG9e8TsAAAAJ&amp;citation_for_view=JG9e8TsAAAAJ:SP6oXDckpogC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-3-4285F4?logo=googlescholar&amp;labelColor=beige" alt="3 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>Post COVID-19 and recent advancement in terms of language models, researchers have shown a lot of interest in analyzing social media posts for analyzing mental state of the users. Social media platforms are the epitome of sharing individual thoughts and feelings through textual posts and linguistic cues. Therefore, the textual modality from social media posts can be leveraged for detecting early signs of stress, depression or other mental health conditions, accordingly. Existing methods mainly focus on the feature engineering, shallow learning, and employing of deep learning architectures to improve the mental state recognition performance. Seldom the study uses an established knowledge-base that is available to model mentalization and emotional aspect to improving the depression and stress recognition. In this regard, we propose emotion aware contrastive networks (EAC-net) that leverages the existing knowledge-base and propose some new ones to model the emotional and mentalization aspect in order to improve the recognition of stress and depression state from textual posts. Furthermore, we propose a feature-level fusion and weighting mechanism using GRUs and self-attention layers to weight and select the important features. Lastly, the EAC-Net uses a supervised contrastive learning strategy to train the network. The proposed method is evaluated on four publicly available datasets. Experimental results reveal that the EAC-Net achieves state-of-the-art results by outperforming baselines and existing methods by atleast 1.86%, 0.72%, 3.43%, and 3.64% on four publicly available datasets using F1-measure as the evaluation metric.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">EACNet2024</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Depression Detection From Social Media Posts Using Emotion Aware Encoders and Fuzzy Based Contrastive Networks}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Khowaja, Sunder Ali and Nkenyereye, Lewis and Khuwaja, Parus and Hamadi, Hussam Al and Dev, Kapal}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Transactions on Fuzzy Systems}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{Early Access}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1-11}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/TFUZZ.2024.3461776}</span><span class="p">,</span>
  <span class="na">dimensions</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/BICE_CVPRW_2024-480.webp 480w,/assets/img/publication_preview/BICE_CVPRW_2024-800.webp 800w,/assets/img/publication_preview/BICE_CVPRW_2024-1400.webp 1400w," type="image/webp" sizes="200px"> <img src="/assets/img/publication_preview/BICE_CVPRW_2024.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="BICE_CVPRW_2024.jpg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="NTIRE2024" class="col-sm-8"> <div class="title">NTIRE 2024 Challenge on Blind Enhancement of Compressed Image: Methods and Results</div> <div class="author"> Ren Yang,¬†Radu Timofte,¬†Bingchen Li, and <span class="more-authors" title="click to view 52 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '52 more authors' ? 'Xin Li, Mengxi Guo, Shijie Zhao, Li Zhang, Zhibo Chen, Dongyang Zhang, Yash Arora, Aditya Arora, Yuanbin Chen, Hui Tang, Tao Wang, Longxuan Zhao, Bin Chen, Tong Tong, Qiao Mo, Jingwei Bao, Jinhua Hao, Yukang Ding, Hantang Li, Ming Sun, Chao Zhou, Shuyuan Zhu, Zhi Jin, Wei Wang, Dandan Zhan, Jiawei Wu, Jiahao Wu, Luwei Tu, Hongyu An, Xinfeng Zhang, Woon-Ha Yeo, Wang-Taek Oh, Young-Il Kim, Han-Cheol Ryu, Long Sun, Mingjun Zhen, Jinshan Pan, Jiangxin Dong, Jinhui Tang, Yapeng Du, Ao Li, Ziyang He, Lei Luo, Ce Zhu, Xin Yao, Sunder Ali Khowaja, IK Hyun Lee, Jaeho Lee, Seongwan Kim, Sharif S M A, Nodirkhuja Khujaev, Roman Tsoy' : '52 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">52 more authors</span> </div> <div class="periodical"> <em>In 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1109/CVPRW63382.2024.00652" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/document/10678543" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://openaccess.thecvf.com/content/CVPR2024W/NTIRE/papers/Yang_NTIRE_2024_Challenge_on_Blind_Enhancement_of_Compressed_Image_Methods_CVPRW_2024_paper.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/sander-ali/JPEG_Compression_Enhancement_RDAB" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-doi="10.1109/CVPRW63382.2024.00652" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=JG9e8TsAAAAJ&amp;citation_for_view=JG9e8TsAAAAJ:CHSYGLWDkRkC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-19-4285F4?logo=googlescholar&amp;labelColor=beige" alt="19 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>This paper reviews the Challenge on Blind Enhancement of Compressed Image at NTIRE 2024, which aims at enhancing the quality of JPEG images which are compressed with unknown quality factor. The challenge requires that the total size of codes and pre-trained model(s) cannot exceed 300 MB, since we encourage solutions for blind enhancement with generalized models, instead of separately training several models for each quality factor. In this report, we summarize the detailed settings of the challenge, the final results, and the solutions proposed by the participants. The challenge has 129 registered participants and received 13 valid submissions. Several teams (including all TOP 3 teams) have publicly released the codes (see Sec. 4). They gauge the state-of-the-art of blind quality enhancement of compressed image.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">NTIRE2024</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{NTIRE 2024 Challenge on Blind Enhancement of Compressed Image: Methods and Results}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Yang, Ren and Timofte, Radu and Li, Bingchen and Li, Xin and Guo, Mengxi and Zhao, Shijie and Zhang, Li and Chen, Zhibo and Zhang, Dongyang and Arora, Yash and Arora, Aditya and Chen, Yuanbin and Tang, Hui and Wang, Tao and Zhao, Longxuan and Chen, Bin and Tong, Tong and Mo, Qiao and Bao, Jingwei and Hao, Jinhua and Ding, Yukang and Li, Hantang and Sun, Ming and Zhou, Chao and Zhu, Shuyuan and Jin, Zhi and Wang, Wei and Zhan, Dandan and Wu, Jiawei and Wu, Jiahao and Tu, Luwei and An, Hongyu and Zhang, Xinfeng and Yeo, Woon-Ha and Oh, Wang-Taek and Kim, Young-Il and Ryu, Han-Cheol and Sun, Long and Zhen, Mingjun and Pan, Jinshan and Dong, Jiangxin and Tang, Jinhui and Du, Yapeng and Li, Ao and He, Ziyang and Luo, Lei and Zhu, Ce and Yao, Xin and Khowaja, Sunder Ali and Lee, IK Hyun and Lee, Jaeho and Kim, Seongwan and A, Sharif S M and Khujaev, Nodirkhuja and Tsoy, Roman}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{6524-6535}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/CVPRW63382.2024.00652}</span><span class="p">,</span>
  <span class="na">dimensions</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/ZETA_WCNC_2024-480.webp 480w,/assets/img/publication_preview/ZETA_WCNC_2024-800.webp 800w,/assets/img/publication_preview/ZETA_WCNC_2024-1400.webp 1400w," type="image/webp" sizes="200px"> <img src="/assets/img/publication_preview/ZETA_WCNC_2024.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="ZETA_WCNC_2024.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="ZETA2024" class="col-sm-8"> <div class="title">ZETA: ZEro-Trust Attack Framework with Split Learning for Autonomous Vehicles in 6G Networks</div> <div class="author"> <em>Sunder Ali Khowaja</em>,¬†Parus Khuwaja,¬†Kapal Dev, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Keshav Singh, Lewis Nkenyereye, Dan Kilper' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> <em>In 2024 IEEE Wireless Communications and Networking Conference (WCNC)</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1109/WCNC57260.2024.10571158" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/document/10571158" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://ieeexplore.ieee.org/document/10571158" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-doi="10.1109/WCNC57260.2024.10571158" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=JG9e8TsAAAAJ&amp;citation_for_view=JG9e8TsAAAAJ:NhqRSupF_l8C" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-3-4285F4?logo=googlescholar&amp;labelColor=beige" alt="3 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>In past, due to data and model security concerns, modern communication systems mainly focus on the use of edge computing devices for enabling immersive applications and services. Federated learning is one of the preferred solutions but it stresses the computation capability of the edge devices for immersive applications. Much research is now focusing on split learning as an alternative due to its ability of performing joint training with limited computing resources. However, split learning is also vulnerable to data reconstruction, feature space hijacking, and model inversion attacks, which are quite common concerning immersive applications such as Metaverse. In this regard, we propose a ZEro-Trust Attack (ZETA) framework for data reconstruction and model inversion attacks for autonomous vehicles opting for split learning strategies. We propose the joint training of client, server, and shadow models for both the reconstruction and main task to fool existing methods. Our experimental results demonstrate that the proposed method is capable of reconstructing client‚Äôs data with an error of 0.0032. This study is proposed as a basis to design more sophisticated defense mechanisms for autonomous vehicles to protect user services in 5G/6G networks.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">ZETA2024</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{ZETA: ZEro-Trust Attack Framework with Split Learning for Autonomous Vehicles in 6G Networks}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Khowaja, Sunder Ali and Khuwaja, Parus and Dev, Kapal and Singh, Keshav and Nkenyereye, Lewis and Kilper, Dan}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2024 IEEE Wireless Communications and Networking Conference (WCNC)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1-6}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/WCNC57260.2024.10571158}</span><span class="p">,</span>
  <span class="na">dimensions</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/DASTAN_Globecom_2023-480.webp 480w,/assets/img/publication_preview/DASTAN_Globecom_2023-800.webp 800w,/assets/img/publication_preview/DASTAN_Globecom_2023-1400.webp 1400w," type="image/webp" sizes="200px"> <img src="/assets/img/publication_preview/DASTAN_Globecom_2023.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="DASTAN_Globecom_2023.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="DASTAN" class="col-sm-8"> <div class="title">DASTAN-CNN: RF Fingerprinting for the Mitigation of Membership Inference Attacks in 5G</div> <div class="author"> <em>Sunder Ali Khowaja</em>,¬†Parus Khuwaja,¬†Kapal Dev, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Angelos Antonopoulos, Maurizio Magarini' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>In GLOBECOM 2023 - 2023 IEEE Global Communications Conference</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1109/GLOBECOM54140.2023.10437263" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/document/10437263" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://ieeexplore.ieee.org/document/10437263" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-doi="10.1109/GLOBECOM54140.2023.10437263" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=JG9e8TsAAAAJ&amp;citation_for_view=JG9e8TsAAAAJ:cFHS6HbyZ2cC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-3-4285F4?logo=googlescholar&amp;labelColor=beige" alt="3 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>The fifth generation (5G) networks are designed to support a large range of diverse services with strict performance requirements. Studies suggest that, 5G uses machine learning technologies for variety of tasks ranging from network management, and resource optimization to automated services. The successful integration of 5G with machine learning has also led to the basis for 6G networks. However, the use of machine learning makes the 5G networks susceptible to adversarial attacks. A few works study the effect of differential privacy and adversarial attacks in the 5G systems let alone to provide the proposal of effective defense mechanism. This study proposes Denoising and Adversarial attack-based STacked AutoeNcoder (DASTAN) convolutional neural networks (CNN) to provide defense against a specific differential privacy attack, i.e. membership inference, optimized to detect the device or data distribution potentially used in the training process. DASTAN initiates an intentional attack to camouflage the characteristics of an authorized user from an adversary and uses a de noising stacked autoencoder to recover the information at service provider‚Äôs end for RF fingerprinting. The aim of RF fingerprinting is to validate the authenticity and identity of the device to preserve the privacy of wireless network. Experimental results demonstrate the efficacy of DASTAN-CNN, which reduces the attack success rate by up to 52.69% in comparison to the case where no defense strategy is employed. The DASTAN-CNN also achieves 75.29% authorized user recognition rate for RF fingerprinting while reducing the attack success rate to 39.23%, which shows the effectiveness in terms of trade-off efficiency.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">DASTAN</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{DASTAN-CNN: RF Fingerprinting for the Mitigation of Membership Inference Attacks in 5G}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Khowaja, Sunder Ali and Khuwaja, Parus and Dev, Kapal and Antonopoulos, Angelos and Magarini, Maurizio}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{GLOBECOM 2023 - 2023 IEEE Global Communications Conference}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{5524-5529}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/GLOBECOM54140.2023.10437263}</span><span class="p">,</span>
  <span class="na">dimensions</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/NTIREx4_CVPRW_2023-480.webp 480w,/assets/img/publication_preview/NTIREx4_CVPRW_2023-800.webp 800w,/assets/img/publication_preview/NTIREx4_CVPRW_2023-1400.webp 1400w," type="image/webp" sizes="200px"> <img src="/assets/img/publication_preview/NTIREx4_CVPRW_2023.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="NTIREx4_CVPRW_2023.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="NTIREx4" class="col-sm-8"> <div class="title">NTIRE 2023 Challenge on Image Super-Resolution (√ó4): Methods and Results</div> <div class="author"> Yulun Zhang,¬†Kai Zhang,¬†Zheng Chen, and <span class="more-authors" title="click to view 75 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '75 more authors' ? 'Yawei Li, Radu Timofte, Junpei Zhang, Kexin Zhang, Rui Peng, Yanbiao Ma, Licheng Jia, Huaibo Huang, Xiaoqiang Zhou, Yuang Ai, Ran He, Yajun Qiu, Qiang Zhu, Pengfei Li, Qianhui Li, Shuyuan Zhu, Dafeng Zhang, Jia Li, Fan Wang, Chunmiao Li, TaeHyung Kim, Jungkeong Kil, Eon Kim, Yeonseung Yu, Beomyeol Lee, Subin Lee, Seokjae Lim, Somi Chae, Heungjun Choi, ZhiKai Huang, YiChung Chen, YuanChun Chiang, HaoHsiang Yang, WeiTing Chen, HuaEn Chang, I-Hsiang Chen, ChiaHsuan Hsieh, SyYen Kuo, Ui-Jin Choi, Marcos V. Conde, Sunder Ali Khowaja, Jiseok Yoon, Ik Hyun Lee, Garas Gendy, Nabil Sabor, Jingchao Hou, Guanghui He, Zhao Zhang, Baiang Li, Huan Zheng, Suiyi Zhao, Yangcheng Gao, Yanyan Wei, Jiahuan Ren, Jiayu Wei, Yanfeng Li, Jia Sun, Zhanyi Cheng, Zhiyuan Li, Xu Yao, Xinyi Wang, Danxu Li, Xuan Cui, Jun Cao, Cheng Li, Jianbin Zheng, Anjali Sarvaiya, Kalpesh Prajapati, Ratnadeep Patra, Pragnesh Barik, Chaitanya Rathod, Kishor Upla, Kiran Raja, Raghavendra Ramachandra, Christoph Busch' : '75 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">75 more authors</span> </div> <div class="periodical"> <em>In 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1109/CVPRW59228.2023.00185" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/abstract/document/10208449" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/papers/Zhang_NTIRE_2023_Challenge_on_Image_Super-Resolution_x4_Methods_and_Results_CVPRW_2023_paper.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/sander-ali/NTIRE_Team01_SAKSRNet" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-doi="10.1109/CVPRW59228.2023.00185" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=JG9e8TsAAAAJ&amp;citation_for_view=JG9e8TsAAAAJ:P5F9QuxV20EC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-54-4285F4?logo=googlescholar&amp;labelColor=beige" alt="54 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>This paper reviews the NTIRE 2023 challenge on image super-resolution (√ó4), focusing on the proposed solutions and results. The task of image super-resolution (SR) is to generate a high-resolution (HR) output from a corresponding low-resolution (LR) input by leveraging prior information from paired LR-HR images. The aim of the challenge is to obtain a network design/solution capable to produce high-quality results with the best performance (e.g., PSNR). We want to explore how high performance we can achieve regardless of computational cost (e.g., model size and FLOPs) and data. The track of the challenge was to measure the restored HR images with the ground truth HR images on DIV2K testing dataset. The ranking of the teams is determined directly by the PSNR value. The challenge has attracted 192 registered participants, where 15 teams made valid submissions. They achieve state-of-the-art performance in single image super-resolution.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">NTIREx4</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{NTIRE 2023 Challenge on Image Super-Resolution (√ó4): Methods and Results}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhang, Yulun and Zhang, Kai and Chen, Zheng and Li, Yawei and Timofte, Radu and Zhang, Junpei and Zhang, Kexin and Peng, Rui and Ma, Yanbiao and Jia, Licheng and Huang, Huaibo and Zhou, Xiaoqiang and Ai, Yuang and He, Ran and Qiu, Yajun and Zhu, Qiang and Li, Pengfei and Li, Qianhui and Zhu, Shuyuan and Zhang, Dafeng and Li, Jia and Wang, Fan and Li, Chunmiao and Kim, TaeHyung and Kil, Jungkeong and Kim, Eon and Yu, Yeonseung and Lee, Beomyeol and Lee, Subin and Lim, Seokjae and Chae, Somi and Choi, Heungjun and Huang, ZhiKai and Chen, YiChung and Chiang, YuanChun and Yang, HaoHsiang and Chen, WeiTing and Chang, HuaEn and Chen, I-Hsiang and Hsieh, ChiaHsuan and Kuo, SyYen and Choi, Ui-Jin and Conde, Marcos V. and Khowaja, Sunder Ali and Yoon, Jiseok and Lee, Ik Hyun and Gendy, Garas and Sabor, Nabil and Hou, Jingchao and He, Guanghui and Zhang, Zhao and Li, Baiang and Zheng, Huan and Zhao, Suiyi and Gao, Yangcheng and Wei, Yanyan and Ren, Jiahuan and Wei, Jiayu and Li, Yanfeng and Sun, Jia and Cheng, Zhanyi and Li, Zhiyuan and Yao, Xu and Wang, Xinyi and Li, Danxu and Cui, Xuan and Cao, Jun and Li, Cheng and Zheng, Jianbin and Sarvaiya, Anjali and Prajapati, Kalpesh and Patra, Ratnadeep and Barik, Pragnesh and Rathod, Chaitanya and Upla, Kishor and Raja, Kiran and Ramachandra, Raghavendra and Busch, Christoph}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1865-1884}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/CVPRW59228.2023.00185}</span><span class="p">,</span>
  <span class="na">dimensions</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/NTIREID_CVPRW_2023-480.webp 480w,/assets/img/publication_preview/NTIREID_CVPRW_2023-800.webp 800w,/assets/img/publication_preview/NTIREID_CVPRW_2023-1400.webp 1400w," type="image/webp" sizes="200px"> <img src="/assets/img/publication_preview/NTIREID_CVPRW_2023.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="NTIREID_CVPRW_2023.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="NTIREdenoise" class="col-sm-8"> <div class="title">NTIRE 2023 Challenge on Image Denoising: Methods and Results</div> <div class="author"> Yawei Li,¬†Yulun Zhang,¬†Radu Timofte, and <span class="more-authors" title="click to view 78 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '78 more authors' ? 'Luc Van Gool, Zhijun Tu, Kunpeng Du, Hailing Wang, Hanting Chen, Wei Li, Xiaofei Wang, Jie Hu, Yunhe Wang, Xiangyu Kong, Jinlong Wu, Dafeng Zhang, Jianxing Zhang, Shuai Liu, Furui Bai, Chaoyu Feng, Hao Wang, Yuqian Zhang, Guangqi Shao, Xiaotao Wang, Lei Lei, Rongjian Xu, Zhilu Zhang, Yunjin Chen, Dongwei Ren, Wangmeng Zuo, Qi Wu, Mingyan Han, Shen Cheng, Haipeng Li, Ting Jiang, Chengzhi Jiang, Xinpeng Li, Jinting Luo, Wenjie Lin, Lei Yu, Haoqiang Fan, Shuaicheng Liu, Aditya Arora, Syed Waqas Zamir, Javier Vazquez-Corral, Konstantinos G. Derpanis, Michael S. Brown, Hao Li, Zhihao Zhao, Jinshan Pan, Jiangxin Dong, Jinhui Tang, Bo Yang, Jingxiang Chen, Chenghua Li, Xi Zhang, Zhao Zhang, Jiahuan Ren, Zhicheng Ji, Kang Miao, Suiyi Zhao, Huan Zheng, YanYan Wei, Kangliang Liu, Xiangcheng Du, Sijie Liu, Yingbin Zheng, Xingjiao Wu, Cheng Jin, Rajeev Irny, Sriharsha Koundinya, Vighnesh Kamath, Gaurav Khandelwal, Sunder Ali Khowaja, Jiseok Yoon, Ik Hyun Lee, Shijie Chen, Chengqiang Zhao, Huabin Yang, Zhongjian Zhang, Junjia Huang, Yanru Zhang' : '78 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">78 more authors</span> </div> <div class="periodical"> <em>In 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1109/CVPRW59228.2023.00188" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/document/10208539" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/papers/Li_NTIRE_2023_Challenge_on_Image_Denoising_Methods_and_Results_CVPRW_2023_paper.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://huggingface.co/spaces/sunder-ali/Image_Denoising_Demo" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-doi="10.1109/CVPRW59228.2023.00188" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=JG9e8TsAAAAJ&amp;citation_for_view=JG9e8TsAAAAJ:SeFeTyx0c_EC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-53-4285F4?logo=googlescholar&amp;labelColor=beige" alt="53 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>This paper reviews the NTIRE 2023 challenge on image denoising (œÉ = 50) with a focus on the proposed solutions and results. The aim is to obtain a network design capable to produce high-quality results with the best performance measured by PSNR for image denoising. Independent additive white Gaussian noise (AWGN) is assumed and the noise level is 50. The challenge had 225 registered participants, and 16 teams made valid submissions. They gauge the state-of-the-art for image denoising.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">NTIREdenoise</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{NTIRE 2023 Challenge on Image Denoising: Methods and Results}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Li, Yawei and Zhang, Yulun and Timofte, Radu and Gool, Luc Van and Tu, Zhijun and Du, Kunpeng and Wang, Hailing and Chen, Hanting and Li, Wei and Wang, Xiaofei and Hu, Jie and Wang, Yunhe and Kong, Xiangyu and Wu, Jinlong and Zhang, Dafeng and Zhang, Jianxing and Liu, Shuai and Bai, Furui and Feng, Chaoyu and Wang, Hao and Zhang, Yuqian and Shao, Guangqi and Wang, Xiaotao and Lei, Lei and Xu, Rongjian and Zhang, Zhilu and Chen, Yunjin and Ren, Dongwei and Zuo, Wangmeng and Wu, Qi and Han, Mingyan and Cheng, Shen and Li, Haipeng and Jiang, Ting and Jiang, Chengzhi and Li, Xinpeng and Luo, Jinting and Lin, Wenjie and Yu, Lei and Fan, Haoqiang and Liu, Shuaicheng and Arora, Aditya and Zamir, Syed Waqas and Vazquez-Corral, Javier and Derpanis, Konstantinos G. and Brown, Michael S. and Li, Hao and Zhao, Zhihao and Pan, Jinshan and Dong, Jiangxin and Tang, Jinhui and Yang, Bo and Chen, Jingxiang and Li, Chenghua and Zhang, Xi and Zhang, Zhao and Ren, Jiahuan and Ji, Zhicheng and Miao, Kang and Zhao, Suiyi and Zheng, Huan and Wei, YanYan and Liu, Kangliang and Du, Xiangcheng and Liu, Sijie and Zheng, Yingbin and Wu, Xingjiao and Jin, Cheng and Irny, Rajeev and Koundinya, Sriharsha and Kamath, Vighnesh and Khandelwal, Gaurav and Khowaja, Sunder Ali and Yoon, Jiseok and Lee, Ik Hyun and Chen, Shijie and Zhao, Chengqiang and Yang, Huabin and Zhang, Zhongjian and Huang, Junjia and Zhang, Yanru}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1905-1921}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/CVPRW59228.2023.00188}</span><span class="p">,</span>
  <span class="na">dimensions</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/SPIN_ICC_2023-480.webp 480w,/assets/img/publication_preview/SPIN_ICC_2023-800.webp 800w,/assets/img/publication_preview/SPIN_ICC_2023-1400.webp 1400w," type="image/webp" sizes="200px"> <img src="/assets/img/publication_preview/SPIN_ICC_2023.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="SPIN_ICC_2023.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="SPIN" class="col-sm-8"> <div class="title">SPIN: Simulated Poisoning and Inversion Network for Federated Learning-Based 6G Vehicular Networks</div> <div class="author"> <em>Sunder Ali Khowaja</em>,¬†Parus Khuwaja,¬†Kapal Dev, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Angelos Antonopoulos' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>In ICC 2023 - IEEE International Conference on Communications</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1109/ICC45041.2023.10279339" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/abstract/document/10279339" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://ieeexplore.ieee.org/abstract/document/10279339" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-doi="10.1109/ICC45041.2023.10279339" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=JG9e8TsAAAAJ&amp;citation_for_view=JG9e8TsAAAAJ:ldfaerwXgEUC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-18-4285F4?logo=googlescholar&amp;labelColor=beige" alt="18 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>The applications concerning vehicular networks benefit from the vision of beyond 5G and 6G technologies such as ultra-dense network topologies, low latency, and high data rates. Vehicular networks have always faced data privacy preservation concerns, which lead to the advent of distributed learning techniques such as federated learning. Although federated learning has solved data privacy preservation issues to some extent, the technique is quite vulnerable to model inversion and model poisoning attacks. We assume that the design of defense mechanism and attacks are two sides of the same coin. Designing a method to reduce vulnerability requires the attack to be effective and challenging with real-world implications. In this work, we propose simulated poisoning and inversion network (SPIN) that leverages the optimization approach for reconstructing data from a differential model trained by a vehicular node and intercepted when transmitted to roadside unit (RSU). We then train a generative adversarial network (GAN) to improve the generation of data with each passing round and global update from the RSU, accordingly. Evaluation results show the qualitative and quantitative effectiveness of the proposed approach. The attack initiated by SPIN can reduce up to 22% accuracy on publicly available datasets while just using a single attacker. We assume that revealing the simulation of such attacks would help us find its defense mechanism in an effective manner.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">SPIN</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{SPIN: Simulated Poisoning and Inversion Network for Federated Learning-Based 6G Vehicular Networks}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Khowaja, Sunder Ali and Khuwaja, Parus and Dev, Kapal and Antonopoulos, Angelos}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ICC 2023 - IEEE International Conference on Communications}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{6205-6210}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/ICC45041.2023.10279339}</span><span class="p">,</span>
  <span class="na">dimensions</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/VSR_NTIRE_2022-480.webp 480w,/assets/img/publication_preview/VSR_NTIRE_2022-800.webp 800w,/assets/img/publication_preview/VSR_NTIRE_2022-1400.webp 1400w," type="image/webp" sizes="200px"> <img src="/assets/img/publication_preview/VSR_NTIRE_2022.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="VSR_NTIRE_2022.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="NTIRE2022" class="col-sm-8"> <div class="title">NTIRE 2022 Challenge on Super-Resolution and Quality Enhancement of Compressed Video: Dataset, Methods and Results</div> <div class="author"> Ren Yang,¬†Radu Timofte,¬†Meisong Zheng, and <span class="more-authors" title="click to view 76 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '76 more authors' ? 'Qunliang Xing, Minglang Qiao, Mai Xu, Lai Jiang, Huaida Liu, Ying Chen, Youcheng Ben, Xiao Zhou, Chen Fu, Pei Cheng, Gang Yu, Junyi Li, Renlong Wu, Zhilu Zhang, Wei Shang, Zhengyao Lv, Yunjin Chen, Mingcai Zhou, Dongwei Ren, Kai Zhang, Wangmeng Zuo, Pavel Ostyakov, Vyal Dmitry, Shakarim Soltanayev, Chervontsev Sergey, Zhussip Magauiya, Xueyi Zou, Youliang Yan, Pablo Navarrete Michelini, Yunhua Lu, Diankai Zhang, Shaoli Liu, Si Gao, Biao Wu, Chengjian Zheng, Xiaofeng Zhang, Kaidi Lu, Ning Wang, Thuong Nguyen Canh, Thong Bach, Qing Wang, Xiaopeng Sun, Haoyu Ma, Shijie Zhao, Junlin Li, Liangbin Xie, Shuwei Shi, Yujiu Yang, Xintao Wang, Jinjin Gu, Chao Dong, Xiaodi Shi, Chunmei Nian, Dong Jiang, Jucai Lin, Zhihuai Xie, Mao Ye, Dengyan Luo, Liuhan Peng, Shengjie Chen, Xin Liu, Qian Wang, Xin Liu, Boyang Liang, Hang Dong, Yuhao Huang, Kai Chen, Xingbei Guo, Yujing Sun, Huilei Wu, Pengxu Wei, Yulin Huang, Junying Chen, Ik Hyun Lee, Sunder Ali Khowaja, Jiseok Yoon' : '76 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">76 more authors</span> </div> <div class="periodical"> <em>In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1109/CVPRW56347.2022.00129" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/document/9857103" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://openaccess.thecvf.com/content/CVPR2022W/NTIRE/papers/Yang_NTIRE_2022_Challenge_on_Super-Resolution_and_Quality_Enhancement_of_Compressed_CVPRW_2022_paper.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/sander-ali/NTIRE_Team01_SAKSRNet" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-doi="10.1109/CVPRW56347.2022.00129" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=JG9e8TsAAAAJ&amp;citation_for_view=JG9e8TsAAAAJ:ns9cj8rnVeAC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-38-4285F4?logo=googlescholar&amp;labelColor=beige" alt="38 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>This paper reviews the NTIRE 2022 Challenge on Super-Resolution and Quality Enhancement of Compressed Video. In this challenge, we proposed the LDV 2.0 dataset, which includes the LDV dataset (240 videos) and 95 additional videos. This challenge includes three tracks. Track 1 aims at enhancing the videos compressed by HEVC at a fixed QP. Track 2 and Track 3 target both the super-resolution and quality enhancement of HEVC compressed video. They require x2 and x4 super-resolution, respectively. The three tracks totally attract more than 600 registrations. In the test phase, 8 teams, 8 teams and 12 teams submitted the final results to Tracks 1, 2 and 3, respectively. The proposed methods and solutions gauge the state-of-the-art of super-resolution and quality enhancement of compressed video. The proposed LDV 2.0 dataset is available at https://github.com/RenYang-home/LDV_dataset. The homepage of this challenge (including open-sourced codes) is at https://github.com/RenYang-home/NTIRE22_VEnh_SR.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">NTIRE2022</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{NTIRE 2022 Challenge on Super-Resolution and Quality Enhancement of Compressed Video: Dataset, Methods and Results}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Yang, Ren and Timofte, Radu and Zheng, Meisong and Xing, Qunliang and Qiao, Minglang and Xu, Mai and Jiang, Lai and Liu, Huaida and Chen, Ying and Ben, Youcheng and Zhou, Xiao and Fu, Chen and Cheng, Pei and Yu, Gang and Li, Junyi and Wu, Renlong and Zhang, Zhilu and Shang, Wei and Lv, Zhengyao and Chen, Yunjin and Zhou, Mingcai and Ren, Dongwei and Zhang, Kai and Zuo, Wangmeng and Ostyakov, Pavel and Dmitry, Vyal and Soltanayev, Shakarim and Sergey, Chervontsev and Magauiya, Zhussip and Zou, Xueyi and Yan, Youliang and Michelini, Pablo Navarrete and Lu, Yunhua and Zhang, Diankai and Liu, Shaoli and Gao, Si and Wu, Biao and Zheng, Chengjian and Zhang, Xiaofeng and Lu, Kaidi and Wang, Ning and Canh, Thuong Nguyen and Bach, Thong and Wang, Qing and Sun, Xiaopeng and Ma, Haoyu and Zhao, Shijie and Li, Junlin and Xie, Liangbin and Shi, Shuwei and Yang, Yujiu and Wang, Xintao and Gu, Jinjin and Dong, Chao and Shi, Xiaodi and Nian, Chunmei and Jiang, Dong and Lin, Jucai and Xie, Zhihuai and Ye, Mao and Luo, Dengyan and Peng, Liuhan and Chen, Shengjie and Liu, Xin and Wang, Qian and Liu, Xin and Liang, Boyang and Dong, Hang and Huang, Yuhao and Chen, Kai and Guo, Xingbei and Sun, Yujing and Wu, Huilei and Wei, Pengxu and Huang, Yulin and Chen, Junying and Hyun Lee, Ik and Ali Khowaja, Sunder and Yoon, Jiseok}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1220-1237}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/CVPRW56347.2022.00129}</span><span class="p">,</span>
  <span class="na">dimensions</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/Facelock_ICCE_2020-480.webp 480w,/assets/img/publication_preview/Facelock_ICCE_2020-800.webp 800w,/assets/img/publication_preview/Facelock_ICCE_2020-1400.webp 1400w," type="image/webp" sizes="200px"> <img src="/assets/img/publication_preview/Facelock_ICCE_2020.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="Facelock_ICCE_2020.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="Facelook" class="col-sm-8"> <div class="title">Face Recognition for Smart Door Lock System using Hierarchical Network</div> <div class="author"> Muhammad Waseem,¬†<em>Sunder Ali Khowaja</em>,¬†Ramesh Kumar Ayyasamy, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Farhan Bashir' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>In 2020 International Conference on Computational Intelligence (ICCI)</em>, 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1109/ICCI51257.2020.9247836" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/document/9247836" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://ieeexplore.ieee.org/document/9247836" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-doi="10.1109/ICCI51257.2020.9247836" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=JG9e8TsAAAAJ&amp;citation_for_view=JG9e8TsAAAAJ:-f6ydRqryjwC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-31-4285F4?logo=googlescholar&amp;labelColor=beige" alt="31 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>Face recognition system is broadly used for human identification because of its capacity to measure the facial points and recognize the identity in an unobtrusive way. The application of face recognition systems can be applied to surveillance at home, workplaces, and campuses, accordingly. The problem with existing face recognition systems is that they either rely on the facial key points and landmarks or the face embeddings from FaceNet for the recognition process. In this paper, we propose a hierarchical network (HN) framework which uses pre-trained architecture for recognizing faces followed by the validation from face embeddings using FaceNet. We also designed a real-time face recognition security door lock system connected with raspberry pi as an implication of the proposed method. The evaluation of the proposed work has been conducted on the dataset collected from 12 students from Faculty of Engineering and Technology, University of Sindh. The experimental results show that the proposed method achieves better results over existing works. We also carried out a comparison on random faces acquired from the Internet to perform face recognition and results shows that the proposed HN framework is resilient to the randomly acquired faces.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Facelook</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Face Recognition for Smart Door Lock System using Hierarchical Network}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Waseem, Muhammad and Khowaja, Sunder Ali and Ayyasamy, Ramesh Kumar and Bashir, Farhan}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2020 International Conference on Computational Intelligence (ICCI)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{51-56}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/ICCI51257.2020.9247836}</span><span class="p">,</span>
  <span class="na">dimensions</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/COMPSAC_2018-480.webp 480w,/assets/img/publication_preview/COMPSAC_2018-800.webp 800w,/assets/img/publication_preview/COMPSAC_2018-1400.webp 1400w," type="image/webp" sizes="200px"> <img src="/assets/img/publication_preview/COMPSAC_2018.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="COMPSAC_2018.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="COMPSAC" class="col-sm-8"> <div class="title">A Framework for Real Time Emotion Recognition Based on Human ANS Using Pervasive Device</div> <div class="author"> Feri Setiawan,¬†<em>Sunder Ali Khowaja</em>,¬†Aria Ghora Prabono, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Bernardo Nugroho Yahya, Seok-Lyong Lee' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>In 2018 IEEE 42nd Annual Computer Software and Applications Conference (COMPSAC)</em>, 2018 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1109/COMPSAC.2018.00129" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/document/8377754" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://ieeexplore.ieee.org/document/8377754" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-doi="10.1109/COMPSAC.2018.00129" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=JG9e8TsAAAAJ&amp;citation_for_view=JG9e8TsAAAAJ:3fE2CSJIrl8C" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-18-4285F4?logo=googlescholar&amp;labelColor=beige" alt="18 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>The concept of connected things by involving emotional aspects has been raised as a new research issue which is known as "emotional IoT". The deeper interaction between object and human shows an importance to develop a system with either cognitive or affective capabilities such as emotion. While the existing works on real time emotion recognition mostly rely on facial data, there are a few works dealing with real time emotion recognition based on physiological data using pervasive devices. In this work, we propose a framework to recognize emotion based on human physiological signals using the pervasive wearable device. This framework opposed most of the works which employed sensors which are expensive and complex in arrangement. The challenge on using pervasive devices is the low accuracy due to the low sampling rate. The approach is implemented in an end-to-end soft real time emotion recognition system using smartphone and smartwatch devices. The performance of our system was evaluated under a common environment and proved the system applicability throughout everyday life.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">COMPSAC</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A Framework for Real Time Emotion Recognition Based on Human ANS Using Pervasive Device}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Setiawan, Feri and Khowaja, Sunder Ali and Prabono, Aria Ghora and Yahya, Bernardo Nugroho and Lee, Seok-Lyong}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2018 IEEE 42nd Annual Computer Software and Applications Conference (COMPSAC)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{805-806}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/COMPSAC.2018.00129}</span><span class="p">,</span>
  <span class="na">dimensions</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/CCODE_2017-480.webp 480w,/assets/img/publication_preview/CCODE_2017-800.webp 800w,/assets/img/publication_preview/CCODE_2017-1400.webp 1400w," type="image/webp" sizes="200px"> <img src="/assets/img/publication_preview/CCODE_2017.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="CCODE_2017.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="CCODE" class="col-sm-8"> <div class="title">Energy efficient mobile user tracking system with node activation mechanism using wireless sensor networks</div> <div class="author"> Rizwan Ali Shah,¬†<em>Sunder Ali Khowaja</em>,¬†Bhawani Shankar Chowdhary, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Sonia Shah' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>In 2017 International Conference on Communication, Computing and Digital Systems (C-CODE)</em>, 2017 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1109/C-CODE.2017.7918906" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/document/7918906" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://ieeexplore.ieee.org/document/7918906" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-doi="10.1109/C-CODE.2017.7918906" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=JG9e8TsAAAAJ&amp;citation_for_view=JG9e8TsAAAAJ:_FxGoFyzp5QC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-3-4285F4?logo=googlescholar&amp;labelColor=beige" alt="3 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>User localization in wireless sensor networks have been given a great attention in recent times and also considered to be one of the promising applications. Many approaches for the same have been proposed based on range based and range free mechanisms for localizing the user. Similarly, tracking the user in a sensor field has also been given equal importance in association with localization. Techniques employing both these methods are mostly based on static anchor nodes or scheduling system which compromises on the lifetime of wireless sensor network. Considering the constraint of network lifetime this paper proposes localizing and tracking method with an activation scheme for tracking the mobile node efficiently while increasing the network lifetime. The system has been tested on two scenarios suggesting that the proposed method can provide the flexibility to the system which could be adjusted with reference to the user requirements. The first scenario suggests that all the nodes are activated only when the mobile user enters but as the user is localized all the nodes will get deactivated except the concerned nodes. The second scenario suggests that a predefined deployment strategy is provided with only 10% of activated nodes. The experimental results show that the proposed system achieves a better trade-off in terms of accuracy and computational complexity for single mobile node tracking.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">CCODE</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Energy efficient mobile user tracking system with node activation mechanism using wireless sensor networks}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Shah, Rizwan Ali and Khowaja, Sunder Ali and Chowdhary, Bhawani Shankar and Shah, Sonia}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2017 International Conference on Communication, Computing and Digital Systems (C-CODE)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{80-85}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2017}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/C-CODE.2017.7918906}</span><span class="p">,</span>
  <span class="na">dimensions</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/ICET_2016-480.webp 480w,/assets/img/publication_preview/ICET_2016-800.webp 800w,/assets/img/publication_preview/ICET_2016-1400.webp 1400w," type="image/webp" sizes="200px"> <img src="/assets/img/publication_preview/ICET_2016.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="ICET_2016.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="ICET" class="col-sm-8"> <div class="title">Facial expression recognition using two-tier classification and its application to smart home automation system</div> <div class="author"> <em>Sunder Ali Khowaja</em>,¬†Kamran Dahri,¬†Muhammad Aslam Kumbhar, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Altaf Mazhar Soomro' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>In 2015 International Conference on Emerging Technologies (ICET)</em>, 2016 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1109/ICET.2015.7389223" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/document/7389223" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://ieeexplore.ieee.org/document/7389223" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-doi="10.1109/ICET.2015.7389223" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=JG9e8TsAAAAJ&amp;citation_for_view=JG9e8TsAAAAJ:W7OEmFMy1HYC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-28-4285F4?logo=googlescholar&amp;labelColor=beige" alt="28 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>With the convergence of smart technologies and advancement in electronic equipment the concept of smart home system swiftly escalates. The idea is to automate the home appliances according to the user requirements without human intervention. After a long tiring day and heavy workloads user will not be in a state of taking out its mobile phone and pressing the buttons for controlling home appliances. Several methods have been proposed in the design of such systems using sensors, biometrics and face detection. This paper proposes a method for detecting human emotions by taking into account the complete facial analysis, suggesting that the emotions can accurately be determined by analyzing eyes, nose and lips separately hence covering a wide range of emotions. The classification is carried out by acquiring the image of user followed by the face detection and segmenting the region of interests (ROI) i.e. eyes, nose and lips for further analysis of emotions. Principle Component Analysis (PCA) along with feature extraction techniques and Support Vector Machines (SVMs) are used for classification of emotion for the said automation system. Policies have been implemented in Java to simulate the home automation environment for testing and validation. At the instant this system has been tested on a single user with 4 basic emotions i.e. sad, anger, happiness and neutral, but this study can be a basis to develop an automated system with variety of emotions for multiple users.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">ICET</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Facial expression recognition using two-tier classification and its application to smart home automation system}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Khowaja, Sunder Ali and Dahri, Kamran and Kumbhar, Muhammad Aslam and Soomro, Altaf Mazhar}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2015 International Conference on Emerging Technologies (ICET)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1-6}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2016}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/ICET.2015.7389223}</span><span class="p">,</span>
  <span class="na">dimensions</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0 text-left"> ¬© Copyright 2025 Sunder Ali Khowaja. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=UA-82147914-4"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","UA-82147914-4");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script>for(var wechatModal=document.getElementById("WeChatMod"),wechatBtn=document.querySelectorAll('[id="WeChatBtn"]'),i=0;i<wechatBtn.length;i++)wechatBtn[i].onclick=function(){wechatModal.style.display="block"};window.onclick=function(t){t.target==wechatModal&&(wechatModal.style.display="none")};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"About",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-cv",title:"CV",description:"Below is a simplified version of my resume. You can find a full version in the pdf. \ud83d\udc49",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"nav-publications",title:"Publications",description:"",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-blogs",title:"Blogs",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-teaching",title:"Teaching",description:"Courses and teaching activities at various institutions",section:"Navigation",handler:()=>{window.location.href="/teaching/"}},{id:"nav-news",title:"News",description:"",section:"Navigation",handler:()=>{window.location.href="/updates/"}},{id:"post-what-is-the-technical-debt-of-large-language-models-llms-and-how-does-it-affect-us",title:"What Is the Technical Debt of Large Language Models (LLMs) and How Does...",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/What-Is-the-Technical-Debt-of-Large-Language-Models-(LLMs)-and-How-Does-It-Affect-Us/"}},{id:"post-open-source-ai-taking-forward-leaps",title:"Open Source AI taking forward leaps",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/Open-Source-AI-taking-forward-leaps/"}},{id:"post-summarizing-youtube-videos-using-python-and-online-ai-tools",title:"Summarizing YouTube Videos using Python and Online AI Tools",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2023/Summarizing-YouTube-Videos-using-Python-and-Online-AI-Tools/"}},{id:"post-taking-a-deep-dive-into-yolov8",title:"Taking a Deep Dive into YOLOv8",description:"Details on YOLOv8",section:"Posts",handler:()=>{window.location.href="/blog/2023/Taking-a-Deep-Dive-into-YOLOv8/"}},{id:"post-brief-timeline-of-yolo-models",title:"Brief Timeline of YOLO models",description:"Brief Timeline of YOLO models from v1 to v8",section:"Posts",handler:()=>{window.location.href="/blog/2023/Brief-Timeline-of-YOLO-models-from-v1-to-v8/"}},{id:"post-combating-urban-flooding-with-internet-of-things-and-artificial-intelligence",title:"Combating Urban flooding with Internet of Things and Artificial Intelligence",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2020/Combating-Urban-flooding-with-Internet-of-Things-and-Artificial-Intelligence/"}},{id:"post-where-do-pakistan-stand-in-ai-race",title:"Where do Pakistan stand in AI race?",description:"A discussion on where Pakistan resides in terms of AI race.",section:"Posts",handler:()=>{window.location.href="/blog/2020/Where-do-Pakistan-stand-in-AI-race/"}},{id:"projects-best-paper-award-at-kdbc-2018",title:"Best Paper Award at KDBC 2018",description:"Our paper titled Human Action Recognition with Sequential Convolution and Recurrent Neural Networks Using 3D Skeleton Data, has won the best paper award in Korean Database Conference held at South Korea.",section:"Projects",handler:()=>{window.location.href="/projects/Best_Paper_Award_KDBC/"}},{id:"projects-first-runner-up-at-cvpr-2023-6th-ug2-challenge",title:"First Runner Up at CVPR 2023 6th UG2+ Challenge",description:"First Runner Up at CVPR 2023 6th UG2+ Challenge",section:"Projects",handler:()=>{window.location.href="/projects/UG2_FRU_Cert/"}},{id:"projects-second-runner-up-at-cvpr-2023-6th-ug2-challenge",title:"Second Runner Up at CVPR 2023 6th UG2+ Challenge",description:"Second Runner Up at CVPR 2023 6th UG2+ Challenge",section:"Projects",handler:()=>{window.location.href="/projects/UG2_SRU_Cert/"}},{id:"projects-ieee-tmi-distinguished-reviewer-bronze-level-2022-to-2023",title:"IEEE TMI Distinguished Reviewer Bronze Level 2022 to 2023",description:"IEEE TMI Distinguished Reviewer Bronze Level 2022 to 2023",section:"Projects",handler:()=>{window.location.href="/projects/TMI_reviewer/"}},{id:"projects-joined-as-ieee-consumer-technology-society-39-s-technical-committee-member",title:"Joined as IEEE Consumer Technology Society&#39;s Technical Committee Member",description:"Joined as IEEE Consumer Technology Society&#39;s Technical Committee Member",section:"Projects",handler:()=>{window.location.href="/projects/CTSoc_technical_member/"}},{id:"projects-best-paper-award-at-ieee-wcnc-2024",title:"Best Paper Award at IEEE WCNC 2024",description:"Our paper titled Zero-Trust Attack Framework with Split Learning for Autonomous Vehicles in 6G Networks, has won the best paper award at 25th IEEE Wireless Communications and Networking Conference held at Dubai, UAE",section:"Projects",handler:()=>{window.location.href="/projects/Best_Paper_Award_WCNC/"}},{id:"projects-serving-as-special-session-chair-at-ieee-gem-2024",title:"Serving as Special Session Chair at IEEE GEM 2024",description:"Call for papers in Special Session at IEEE Gaming, Entertainment and Media (GEM) conference, 2024",section:"Projects",handler:()=>{window.location.href="/projects/Chair_special_session_GEM/"}},{id:"projects-top-10-methods-at-cvpr-39-s-ntire-blind-compressed-image-enhancement-challenge",title:"Top 10 Methods at CVPR&#39;s NTIRE Blind Compressed Image Enhancement Challenge",description:"Our submission has been included in the top methods for NTIRE 2024 Blind Compressed Image Enhancement Challenge held at CVPR competitions.",section:"Projects",handler:()=>{window.location.href="/projects/NTIRE_BICE_Challenge/"}},{id:"projects-best-researcher-nomination-certificate",title:"Best Researcher Nomination Certificate",description:"Nomination Certificate for Early Career Researcher Award at Technological University (TU) Dublin, Ireland.",section:"Projects",handler:()=>{window.location.href="/projects/Best_researcher_nomination/"}},{id:"projects-daily-ai-news",title:"Daily AI news",description:"This page is dedicated to your daily AI news especially related to Agents, LLMs, and Agentic AI",section:"Projects",handler:()=>{window.location.href="/projects/Daily_AI_News/"}},{id:"teaching-machine-learning",title:"Machine Learning",description:"2024_Machine_Learning.",section:"Teaching",handler:()=>{window.location.href="/teaching/Machine-Learning/"}},{id:"teaching-object-oriented-programming-java",title:"Object Oriented Programming (Java)",description:"2024_Object_oriented_programming_java.",section:"Teaching",handler:()=>{window.location.href="/teaching/object-oriented-programming-java/"}},{id:"teaching-object-oriented-programming-python",title:"Object Oriented Programming (Python)",description:"2024_Object_oriented_programming_python.",section:"Teaching",handler:()=>{window.location.href="/teaching/object-oriented-programming-python/"}},{id:"teaching-systems-and-network-database-administration",title:"Systems and Network Database Administration",description:"2024_Systems_Database_Administration.",section:"Teaching",handler:()=>{window.location.href="/teaching/systems-database-administration/"}},{id:"teaching-python-for-data-management",title:"Python for Data Management",description:"2024_python_for_data_management.",section:"Teaching",handler:()=>{window.location.href="/teaching/python-for-data-management/"}},{id:"teaching-program-design",title:"Program Design",description:"2024_Program_Design.",section:"Teaching",handler:()=>{window.location.href="/teaching/program-design/"}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}],console.log(ninja.data);</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>