---
layout: page
title: Daily AI news
date: 2025-03-16
description: This page is dedicated to your daily AI news especially related to Agents, LLMs, and Agentic AI
img: assets/img/news/logo_SAK_15.PNG

images:
  lightbox2: false
  photoswipe: false
  spotlight: false
  venobox: false
---


<div style="display: flex; justify-content: center; align-items: center;">
  <div class="pswp-gallery pswp-gallery--single-column" id="gallery--news" style="display: flex; gap: 20px; flex-wrap: wrap; justify-content: center;">
    <a href="/assets/img/news/logo_SAK_15.PNG"
      target="_blank">
      <img src="/assets/img/news/logo_SAK_15.PNG" 
           alt="Comapny Logo" 
           />
</a>

  </div>
</div>

<h1> 16th March 2025 </h1>

<h1> ğŸ” Perplexity API Now Supports MCP Server for Real-Time AI Research </h1>

The Perplexity API has integrated Model Context Protocol (MCP) Server, allowing AI assistants to access live web data seamlessly. 

This means AI agents can retrieve up-to-date information instead of relying solely on pre-trained models.

Key updates:

1ï¸âƒ£AI assistants like Claude can now use Perplexity for real-time research.

2ï¸âƒ£Enables AI chatbots and agents to fetch fresh, relevant data on demand.

3ï¸âƒ£Reduces reliance on static datasets, making AI responses more dynamic and accurate.

4ï¸âƒ£Useful for customer support, research assistants, and real-time decision-making.

â“Why this matters: 

AI models have traditionally struggled with outdated knowledge. Live web research bridges the gap, making AI more reliable in fast-changing domains like news, finance, and scientific research.

What AI applications would benefit most from real-time web integration?




<div style="display: flex; justify-content: center; align-items: center;">
  <div class="pswp-gallery pswp-gallery--single-column" id="gallery--news" style="display: flex; gap: 10px; flex-wrap: wrap; justify-content: center;">
    <iframe width="640" height="480"
src="https://www.youtube.com/watch?v=uZVPevhff6E">
</iframe>

  </div>
</div>

<hr>
<hr>

<h1> ğŸ¤¯ Are Photoshoots a Concept of the Past? </h1>

Google Unveils Native Image Generation with Gemini 2.0 Flash Experimental 

Google has just unlocked a new frontier in AI with the experimental release of Gemini 2.0 Flash, enabling native image generation for developers worldwide. This multimodal model combines text understanding, reasoning, and image generation seamlessly, making it a game-changer for creative and professional applications. 

This step by step recipe enhanced with Native Image Generation is a striking example:

1ï¸âƒ£ Gemini 2.0 Flash can create detailed, realistic images for each step of a recipe, enhancing the cooking experience.

2ï¸âƒ£ Edit images iteratively through natural language dialogue, perfect for refining recipe illustrations or exploring different presentation styles.

3ï¸âƒ£ Generate consistent visual narratives with characters and settings that evolve based on user feedback.

4ï¸âƒ£ Superior text integration within images, ideal for recipe titles, ingredient lists, or cooking instructions. 

ğŸ‘‰ What innovative use cases do you envision for multimodal AI in your industry?

Video: Google DeepMind and AISeeKing



<div style="display: flex; justify-content: center; align-items: center;">
  <div class="pswp-gallery pswp-gallery--single-column" id="gallery--news" style="display: flex; gap: 10px; flex-wrap: wrap; justify-content: center;">
    <iframe width="640" height="480"
src="https://www.youtube.com/watch?v=WDU_CsZ9bgc">
</iframe>

  </div>
</div>

<hr>
<hr>

<h1>ğŸ¯ğŸ¯RexSeek: Referring to Any ObjectğŸ¯ğŸ¯ </h1>

ğŸ‘‰IDEA unveils RexSeek: referring object detection model based on a multimodal LLM, designed to precisely locate objects based on user-input natural language descriptions. Model specialization on humans. Code released under IDEA License 1.0ğŸ’™

ğ‡ğ¢ğ ğ¡ğ¥ğ¢ğ ğ¡ğ­ğ¬:

âœ…RexSeek: detection-oriented multimodal LLMs

âœ…Robust Perception Ability (ie. humans / person)

âœ…Strong Complex Language Comprehension

âœ…HumanRef: new dataset for human targetization

ğŸ‘‰Paper https://arxiv.org/pdf/2503.08507 

ğŸ‘‰Code https://github.com/IDEA-Research/RexSeek


<div style="display: flex; justify-content: center; align-items: center;">
  <div class="pswp-gallery pswp-gallery--single-column" id="gallery--news" style="display: flex; gap: 20px; flex-wrap: wrap; justify-content: center;">
    <a href="/assets/img/news/AI news/3.jpg"

      target="_blank">
      <img src="/assets/img/news/AI news/3.jpg" 
           alt="Nyx" 
          />
</a>

  </div>
</div>

<hr>
<hr>

<h1> Mistral just released a groundbreaking document understanding API </h1>

Mistral just released a groundbreaking document understanding API that processes 2000 pages for $1, outperforming every leading solution on the market.

Their new Mistral OCR Document Intelligence API sets a new standard in document understanding with unprecedented capabilities:

(1) state-of-the-art comprehension - handles complex documents including tables, equations, and LaTeX formatting with superior accuracy

(2) Natively multilingual - processes thousands of scripts, fonts, and languages across all continents

(3) Extremely fast - processes up to 2000 pages per minute on a single node

(4) Structured output - enables document-as-prompt functionality for precise information extraction

(5) On-premises deployment - available for organizations with sensitive data requirements

The benchmarks are impressive: Mistral OCR scored 94.89% overall accuracy, significantly outperforming Google Document AI (83.42%), Azure OCR (89.52%), and even GPT-4o (89.77%) and Gemini models.

With 90% of organizational data stored as documents, this API represents a critical advancement in unlocking collective intelligence from digital information. The price point of 1000-2000 pages for just $1 makes it accessible for organizations of all sizes.

Release post https://mistral.ai/news/mistral-ocr


<div style="display: flex; justify-content: center; align-items: center;">
  <div class="pswp-gallery pswp-gallery--single-column" id="gallery--news" style="display: flex; gap: 20px; flex-wrap: wrap; justify-content: center;">
    <a href="/assets/img/news/AI news/4.jpg"
      target="_blank">
      <img src="/assets/img/news/AI news/4.jpg" 
           alt="ClaudeCode" 
           />
</a>

  </div>
</div>

<hr>
<hr>

<h1> Sesame Labs just dropped CSM (Conversational Speech Model)  </h1>

Sesame Labs just dropped CSM (Conversational Speech Model) - Apache 2.0 licensed! ğŸ’¥

> Trained on 1 MILLION hours of data ğŸ¤¯

> Contextually aware, emotionally intelligent speech

> Voice cloning & watermarking

> Ultra fast, real-time synthesis

> Based on llama architecture & Mimi like decoder

> Apache 2.0 licensed

> Weights on the Hub

So cool to see such a strong Speech backbone out in the wild! Kudos Sesame team! ğŸ¤—

Model: https://huggingface.co/sesame/csm-1b

Space: https://huggingface.co/spaces/sesame/csm-1b

Video Credits: Brian Buntz

<div style="display: flex; justify-content: center; align-items: center;">
  <div class="pswp-gallery pswp-gallery--single-column" id="gallery--news" style="display: flex; gap: 20px; flex-wrap: wrap; justify-content: center;">
    <iframe width="640" height="480"
src="https://www.youtube.com/watch?v=jqHNoTs4Meo">
</iframe>


  </div>
</div>

<hr>
<hr>

<h1> AI just invested $100k in a startup - completely on its own ğŸ˜³
 </h1>

Meet No Cap: the world's first AI angel investor ğŸ¤–

In just 3 minutes, No Cap:

- Signed a SAFE

- Wired $100,000

- Introduced founder to 5 top-tier investors

- Recommended a potential key hire

All on one quick call ğŸ¤¯

No meetings. No back-and-forth emails. No wasted time.

Imagine a world where fundraising doesn't suck. That's exactly what the team is building.

Trained by 60+ elite Y Combinator founders, No Cap isnâ€™t just investing - sheâ€™s connecting, advising, and doing the "dirty work" of hiring and business dev.

The future of venture is here and itâ€™s powered by AI.

<div style="display: flex; justify-content: center; align-items: center;">
  <div class="pswp-gallery pswp-gallery--single-column" id="gallery--news" style="display: flex; gap: 20px; flex-wrap: wrap; justify-content: center;">
    <a href="/assets/img/news/AI news/6.jpg"
      target="_blank">
      <img src="/assets/img/news/AI news/6.jpg" 
           alt="BirdSQL" 
           />
</a>

  </div>
</div>

<hr>
<hr>

<h1> FastRTC just made AI voice agents accessible to everyone...ğŸ¤¯ </h1>

Without needing to be an expert in WebRTC.

And it's 100% open source.

Here's why this matters:

Python developers can now build voice interactions in minutes.

â†³ Turn any Python function into a real-time voice stream

â†³ Built-in voice detection that knows when to respond

â†³ No complex WebRTC configuration required

â†³ Opensource with no vendor lock-in

It creates a voice agent that automatically:

â€¢ Transcribes speech in real-time

â€¢ Waits for natural pauses to respond

â€¢ Handles turn-taking like a human conversation

The best part?

You can get a temporary phone number instantly to test your creation.

No more complicated deployment steps or spending hours debugging audio streams.

This could fundamentally change how we build voice AI applications.

Voice interfaces have always been the most natural way for humans to interact with technology.

But the technical barriers were too high for most developers.

Now anyone with basic Python skills can create experiences like talking to Claude, Gemini, or ChatGPT through voice.

What voice application would you build if the technical complexity was removed?

<div style="display: flex; justify-content: center; align-items: center;">
  <div class="pswp-gallery pswp-gallery--single-column" id="gallery--news" style="display: flex; gap: 20px; flex-wrap: wrap; justify-content: center;">
    <a href="/assets/img/news/AI news/7.jpg"
      target="_blank">
      <img src="/assets/img/news/AI news/7.jpg" 
           alt="Selene" 
           />
</a>

  </div>
</div>
