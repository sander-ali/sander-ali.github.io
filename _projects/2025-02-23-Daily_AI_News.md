---
layout: page
title: Daily AI news
date: 2025-03-24
description: This page is dedicated to your daily AI news especially related to Agents, LLMs, and Agentic AI
img: assets/img/news/logo_SAK_15.PNG

images:
  lightbox2: false
  photoswipe: false
  spotlight: false
  venobox: false
---


<div style="display: flex; justify-content: center; align-items: center;">
  <div class="pswp-gallery pswp-gallery--single-column" id="gallery--news" style="display: flex; gap: 20px; flex-wrap: wrap; justify-content: center;">
    <a href="/assets/img/news/logo_SAK_15.PNG"
      target="_blank">
      <img src="/assets/img/news/logo_SAK_15.PNG" 
           alt="Comapny Logo" 
           />
</a>

  </div>
</div>

<h1> 24th March 2025 </h1>

<h1> ImageRAG: Transforming Satellite Imagery, Medical Imaging, and Climate Monitoring with AI 🌍🔬🌦️ </h1>

Ultra-High-Resolution (UHR) images used in satellite imagery often exceeding 100,000 × 100,000 pixels, overwhelm existing AI tools due to computational and memory limits.

But we need detail in these images for tasks like:

❊ Semantic Segmentation: Identifying detailed regions.

❊ Object Detection: Pinpointing structures, vehicles, or natural features.

❊ Change Detection: Monitoring landscape evolution.

ImageRAG solves these callenges.

﹋﹋﹋﹋﹋

》 𝗧𝗵𝗲 𝗚𝗮𝗺𝗲-𝗖𝗵𝗮𝗻𝗴𝗲𝗿: 𝗜𝗺𝗮𝗴𝗲𝗥𝗔𝗚 

❊ What is ImageRAG?

- A training-free framework designed to tackle UHR image analysis.

- Integrates Retrieval-Augmented Generation (RAG) to selectively process and analyze crucial image portions.

❊ Why it’s revolutionary:

- Efficiently handles vast image data without sacrificing detail.

- Focuses on small but critical details, boosting accuracy.

- Avoids costly model retraining.

﹋﹋﹋﹋﹋
》 𝗖𝗼𝗿𝗲 𝗙𝗲𝗮𝘁𝘂𝗿𝗲𝘀 𝗼𝗳 𝗜𝗺𝗮𝗴𝗲𝗥𝗔𝗚

❊ Two-Path Retrieval System:

✣ Fast Path: Quickly retrieves relevant image patches for straightforward queries.

✣ Slow Path: Digs deeper into labeled databases when the fast path fails.

❊ Training-Free:

- No need for additional annotations or retraining.

- Ready-to-use for diverse applications.

❊ Adaptable and Modular:

✣ Integrates with existing AI models and workflows.

﹋﹋﹋﹋﹋
》 𝗛𝗼𝘄 𝗗𝗼𝗲𝘀 𝗜𝗺𝗮𝗴𝗲𝗥𝗔𝗚 𝗪𝗼𝗿𝗸? 

❊ Image Patch Division:

- Splits large images into smaller, manageable patches.

- Retains context for seamless analysis.

❊ Instruction Analyzing Module:

- Extracts key phrases from user queries.

- Matches text with visual elements.

❊ Text-Image and Image-Image Retrieval:

- Finds the most relevant visual data for any given query.

- Combines satellite imagery with textual inputs for precise results.

﹋﹋﹋﹋﹋
》 𝗔𝗽𝗽𝗹𝗶𝗰𝗮𝘁𝗶𝗼𝗻𝘀: 𝗕𝗲𝘆𝗼𝗻𝗱 𝗦𝗮𝘁𝗲𝗹𝗹𝗶𝘁𝗲 𝗜𝗺𝗮𝗴𝗲𝗿𝘆

ImageRAG works for:

❊ Medical Imaging:

✣ Focusing on anomalies in high-resolution scans.

❊ Retail and Supply Chain:

✣ Monitoring large-scale operations via aerial imagery.

❊ Climate Monitoring:

✣ Analyzing environmental changes with precision.



<div style="display: flex; justify-content: center; align-items: center;">
  <div class="pswp-gallery pswp-gallery--single-column" id="gallery--news" style="display: flex; gap: 10px; flex-wrap: wrap; justify-content: center;">
    <a href="/assets/img/news/AI news/1.gif"

      target="_blank">
      <img src="/assets/img/news/AI news/1.gif" 
           alt="Nyx" 
          />
</a>
  </div>
</div>

<hr>
<hr>

<h1> 𝗔𝗟𝗟𝗮𝗠-𝗧𝗵𝗶𝗻𝗸𝗶𝗻𝗴 - 𝗮𝗻 𝗔𝗿𝗮𝗯𝗶𝗰 𝗹𝗮𝗻𝗴𝘂𝗮𝗴𝗲 𝗺𝗼𝗱𝗲𝗹 𝗼𝗽𝘁𝗶𝗺𝗶𝘇𝗲𝗱 𝗳𝗼𝗿 𝗿𝗲𝗮𝘀𝗼𝗻𝗶𝗻𝗴 𝗮𝗻𝗱 𝗺𝗮𝘁𝗵𝗲𝗺𝗮𝘁𝗶𝗰𝗮𝗹 𝗽𝗿𝗼𝗯𝗹𝗲𝗺-𝘀𝗼𝗹𝘃𝗶𝗻𝗴! </h1>

ALLaM-Thinking, a specialized Arabic Large Language Model that excels at step-by-step reasoning. The model is now available on both Ollama and Hugging Face platforms!

🧠 𝗞𝗲𝘆 𝗙𝗲𝗮𝘁𝘂𝗿𝗲𝘀:

• Arabic-first design built specifically for high-quality Arabic text generation

• Enhanced reasoning capabilities, particularly for mathematical problems

• Step-by-step problem-solving methodology with clear explanations

📊 𝗔𝗟𝗟𝗮𝗠-𝗧𝗵𝗶𝗻𝗸𝗶𝗻𝗴 𝗯𝗿𝗲𝗮𝗸𝘀 𝗱𝗼𝘄𝗻 𝗰𝗼𝗺𝗽𝗹𝗲𝘅 𝗽𝗿𝗼𝗯𝗹𝗲𝗺𝘀 𝗺𝗲𝘁𝗵𝗼𝗱𝗶𝗰𝗮𝗹𝗹𝘆, 𝗱𝗲𝗺𝗼𝗻𝘀𝘁𝗿𝗮𝘁𝗶𝗻𝗴 𝘀𝘁𝗿𝗼𝗻𝗴 𝗰𝗮𝗽𝗮𝗯𝗶𝗹𝗶𝘁𝗶𝗲𝘀 𝗶𝗻:

• Mathematical reasoning with detailed explanations

• Logical analysis and deduction

• Maintaining coherence in complex technical responses

This model is built upon ALLaM-AI/ALLaM-7B-Instruct-preview and implements Group Relative Policy Optimization (GRPO) for improved alignment.

✅ 𝗧𝗿𝘆 𝗶𝘁 𝘆𝗼𝘂𝗿𝘀𝗲𝗹𝗳:

• On Ollama: `ollama pull almaghrabima/ALLaM-Thinking` or visit https://ollama.com/almaghrabima/ALLaM-Thinking

• On Hugging Face: https://huggingface.co/almaghrabima/ALLaM-Thinking

<div style="display: flex; justify-content: center; align-items: center;">
  <div class="pswp-gallery pswp-gallery--single-column" id="gallery--news" style="display: flex; gap: 10px; flex-wrap: wrap; justify-content: center;">
    <a href="/assets/img/news/AI news/2.jpg"

      target="_blank">
      <img src="/assets/img/news/AI news/2.jpg" 
           alt="Nyx" 
          />
</a>

  </div>
</div>

<hr>
<hr>

<h1>AI just changed startups forever </h1>

Y Combinator’s newest batch of startups is breaking every record.

Here's what’s happening:

→ 25% of startups wrote 95% of their code using AI.

→ Teams of fewer than 10 people are hitting $10M+ revenue.

→ They’re growing at 10% per week. Not just one or two - the entire batch.

The era of needing 100 engineers and huge funding rounds? Gone.

AI-powered "vibe coding" means smaller teams, less capital, faster growth.

Silicon Valley’s "growth at all costs" mindset is officially dead.

Profitability and lean teams are the new standard.

Forget landing that job at Google, Amazon, Apple or Meta.
You might find it easier & more enjoyable to build the next $100M AI startup instead.

Because the playing field just leveled.

All thanks to AI.


<div style="display: flex; justify-content: center; align-items: center;">
  <div class="pswp-gallery pswp-gallery--single-column" id="gallery--news" style="display: flex; gap: 20px; flex-wrap: wrap; justify-content: center;">
    <a href="/assets/img/news/AI news/3.jpg"

      target="_blank">
      <img src="/assets/img/news/AI news/3.jpg" 
           alt="Nyx" 
          />
</a>

  </div>
</div>

<hr>
<hr>

<h1>  Kyutai just released MoshiVis - an end-to-end low-latency Vision Speech Model, CC-BY license 🔥 </h1>

> Only adds 206M parameters via lightweight cross-attention (CA) modules to integrate visual inputs from a frozen PaliGemma2-3B-448 vision encoder

> Uses a learnable gating mechanism in the CA modules allows MoshiVis to "turn off" visual input streams when unnecessary, preserving Moshi's conversational abilities

> Adds only ~7ms per inference step on a MacMini with M4 Pro Chip, maintaining real-time performance

> Best part: it keeps the tone, emotion and the prosody of the original Moshi model

> CC-BY-4.0 licensed weights on the hub, allows commercial use 

> Works with MLX, Candle and PyTorch from day-0


<div style="display: flex; justify-content: center; align-items: center;">
  <div class="pswp-gallery pswp-gallery--single-column" id="gallery--news" style="display: flex; gap: 20px; flex-wrap: wrap; justify-content: center;">
    <iframe width="560" height="315" src="https://www.youtube.com/embed/coroLWOS7II?si=bUbzE0COHvJOlX0H" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

  </div>
</div>

<hr>
<hr>

<h1> Langchain announces 🔍📚 Local Deep Research </h1>

An AI research assistant that runs locally, leveraging multiple LLMs for deep analysis across academic and web sources. Built with LangChain, it features RAG-powered search and flexible model support.

Explore this powerful research tool on GitHub 🚀
https://github.com/LearningCircuit/local-deep-research

<div style="display: flex; justify-content: center; align-items: center;">
  <div class="pswp-gallery pswp-gallery--single-column" id="gallery--news" style="display: flex; gap: 20px; flex-wrap: wrap; justify-content: center;">
    <a href="/assets/img/news/AI news/5.jpg"

      target="_blank">
      <img src="/assets/img/news/AI news/5.jpg" 
           alt="Nyx" 
          />
</a>


  </div>
</div>

<hr>
<hr>

<h1> 🚀 OpenAI Releases Next-Gen Audio Models </h1>

OpenAI has just announced the launch of new speech-to-text and text-to-speech models, marking a significant update in voice AI capabilities. 

Key Highlights:

1️⃣Enhanced Accuracy: New speech-to-text models outperform existing solutions, especially in challenging scenarios.

2️⃣Customizable Voices: Developers can now instruct text-to-speech models on how to speak, enabling tailored experiences.

3️⃣Multilingual Support: Strong performance across over 100 languages ensures global reach.

4️⃣Technical Innovations: Advanced distillation techniques and pretraining on authentic audio datasets drive these advancements.

These updates empower developers to build more accurate and expressive voice agents, enhancing user experiences.

What practical applications do you see for these enhanced audio models in your industry?

Video: OpenAI Developers (via 1 Zero)

<div style="display: flex; justify-content: center; align-items: center;">
  <div class="pswp-gallery pswp-gallery--single-column" id="gallery--news" style="display: flex; gap: 20px; flex-wrap: wrap; justify-content: center;">
    <iframe width="560" height="315" src="https://www.youtube.com/embed/sCrM4TBWLig?si=yOSVr7aIM3q8c1Mn" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

  </div>
</div>

<hr>
<hr>

<h1> 🥎🥎 3D LLM Spatial Understanding 🥎🥎 </h1>

👉Manycore unveils SpatialLM: novel LLM designed to process 3D point cloud data and generate structured 3D scene understanding outputs. Paper announced, code, model & test data released💙

𝐇𝐢𝐠𝐡𝐥𝐢𝐠𝐡𝐭𝐬:

✅Reasoning capabilities in complex scene

✅Elements like walls, doors, windows

✅Object BBoxes w/ semantic categories

✅Point clouds from many diverse sources


👉Project https://manycore-research.github.io/SpatialLM/

👉Code https://github.com/manycore-research/SpatialLM

🤗Models https://huggingface.co/manycore-research

<div style="display: flex; justify-content: center; align-items: center;">
  <div class="pswp-gallery pswp-gallery--single-column" id="gallery--news" style="display: flex; gap: 20px; flex-wrap: wrap; justify-content: center;">
   <iframe width="560" height="315" src="https://manycore-research-azure.kujiale.com/manycore-research/SpatialLM/teaser.mp4"></iframe>

  </div>
</div>
