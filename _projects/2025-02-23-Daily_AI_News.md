---
layout: page
title: Daily AI news
date: 2025-04-03
description: This page is dedicated to your daily AI news especially related to Agents, LLMs, and Agentic AI
img: assets/img/news/logo_SAK_15.PNG

images:
  lightbox2: false
  photoswipe: false
  spotlight: false
  venobox: false
---

<div style="display: flex; justify-content: center; align-items: center;">
  <div class="pswp-gallery pswp-gallery--single-column" id="gallery--news" style="display: flex; gap: 10px; flex-wrap: wrap; justify-content: center;">
    <a href="/assets/img/news/logo_SAK_15.PNG"
      target="_blank">
      <img src="/assets/img/news/logo_SAK_15.PNG" 
           alt="Comapny Logo" 
           />
</a>
    
  </div>
</div>

<h1> 3rd April 2025 </h1>

<hr>
<hr>

<h1> ğŸŒ³ğŸŒ³MVSAnywhere: Zero-Shot Multi-ViewğŸŒ³ğŸŒ³  </h1>

ğŸ‘‰Niantic unveils MVSA, novel Multi-View Stereo Architecture to work anywhere by generalizing across diverse domains & depth ranges. Combining monocular & multi-view cues with an adaptive cost volume to deal with scale-related issues. Highly accurate & 3D-consistent depths. Code & models to be releasedğŸ’™

ğ‡ğ¢ğ ğ¡ğ¥ğ¢ğ ğ¡ğ­ğ¬:

âœ…A novel transformer-based architecture

âœ…General-purpose multi-view depth estimation

âœ…View-count-agnostic & scale-agnostic

âœ…SOTA: Robust Multi-View Depth Benchmark

ğŸ‘‰Paper https://arxiv.org/pdf/2503.22430

ğŸ‘‰Project https://nianticlabs.github.io/mvsanywhere/


<div style="display: flex; justify-content: center; align-items: center;">
  <div class="pswp-gallery pswp-gallery--single-column" id="gallery--news" style="display: flex; gap: 20px; flex-wrap: wrap; justify-content: center;">
    <a href="/assets/img/news/AI news/1.jpg"

      target="_blank">
      <img src="/assets/img/news/AI news/1.jpg" 
           alt="Nyx" 
          />
</a>

  </div>
</div>

<hr>
<hr>

<h1> China recently dropped DeepSeek V3-0324 ğŸ˜³ </h1>

In case you missed it, China recently dropped DeepSeek V3-0324 ğŸ˜³

It's only 700GB & 100% open-source AI model.

DeepSeek V3-0324 is a massive 685B-parameter AI model that outperforms Claude 3.5 Sonnet in coding and math.

More importantly, it is: Open Source. Lightning-fast. Affordable.

Hereâ€™s everything you need to know ğŸ‘‡

Runs on Your Desk:

- Operates smoothly (20 tokens/sec) on a Mac Studio (consumer hardware).

- You no longer need enterprise-level budgets or massive data centers.

Coding Powerhouse:

- Effortlessly generates interactive websites and clean, production-ready code.

- Excels at debugging and solving complex coding challenges step-by-step.

Costs Pennies, Not Millions:

- Trained for just $5.58M vs. $100M+ for GPT-4o.
- API price: Only $0.14 per million tokens (OpenAI, watch out).

Fully Open-Source:

- Released under the MIT license.

- Anyone, anywhere, can modify and use it freely.

- This means huge potential to democratize advanced AI, enabling startups and small businesses globally.

DeepSeek is once again shaking up AI dominance, challenging giants like OpenAI and Anthropic to rethink pricing.

Global shift toward accessible, sustainable, and open-source AI tech is closer than we think.

Paradigm is shifting.


<div style="display: flex; justify-content: center; align-items: center;">
  <div class="pswp-gallery pswp-gallery--single-column" id="gallery--news" style="display: flex; gap: 20px; flex-wrap: wrap; justify-content: center;">
    <a href="/assets/img/news/AI news/2.jpg"

      target="_blank">
      <img src="/assets/img/news/AI news/2.jpg" 
           alt="Nyx" 
          />
</a>
  </div>
</div>


<hr>
<hr>

<h1>  ğŸŸğŸŸSegment Any Motion in VideoğŸŸğŸŸ </h1>

ğŸ‘‰UC Berkeley & Peking University unveil a novel approach for moving object segmentation that combines DINO-based semantic features and SAM2. Code under MIT licenseğŸ’™

ğ‡ğ¢ğ ğ¡ğ¥ğ¢ğ ğ¡ğ­ğ¬:

âœ…Novel long-range tracks with SAM2

âœ…Efficient mask densification and tracking

âœ…Motion-Semantic Decoupled Embedding

âœ…SOTA w/ fine-grained moving segmentation

ğŸ‘‰Paper arxiv.org/pdf/2503.22268

ğŸ‘‰Project motion-seg.github.io/

ğŸ‘‰Repo https://github.com/nnanhuang/SegAnyMo


<div style="display: flex; justify-content: center; align-items: center;">
  <div class="pswp-gallery pswp-gallery--single-column" id="gallery--news" style="display: flex; gap: 20px; flex-wrap: wrap; justify-content: center;">
    <a href="/assets/img/news/AI news/3.jpg"

      target="_blank">
      <img src="/assets/img/news/AI news/3.jpg" 
           alt="Nyx" 
          />
</a>

  </div>
</div>

<hr>
<hr>

<h1>  ğŸš€Breaking: Runway Unveils Gen-4 </h1>

ğŸš€Breaking: Runway Unveils Gen-4, The Future of AI-Powered Media Creation

Runway has just launched Gen-4, its next-generation AI model, and its consistency, control, and realism are creating waves! ğŸ”¥ 

Key points:

ğŸ”¹Consistent Characters Across Scenes: Maintains character uniformity across lighting, locations, and styles with just one reference image.

ğŸ”¹Seamless Object Placement: Effortlessly generates objects in any environment while preserving visual coherence.

ğŸ”¹Dynamic Scene Coverage: Captures every angle of your shot using reference images and detailed prompts.

ğŸ”¹Production-Ready Video Quality: Realistic motion, superior prompt adherence, and unparalleled world understanding for professional-grade results.

What possibilities do you see opening up with AI tools like Runway Gen-4 in the creative industry?


<div style="display: flex; justify-content: center; align-items: center;">
  <div class="pswp-gallery pswp-gallery--single-column" id="gallery--news" style="display: flex; gap: 20px; flex-wrap: wrap; justify-content: center;">
    <a href="/assets/img/news/AI news/4.jpg"

      target="_blank">
      <img src="/assets/img/news/AI news/4.jpg" 
           alt="Nyx" 
          />
</a>


  </div>
</div>

<hr>
<hr>

<h1> ComfyUI-Copilot </h1>

It's huge! ComfyUI-Copilot: Your Intelligent Assistant for Comfy-UI!

Key Features ğŸ”¥ :

 ğŸ”· Interactive Q&A: Ask about models, nodes, and parameters with ease

 ğŸ”· Smart Node Search: Find the right nodes using natural language

 ğŸ”· Node Explorer: View explanations, usage tips, and best practices

 ğŸ”· Workflow Builder: Get AI-powered recommendations for building workflows faster

 ğŸ”· Model Finder: Quickly locate base models and LoRAs by prompt

Coming Soon:

 ğŸ”· Auto Parameter Tuning: ML-powered optimization for better results

 ğŸ”· Error Fix Assistant: Instant error detection with suggested solutions

Github code:

ğŸ‘‰ https://github.com/AIDC-AI/ComfyUI-Copilot

<div style="display: flex; justify-content: center; align-items: center;">
  <div class="pswp-gallery pswp-gallery--single-column" id="gallery--news" style="display: flex; gap: 20px; flex-wrap: wrap; justify-content: center;">
   <a href="/assets/img/news/AI news/5.jpg"

      target="_blank">
      <img src="/assets/img/news/AI news/5.jpg" 
           alt="Nyx" 
          />
</a>

  </div>
</div>

<hr>
<hr>

<h1> ğŸš€Breaking: NVIDIA + UC Berkeley = Real-Time Brain-to-Voice AI Breakthrough  </h1>

UC Berkeley and UCSF, powered by NVIDIAâ€™s GPUs, just unveiled a neuroprosthesis that turns brain signals into real-time speech and here are the key highlights:

1ï¸âƒ£NVIDIAâ€™s GPUs at the core: The system relies on NVIDIAâ€™s Tesla GPUs to process complex neural data, enabling accurate and near-instantaneous speech synthesis.

2ï¸âƒ£Real-time speech streaming: The neuroprosthesis delivers fluent voice output within one second of attempted speech, overcoming latency challenges.

3ï¸âƒ£AI-powered adaptability: The technology decodes neural signals with precision, even for unseen words, showcasing its robust learning capabilities.

4ï¸âƒ£Empowering embodiment: By replicating the userâ€™s voice in real time, the system restores a sense of self and natural communication.

5ï¸âƒ£Broad applicability: Compatible with invasive and non-invasive brain-sensing devices, this innovation could soon benefit millions globally. 

What other medical applications do you think NVIDIAâ€™s AI technology could revolutionize next?

<div style="display: flex; justify-content: center; align-items: center;">
  <div class="pswp-gallery pswp-gallery--single-column" id="gallery--news" style="display: flex; gap: 20px; flex-wrap: wrap; justify-content: center;">
    <a href="/assets/img/news/AI news/6.jpg"

      target="_blank">
      <img src="/assets/img/news/AI news/6.jpg" 
           alt="Nyx" 
          />
</a>


  </div>
</div>

<hr>
<hr>

<h1> ğŸ“½ï¸ Automated filmmaking/digital human is the future </h1>

Thrilled to introduce **â˜•MoCha: Towards Movie-Grade Talking Character Synthesis**

- â­ We define a new taskâ€”Talking Charactersâ€”that generates lifelike character animations directly from natural language + speech input.

- â­ Our proposed model, â˜•MoCha, is the first DiT-based system capable of producing cinema-quality talking characters.

- â­ For the first time, â˜•MoCha enables **multi-character conversations with turn-based dialogue** and expressive motion, pushing the frontier of automated, AI-powered storytelling.

Project Page: https://congwei1230.github.io/MoCha/

Paper: https://arxiv.org/pdf/2503.23307


<div style="display: flex; justify-content: center; align-items: center;">
  <div class="pswp-gallery pswp-gallery--single-column" id="gallery--news" style="display: flex; gap: 20px; flex-wrap: wrap; justify-content: center;">

	<a href="/assets/img/news/AI news/7.jpg"

      target="_blank">
      <img src="/assets/img/news/AI news/7.jpg" 
           alt="Nyx" 
          />
</a>
  </div>
</div>
