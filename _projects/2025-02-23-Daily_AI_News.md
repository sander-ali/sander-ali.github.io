---
layout: page
title: Daily AI news
date: 2025-03-25
description: This page is dedicated to your daily AI news especially related to Agents, LLMs, and Agentic AI
img: assets/img/news/logo_SAK_15.PNG

images:
  lightbox2: false
  photoswipe: false
  spotlight: false
  venobox: false
---


<div style="display: flex; justify-content: center; align-items: center;">
  <div class="pswp-gallery pswp-gallery--single-column" id="gallery--news" style="display: flex; gap: 20px; flex-wrap: wrap; justify-content: center;">
    <a href="/assets/img/news/logo_SAK_15.PNG"
      target="_blank">
      <img src="/assets/img/news/logo_SAK_15.PNG" 
           alt="Comapny Logo" 
           />
</a>

  </div>
</div>

<h1> 25th March 2025 </h1>

<h1> ğŸ¦¾ JUST IN: Unitree's G1 Robot Nails First-Ever Standing Side Flip </h1>

Humanoid robotics just reached a milestone that seemed impossible just months ago. 

â†˜ï¸Comes just one year after Unitree's H1 (1.8m) pioneered the first standing backflip by an electric humanoid.

â†˜ï¸Demonstrates flawless execution of a more technically challenging maneuver than the backflip.

â†˜ï¸Zero hardware malfunctions or damage occurred throughout programming and filming.

The precision control systems that enable these flips are the same technologies that will allow humanoids to navigate complex real-world environments, respond to unexpected obstacles, and perform intricate tasks alongside humans.

What everyday applications do you see for humanoid robots with this level of physical dexterity and dynamic control?


<div style="display: flex; justify-content: center; align-items: center;">
  <div class="pswp-gallery pswp-gallery--single-column" id="gallery--news" style="display: flex; gap: 10px; flex-wrap: wrap; justify-content: center;">
    <iframe width="560" height="315" src="https://www.youtube.com/embed/29xLWhqME2Q?si=jd4rBvX4774GRODg" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
  </div>
</div>

<hr>
<hr>

<h1> ğŸ™ï¸ Oliva: Voice RAG Assistant </h1>

A powerful open-source assistant that enables natural voice search of vector databases. Built with LangChain's workflow engine and cutting-edge voice tech, Oliva delivers seamless agentic RAG capabilities.

Check it out ğŸš€
https://github.com/Deluxer/oliva

<div style="display: flex; justify-content: center; align-items: center;">
  <div class="pswp-gallery pswp-gallery--single-column" id="gallery--news" style="display: flex; gap: 10px; flex-wrap: wrap; justify-content: center;">
    <a href="/assets/img/news/AI news/2.jpg"

      target="_blank">
      <img src="/assets/img/news/AI news/2.jpg" 
           alt="Nyx" 
          />
</a>

  </div>
</div>

<hr>
<hr>

<h1> LlamaExtract is now in public beta ğŸ”¥ </h1>

The leading, genAI-native agent for structured document extraction.

We adapt the latest models and tune them so that you can structure even the most complex documents (financial reports, invoices, resumes, and more) in a highly accurate, reliable way. 

No more worrying about 

1) overflowing the context window, 

2) getting misformatted json, 

3) figuring out how to version your agent.

Itâ€™s built on the same foundation as LlamaParse, our parsing service, which allows understanding of even the most complex documents.

Big shoutout to Neeraj Pradhan for driving this forward. 

Check it out: https://cloud.llamaindex.ai/login
Sample Notebook: https://github.com/run-llama/llama_cloud_services/blob/main/examples/extract/resume_screening.ipynb


<div style="display: flex; justify-content: center; align-items: center;">
  <div class="pswp-gallery pswp-gallery--single-column" id="gallery--news" style="display: flex; gap: 20px; flex-wrap: wrap; justify-content: center;">
    <a href="/assets/img/news/AI news/3.jpg"

      target="_blank">
      <img src="/assets/img/news/AI news/3.jpg" 
           alt="Nyx" 
          />
</a>

  </div>
</div>

<hr>
<hr>

<h1>  Forget RAG, welcome Agentic RAG  </h1>

ğ—¡ğ—®ğ˜ğ—¶ğ˜ƒğ—² ğ—¥ğ—”ğ—š 

In Native RAG, the most common implementation nowadays, the user query is processed through a pipeline that includes retrieval, reranking, synthesis, and generation of a response. 
 
This process leverages retrieval and generation-based methods to provide accurate and contextually relevant answers. 
 
ğ—”ğ—´ğ—²ğ—»ğ˜ğ—¶ğ—° ğ—¥ğ—”ğ—š 

Agentic RAG is an advanced, agent-based approach to question answering over multiple documents in a coordinated manner. It involves comparing different documents, summarizing specific documents, or comparing various summaries. 
 
Agentic RAG is a flexible framework that supports complex tasks requiring planning, multi-step reasoning, tool use, and learning over time. 
 
ğ—ğ—²ğ˜† ğ—–ğ—¼ğ—ºğ—½ğ—¼ğ—»ğ—²ğ—»ğ˜ğ˜€ ğ—®ğ—»ğ—± ğ—”ğ—¿ğ—°ğ—µğ—¶ğ˜ğ—²ğ—°ğ˜ğ˜‚ğ—¿ğ—² 

- Document Agents: Each document is assigned a dedicated agent capable of answering questions and summarizing within its own document. 
 
- Meta-Agent: A top-level agent manages all the document agents, orchestrating their interactions and integrating their outputs to generate a coherent and comprehensive response. 
 
ğ—™ğ—²ğ—®ğ˜ğ˜‚ğ—¿ğ—²ğ˜€ ğ—®ğ—»ğ—± ğ—•ğ—²ğ—»ğ—²ğ—³ğ—¶ğ˜ğ˜€ 

- Autonomy: Agents act independently to retrieve, process, and generate information. 
 
- Adaptability: The system can adjust strategies based on new data and changing contexts. 
 
- Proactivity: Agents can anticipate needs and take preemptive actions to achieve goals. 
Applications 
 
Agentic RAG is particularly useful in scenarios requiring thorough and nuanced information processing and decision-making. 


<div style="display: flex; justify-content: center; align-items: center;">
  <div class="pswp-gallery pswp-gallery--single-column" id="gallery--news" style="display: flex; gap: 20px; flex-wrap: wrap; justify-content: center;">
    <a href="/assets/img/news/AI news/4.jpg"

      target="_blank">
      <img src="/assets/img/news/AI news/4.jpg" 
           alt="Nyx" 
          />
</a>

  </div>
</div>

<hr>
<hr>

<h1> ğŸ¤–BREAKING: NVIDIA Drops First-Ever Open Foundation Model for Humanoid Robots  </h1>

The race to develop general-purpose humanoid robots just accelerated dramatically!

Key information:

ğŸ”¸GR00T N1 features a dual-system cognitive architecture with a Vision-Language Model for reasoning and a Diffusion Transformer for generating precise movements

ğŸ”¸The model achieves 76.8% success rate on real-world tasks - dramatically outperforming baseline methods, especially with limited training data

ğŸ”¸NVIDIA's synthetic data strategy generated 750K trajectories in just 11 hours (equivalent to 9 months of human demonstrations)

ğŸ”¸The 2B parameter model is now available on Hugging Face with sample datasets and PyTorch scripts for customization

How do you think general-purpose humanoid robots will transform manufacturing and logistics in the coming years?

<div style="display: flex; justify-content: center; align-items: center;">
  <div class="pswp-gallery pswp-gallery--single-column" id="gallery--news" style="display: flex; gap: 20px; flex-wrap: wrap; justify-content: center;">
    <iframe width="560" height="315" src="https://www.youtube.com/embed/BslFNYJjjb4?si=6Z2UtdVkBAG8ODnj" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>


  </div>
</div>

<hr>
<hr>

<h1> Improving small model abilities with DeepSeek AI R1 few shots.  </h1>

A new experiment powered by Hugging Face Inference Providers and Data Studio!

Last week, I shared the LIMO (Less is more for reasoning) dataset categorized with Llama 70B. I aimed to understand the topic distribution of this small but powerful dataset.

This time, I wanted to experiment with a new idea: what if we could use few-shot examples (demonstrations) from R1 to improve Llama's ability to extract topics?

Here's what I did:

1. Run R1 over a few rows. 

2. Inspect and validate the results.

3. Use 5 validated R1 responses as part of the prompt to Llama.

4. Categorize the entire dataset.

5. Use Data Studio to extract the distribution of topics and compare it to my previous dataset.

It all took under 30 minutes. It's amazing how quickly you can turn ideas into results using the Hub!

Dataset:

https://huggingface.co/datasets/dvilasuero/LIMO-topics-r1-fewshots

Topics with Llama 70B:

https://huggingface.co/datasets/dvilasuero/GAIR_LIMO_topics/sql-console/eNIH1nJ

Topics with Llama + R1 few shots:

https://huggingface.co/datasets/dvilasuero/LIMO-topics-r1-fewshots/sql-console/rDsKKX9

Prompt for R1 (needed to add explicit instruction not to solve the problem ğŸ¤£ )

Classify the math topics of the question. You MUST NO SOLVE the question, just analyze and give me a list of very precise topics that are needed to solve the question: 

{{question}} 

Pipeline:

https://huggingface.co/datasets/dvilasuero/LIMO-topics-r1-fewshots/blob/main/config.yml

<div style="display: flex; justify-content: center; align-items: center;">
  <div class="pswp-gallery pswp-gallery--single-column" id="gallery--news" style="display: flex; gap: 20px; flex-wrap: wrap; justify-content: center;">
    <a href="/assets/img/news/AI news/5.jpg"

      target="_blank">
      <img src="/assets/img/news/AI news/5.jpg" 
           alt="Nyx" 
          />
</a>

  </div>
</div>

<hr>
<hr>

<h1> Qwen just DROPPED a 32B VLM - beats Qwen 2.5 72B and GPT 4o Mini - Apache 2.0 licensed ğŸ”¥ </h1>

Vision Tasks (vs. Qwen2-VL-72B):

> MMMU: 70.0 (vs. 64.5)

> MathVista: 74.7 (vs. 70.5)

> OCRBenchV2: 57.2/59.1 (vs. 47.8/46.1)

> Android Control: 69.6/93.3 (vs. 66.4/84.4)

Text Tasks (vs. Competing Models):

> MMLU: 78.4 (vs. Mistral-24B: 80.6, GPT-4o-Mini: 82.0)

> MATH: 82.2 (vs. Gemma3-27B: 89.0, GPT-4o-Mini: 70.2)

> HumanEval: 91.5 (vs. Claude-3.5-Haiku: 88.1)

Multimodal Capabilities:

> Recognizes objects, analyzes text/charts/graphics, and supports structured outputs (e.g., invoices, tables)

> Acts as a visual agent, dynamically using tools for computer/phone interactions

> Understands 1+ hour videos and pinpoints relevant segments (temporal localization)

> Generates bounding boxes/points with stable JSON outputs

Very cool to see Qwen ship such a competitive multimodal model! ğŸ¤—

<div style="display: flex; justify-content: center; align-items: center;">
  <div class="pswp-gallery pswp-gallery--single-column" id="gallery--news" style="display: flex; gap: 20px; flex-wrap: wrap; justify-content: center;">
   <a href="/assets/img/news/AI news/7.jpg"

      target="_blank">
      <img src="/assets/img/news/AI news/7.jpg" 
           alt="Nyx" 
          />
</a>

  </div>
</div>
