---
layout: page
title: Daily AI news
date: 2025-03-10
description: This page is dedicated to your daily AI news especially related to Agents, LLMs, and Agentic AI
img: assets/img/news/logo_SAK_15.PNG

images:
  lightbox2: false
  photoswipe: false
  spotlight: false
  venobox: false
---


<div style="display: flex; justify-content: center; align-items: center;">
  <div class="pswp-gallery pswp-gallery--single-column" id="gallery--news" style="display: flex; gap: 20px; flex-wrap: wrap; justify-content: center;">
    <a href="/assets/img/news/logo_SAK_15.PNG"
      target="_blank">
      <img src="/assets/img/news/logo_SAK_15.PNG" 
           alt="Comapny Logo" 
           />
</a>

  </div>
</div>

<h1> 10th March 2025 </h1>

<h1> New dataset from researchers at Meta </h1>

New dataset from researchers at Meta — uCO3D, or UnCommon Objects in 3D, is the largest publicly-available object-centric dataset for 3D deep learning and 3D generative AI.

More on this project ➡️ https://go.fb.me/8u86hq

Documentation and download ➡️ https://go.fb.me/izrajn

Highlights

• 170,000 videos depicting diverse objects from all directions.

• 19.3TB of data.

• Objects come from the LVIS taxonomy of ~1000 categories, grouped into 50 super-categories.

• Full original videos instead of frames — each annotated with object segmentation, camera poses and point clouds.

• 3D Gaussian Splat reconstruction for each video.

• Long and short caption obtained for each scene with a large video-language model.

• Significantly improved annotation quality and size compared to previous datasets of its kind.

<div style="display: flex; justify-content: center; align-items: center;">
  <div class="pswp-gallery pswp-gallery--single-column" id="gallery--news" style="display: flex; gap: 10px; flex-wrap: wrap; justify-content: center;">
    <a href="/assets/img/news/AI news/1.jpg"
      target="_blank">
      <img src="/assets/img/news/AI news/1.jpg" 
           alt="Gemini" 
            />
</a>

  </div>
</div>


<h1> 🚨 Meta Unveils the Compute Puck – The Key to AR Glasses </h1>

Meta’s Orion AR glasses wouldn’t be possible without a small but powerful innovation: the Compute Puck. This pocket-sized device offloads processing, enabling lighter, high-performance AR experiences.

👓Powers Orion’s AI and machine perception

👓Reduces heat, boosts battery life, and enhances wireless performance

👓Designed for seamless integration with AR glasses and EMG wristbands

The Compute Puck shows how AR is evolving beyond phones and wearables.


<div style="display: flex; justify-content: center; align-items: center;">
  <div class="pswp-gallery pswp-gallery--single-column" id="gallery--news" style="display: flex; gap: 10px; flex-wrap: wrap; justify-content: center;">
    <a href="/assets/img/news/AI news/2.jpg"
      target="_blank">
      <img src="/assets/img/news/AI news/2.jpg" 
           alt="OMi" 
            />
</a>

  </div>
</div>


<h1>Microsoft & OpenAI, the AI Power Couple, are breaking up 😳</h1>

Microsoft invested billions in OpenAI. But something just changed.

Despite revolutionizing the AI landscape, Microsoft is now:

- Quietly stepping away from OpenAI.

- Testing new partners: Elon Musk’s xAI, Meta, and DeepSeek.

- Building its own AI models to rival ChatGPT.

Why the sudden shift?

↳ Cost Control: OpenAI tech is solid but expensive.

Microsoft seeks cheaper, equally powerful solutions.

↳ Independence: Reducing dependency means more flexibility.

Microsoft wants full control over its AI future.

↳ Competition: Microsoft's own AI research is catching up fast.

It no longer needs OpenAI as its sole powerhouse.

On top of that, there's some talking around:

- Rumors of canceled projects (like OpenAI's Stargate).

- Speculation about layoffs and data center closures.

- Questions raised about OpenAI’s leadership status.

Of course, Microsoft isn’t cutting ties completely yet.
But the relationship just went from exclusive to complicated.

AI's biggest alliance is evolving fast 🍿



<div style="display: flex; justify-content: center; align-items: center;">
  <div class="pswp-gallery pswp-gallery--single-column" id="gallery--news" style="display: flex; gap: 20px; flex-wrap: wrap; justify-content: center;">
    <a href="/assets/img/news/AI news/3.jpg"

      target="_blank">
      <img src="/assets/img/news/AI news/3.jpg" 
           alt="Nyx" 
          />
</a>

  </div>
</div>


<h1> Gemini models for Embeddings! </h1>

Gemini models for Embeddings! Yes, Google DeepMind just released a new experimental embedding model ranking #1 on MMTEB with 8k context. 👀

- 🧪 Experimental version, free to try in AI Studio and via API

- 🥇 Top MMTEB leaderboard with a score of 68.32 (+5.81 lead)

- 💡 Build for finance, science, legal, search, code

- 📏 8K input token context

- 🪆Matryoshka Representation Learning (MRL) for flexible dimensionality

- 🌐 Multilingual in +100 languages

Blog: https://developers.googleblog.com/en/gemini-embedding-text-model-now-available-gemini-api/

<div style="display: flex; justify-content: center; align-items: center;">
  <div class="pswp-gallery pswp-gallery--single-column" id="gallery--news" style="display: flex; gap: 20px; flex-wrap: wrap; justify-content: center;">
    <a href="/assets/img/news/AI news/4.jpg"
      target="_blank">
      <img src="/assets/img/news/AI news/4.jpg" 
           alt="ClaudeCode" 
           />
</a>

  </div>
</div>

<h1> China's DeepSeek moment 2.0 is here, and it's called Manus AI 😳 </h1>

Already viral in China, this AI agent is about to go global.

Think Perplexity or xAI Deep Research meets OpenaAI Operator and Claude Computer - rolled into ONE unstoppable AI agent.

It reportedly can handle up to 50 complex tasks simultaneously:

- Managing financial transactions

- Deep-dive research

- Planning your dream vacation

- Even launching websites - completely autonomously.

Reports already suggest it's outperforming DeepSeek and OpenAI models in real-world benchmarks.

But here comes the crazy part.

Just this week, reports came that OpenAI aims to launch $20k/month AI agents.

Well, Manus just flipped the table as their AI agent is reportedly partially open-source, affordable, and powerful enough to disrupt entire industries.

China is no longer catching up - it's leading in AI globally.

<div style="display: flex; justify-content: center; align-items: center;">
  <div class="pswp-gallery pswp-gallery--single-column" id="gallery--news" style="display: flex; gap: 20px; flex-wrap: wrap; justify-content: center;">
    <a href="/assets/img/news/AI news/5.JPG"
      target="_blank">
      <img src="/assets/img/news/AI news/5.JPG" 
           alt="Gradio" 
          />
</a>

  </div>
</div>

<h1> Real-time Dense SLAM with 3D Reconstruction from MASt3R-SLAM 🔥🔥 </h1>

Dense reconstruction and camera localization have always been a big problem in the computer vision space and especially in real-time. This is even from an uncalibrated single RGB camera. No depth, no stereo, no calibration and it runs in real-time. This is a real breakthrough and will create videos around it on my YouTube channel. Even how we can get it up on running on an edge device which can be portable. 


A real-time monocular dense SLAM system designed bottom-up from MASt3R, a two-view 3D reconstruction and matching prior. Equipped with this strong prior, the system is robust on in-the-wild video sequences despite making no assumption on a fixed or parametric camera model beyond a unique camera centre. They introduce efficient methods for pointmap matching, camera tracking and local fusion, graph construction and loop closure, and second-order global optimisation. With known calibration, a simple modification to the system achieves state-of-the-art performance across various benchmarks. Altogether. It's a plug-and-play monocular SLAM system capable of producing globally-consistent poses and dense geometry while operating at 15 FPS.


👉 Check out the project page here and code: https://edexheim.github.io/mast3r-slam/

<div style="display: flex; justify-content: center; align-items: center;">
  <div class="pswp-gallery pswp-gallery--single-column" id="gallery--news" style="display: flex; gap: 20px; flex-wrap: wrap; justify-content: center;">
    <a href="/assets/img/news/AI news/6.jpg"
      target="_blank">
      <img src="/assets/img/news/AI news/6.jpg" 
           alt="BirdSQL" 
           />
</a>

  </div>
</div>

<h1> The world's first diffusion LLM is here - faster, cheaper, and what else? </h1>

Diffusion models - used in text-to-image AI like Stable Diffusion.
LLMs - you already know!

This new type of LLM - Diffusion Large Language Models (DLMMs) - combines both.

The model was developed by Inception Labs, called Mercury Coder, specifically for code generation.

It is the first working large-scale diffusion-based LLM.

As an end user, two points stood out to me as the most attractive: 

1- It’s up to 10x faster & 10x cheaper than traditional LLMs.

2- It runs on standard Nvidia H100 GPUs—no specialized hardware needed.

Traditional LLMs (like GPT-4, Claude, etc.) generate text one token at a time, waiting for the previous token before predicting the next. This process is inherently sequential and slow.

(Btw, it’s crazy how fast GenAI is evolving—now we have “traditional LLMs,” even though LLMs have only been around for a few years!)

Back to this new DLMM, it works differently:

🔹It generates the entire output at once—but in a rough, noisy state.

🔹It then refines the output step by step, making it more coherent.

This is similar to how diffusion models generate images—starting with pure noise and gradually refining it into something recognizable.

Because the model sees the entire output at once, it can self-correct and refine mistakes, leading to better reasoning and fewer hallucinations.

We’ve talked about coding AI for a while, but this one might be a real game-changer. 

There are already some testing videos on YouTube showing impressive results—Mercury Coder can generate complete working programs in just seconds, with decent quality.

So, can we try it?

Right now, full access is limited to enterprises, but you can test the model in its Playground and see how it works. I’ll drop the link in the comments.


<div style="display: flex; justify-content: center; align-items: center;">
  <div class="pswp-gallery pswp-gallery--single-column" id="gallery--news" style="display: flex; gap: 20px; flex-wrap: wrap; justify-content: center;">
    <a href="/assets/img/news/AI news/7.jpg"
      target="_blank">
      <img src="/assets/img/news/AI news/7.jpg" 
           alt="Selene" 
           />
</a>

  </div>
</div>
