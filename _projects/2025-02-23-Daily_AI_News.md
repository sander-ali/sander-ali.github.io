---
layout: page
title: Daily AI news
date: 2025-04-03
description: This page is dedicated to your daily AI news especially related to Agents, LLMs, and Agentic AI
img: assets/img/news/logo_SAK_15.PNG

images:
  lightbox2: false
  photoswipe: false
  spotlight: false
  venobox: false
---

<div style="display: flex; justify-content: center; align-items: center;">
  <div class="pswp-gallery pswp-gallery--single-column" id="gallery--news" style="display: flex; gap: 10px; flex-wrap: wrap; justify-content: center;">
    <a href="/assets/img/news/logo_SAK_15.PNG"
      target="_blank">
      <img src="/assets/img/news/logo_SAK_15.PNG" 
           alt="Comapny Logo" 
           />
</a>
    
  </div>
</div>

<h1> 3rd April 2025 </h1>

<hr>
<hr>

<h1> 🌳🌳MVSAnywhere: Zero-Shot Multi-View🌳🌳  </h1>

👉Niantic unveils MVSA, novel Multi-View Stereo Architecture to work anywhere by generalizing across diverse domains & depth ranges. Combining monocular & multi-view cues with an adaptive cost volume to deal with scale-related issues. Highly accurate & 3D-consistent depths. Code & models to be released💙

𝐇𝐢𝐠𝐡𝐥𝐢𝐠𝐡𝐭𝐬:

✅A novel transformer-based architecture

✅General-purpose multi-view depth estimation

✅View-count-agnostic & scale-agnostic

✅SOTA: Robust Multi-View Depth Benchmark

👉Paper https://arxiv.org/pdf/2503.22430

👉Project https://nianticlabs.github.io/mvsanywhere/


<div style="display: flex; justify-content: center; align-items: center;">
  <div class="pswp-gallery pswp-gallery--single-column" id="gallery--news" style="display: flex; gap: 20px; flex-wrap: wrap; justify-content: center;">
    <a href="/assets/img/news/AI news/1.jpg"

      target="_blank">
      <img src="/assets/img/news/AI news/1.jpg" 
           alt="Nyx" 
          />
</a>

  </div>
</div>

<hr>
<hr>

<h1> China recently dropped DeepSeek V3-0324 😳 </h1>

In case you missed it, China recently dropped DeepSeek V3-0324 😳

It's only 700GB & 100% open-source AI model.

DeepSeek V3-0324 is a massive 685B-parameter AI model that outperforms Claude 3.5 Sonnet in coding and math.

More importantly, it is: Open Source. Lightning-fast. Affordable.

Here’s everything you need to know 👇

Runs on Your Desk:

- Operates smoothly (20 tokens/sec) on a Mac Studio (consumer hardware).

- You no longer need enterprise-level budgets or massive data centers.

Coding Powerhouse:

- Effortlessly generates interactive websites and clean, production-ready code.

- Excels at debugging and solving complex coding challenges step-by-step.

Costs Pennies, Not Millions:

- Trained for just $5.58M vs. $100M+ for GPT-4o.
- API price: Only $0.14 per million tokens (OpenAI, watch out).

Fully Open-Source:

- Released under the MIT license.

- Anyone, anywhere, can modify and use it freely.

- This means huge potential to democratize advanced AI, enabling startups and small businesses globally.

DeepSeek is once again shaking up AI dominance, challenging giants like OpenAI and Anthropic to rethink pricing.

Global shift toward accessible, sustainable, and open-source AI tech is closer than we think.

Paradigm is shifting.


<div style="display: flex; justify-content: center; align-items: center;">
  <div class="pswp-gallery pswp-gallery--single-column" id="gallery--news" style="display: flex; gap: 20px; flex-wrap: wrap; justify-content: center;">
    <a href="/assets/img/news/AI news/2.jpg"

      target="_blank">
      <img src="/assets/img/news/AI news/2.jpg" 
           alt="Nyx" 
          />
</a>
  </div>
</div>


<hr>
<hr>

<h1>  🐟🐟Segment Any Motion in Video🐟🐟 </h1>

👉UC Berkeley & Peking University unveil a novel approach for moving object segmentation that combines DINO-based semantic features and SAM2. Code under MIT license💙

𝐇𝐢𝐠𝐡𝐥𝐢𝐠𝐡𝐭𝐬:

✅Novel long-range tracks with SAM2

✅Efficient mask densification and tracking

✅Motion-Semantic Decoupled Embedding

✅SOTA w/ fine-grained moving segmentation

👉Paper arxiv.org/pdf/2503.22268

👉Project motion-seg.github.io/

👉Repo https://github.com/nnanhuang/SegAnyMo


<div style="display: flex; justify-content: center; align-items: center;">
  <div class="pswp-gallery pswp-gallery--single-column" id="gallery--news" style="display: flex; gap: 20px; flex-wrap: wrap; justify-content: center;">
    <a href="/assets/img/news/AI news/3.jpg"

      target="_blank">
      <img src="/assets/img/news/AI news/3.jpg" 
           alt="Nyx" 
          />
</a>

  </div>
</div>

<hr>
<hr>

<h1>  🚀Breaking: Runway Unveils Gen-4 </h1>

🚀Breaking: Runway Unveils Gen-4, The Future of AI-Powered Media Creation

Runway has just launched Gen-4, its next-generation AI model, and its consistency, control, and realism are creating waves! 🔥 

Key points:

🔹Consistent Characters Across Scenes: Maintains character uniformity across lighting, locations, and styles with just one reference image.

🔹Seamless Object Placement: Effortlessly generates objects in any environment while preserving visual coherence.

🔹Dynamic Scene Coverage: Captures every angle of your shot using reference images and detailed prompts.

🔹Production-Ready Video Quality: Realistic motion, superior prompt adherence, and unparalleled world understanding for professional-grade results.

What possibilities do you see opening up with AI tools like Runway Gen-4 in the creative industry?


<div style="display: flex; justify-content: center; align-items: center;">
  <div class="pswp-gallery pswp-gallery--single-column" id="gallery--news" style="display: flex; gap: 20px; flex-wrap: wrap; justify-content: center;">
    <a href="/assets/img/news/AI news/4.jpg"

      target="_blank">
      <img src="/assets/img/news/AI news/4.jpg" 
           alt="Nyx" 
          />
</a>


  </div>
</div>

<hr>
<hr>

<h1> ComfyUI-Copilot </h1>

It's huge! ComfyUI-Copilot: Your Intelligent Assistant for Comfy-UI!

Key Features 🔥 :

 🔷 Interactive Q&A: Ask about models, nodes, and parameters with ease

 🔷 Smart Node Search: Find the right nodes using natural language

 🔷 Node Explorer: View explanations, usage tips, and best practices

 🔷 Workflow Builder: Get AI-powered recommendations for building workflows faster

 🔷 Model Finder: Quickly locate base models and LoRAs by prompt

Coming Soon:

 🔷 Auto Parameter Tuning: ML-powered optimization for better results

 🔷 Error Fix Assistant: Instant error detection with suggested solutions

Github code:

👉 https://github.com/AIDC-AI/ComfyUI-Copilot

<div style="display: flex; justify-content: center; align-items: center;">
  <div class="pswp-gallery pswp-gallery--single-column" id="gallery--news" style="display: flex; gap: 20px; flex-wrap: wrap; justify-content: center;">
   <a href="/assets/img/news/AI news/5.jpg"

      target="_blank">
      <img src="/assets/img/news/AI news/5.jpg" 
           alt="Nyx" 
          />
</a>

  </div>
</div>

<hr>
<hr>

<h1> 🚀Breaking: NVIDIA + UC Berkeley = Real-Time Brain-to-Voice AI Breakthrough  </h1>

UC Berkeley and UCSF, powered by NVIDIA’s GPUs, just unveiled a neuroprosthesis that turns brain signals into real-time speech and here are the key highlights:

1️⃣NVIDIA’s GPUs at the core: The system relies on NVIDIA’s Tesla GPUs to process complex neural data, enabling accurate and near-instantaneous speech synthesis.

2️⃣Real-time speech streaming: The neuroprosthesis delivers fluent voice output within one second of attempted speech, overcoming latency challenges.

3️⃣AI-powered adaptability: The technology decodes neural signals with precision, even for unseen words, showcasing its robust learning capabilities.

4️⃣Empowering embodiment: By replicating the user’s voice in real time, the system restores a sense of self and natural communication.

5️⃣Broad applicability: Compatible with invasive and non-invasive brain-sensing devices, this innovation could soon benefit millions globally. 

What other medical applications do you think NVIDIA’s AI technology could revolutionize next?

<div style="display: flex; justify-content: center; align-items: center;">
  <div class="pswp-gallery pswp-gallery--single-column" id="gallery--news" style="display: flex; gap: 20px; flex-wrap: wrap; justify-content: center;">
    <a href="/assets/img/news/AI news/6.jpg"

      target="_blank">
      <img src="/assets/img/news/AI news/6.jpg" 
           alt="Nyx" 
          />
</a>


  </div>
</div>

<hr>
<hr>

<h1> 📽️ Automated filmmaking/digital human is the future </h1>

Thrilled to introduce **☕MoCha: Towards Movie-Grade Talking Character Synthesis**

- ⭐ We define a new task—Talking Characters—that generates lifelike character animations directly from natural language + speech input.

- ⭐ Our proposed model, ☕MoCha, is the first DiT-based system capable of producing cinema-quality talking characters.

- ⭐ For the first time, ☕MoCha enables **multi-character conversations with turn-based dialogue** and expressive motion, pushing the frontier of automated, AI-powered storytelling.

Project Page: https://congwei1230.github.io/MoCha/

Paper: https://arxiv.org/pdf/2503.23307


<div style="display: flex; justify-content: center; align-items: center;">
  <div class="pswp-gallery pswp-gallery--single-column" id="gallery--news" style="display: flex; gap: 20px; flex-wrap: wrap; justify-content: center;">

	<a href="/assets/img/news/AI news/7.jpg"

      target="_blank">
      <img src="/assets/img/news/AI news/7.jpg" 
           alt="Nyx" 
          />
</a>
  </div>
</div>
