---
layout: page
title: Daily AI news
date: 2025-03-21
description: This page is dedicated to your daily AI news especially related to Agents, LLMs, and Agentic AI
img: assets/img/news/logo_SAK_15.PNG

images:
  lightbox2: false
  photoswipe: false
  spotlight: false
  venobox: false
---


<div style="display: flex; justify-content: center; align-items: center;">
  <div class="pswp-gallery pswp-gallery--single-column" id="gallery--news" style="display: flex; gap: 20px; flex-wrap: wrap; justify-content: center;">
    <a href="/assets/img/news/logo_SAK_15.PNG"
      target="_blank">
      <img src="/assets/img/news/logo_SAK_15.PNG" 
           alt="Comapny Logo" 
           />
</a>

  </div>
</div>

<h1> 21st March 2025 </h1>

<h1> Every 0.2 seconds on Earth, someone downloads a small language model </h1>

15M times small language models were downloaded from Hugging Face in the last month. How different are these models from each other?

The recent addition to SLM is Gemma-3 (1B to 27B). The new generation of models has a bigger context of 128K tokens. An increased context window size requires more computing and memory. It was solved by Grouped-Query Attention mechanisms and an optimized KV-cache - the model uses 5 local attention layers for 1 global. Gemma-3 is trained on 14T tokens, including text and images. For text data, the model uses the same tokenizer as Gemini 2.0 with a 256K vocabulary size. The SigLIP encoder encodes images. The encoder segments images into non-overlapping crops and uses the P&S method for different aspect ratios. An interesting solution is to keep the image encoder frozen during training; it saves some resources on pre-training of the model, as we can pre-compute embeddings of images. The authors produced different quantized models focusing on open-source inference engines such as llama.cpp.

The Qwen2.5 (0.5B to 72B) model is trained on 18T tokens. It uses GQ attention as Gemma. Vocabulary size is 151K tokens and uses Qwen's tokenizer. Control tokens were expanded from 3 to 22 to support tool-calling functionality and structure output. The model developers use the scaling law to predict future model performance and determine its optimal parameters, such as model size for a given compute budget. In the Qwen paper, it's used for Hyper-parameters, such as batch size and learning rate.

In Dec 2024, the new version of Phi model was released Phi-4 (14B). Phi models are built focusing on quality synthetic data. Every new generation improves the data creation pipeline. The pipeline includes multi-agent prompting, self-revision workflows, and instruction reversal. Additional efforts were spent on preventing data contamination(leaking of benchmarks into training data). The paper describes a hybrid n-gram algorithm that uses 13- and 7-gram features to detect data from benchmarks. The total size of pre-trained data is 10T tokens. The context length of the model is 16K tokens, which is achieved via extending the default context 4K during midtraining. An interesting aspect of the model post-training is the Pivotal Token Search. The idea is based on probabilities that LLM produce during response generation. We can calculate the difference between before and after one token generation. Tokens which impact the overall probability the most are pivotal or crucial decision-making points. So when we do post-training, we should target such pivotal tokens to train the model to make better decisions.

There are more than 70 open-source small language models on the market, and the upper bound of model size is slightly growing. By today's standards, GPT-2 is SLM. However, a few things stay the same: focus on quality data and inference efficiency.


<div style="display: flex; justify-content: center; align-items: center;">
  <div class="pswp-gallery pswp-gallery--single-column" id="gallery--news" style="display: flex; gap: 10px; flex-wrap: wrap; justify-content: center;">
    <a href="/assets/img/news/AI news/1.jpg"

      target="_blank">
      <img src="/assets/img/news/AI news/1.jpg" 
           alt="Nyx" 
          />
</a>
  </div>
</div>

<hr>
<hr>

<h1> QuantumBlack, AI by McKinsey has released an interesting State of AI in 2025 </h1>

Report packed with interesting statistics. I totally agree with their frame "The value of AI comes from rewiring how companies run." That's the imperative for this year and beyond.

Some of the particularly interesting statistics:

ğŸ’° Over half of respondents using generative AI report cost reductions in the business units where itâ€™s deployed.

 ğŸ“‰ More than 80% do not yet see a material enterprise-wide EBIT effect from generative AI, despite functional gains.

 âš™ï¸ 21% of organizations that have adopted generative AI have fundamentally redesigned some workflows.

 âœ… 71% regularly use generative AI in at least one function, a jump from 65% in early 2024.

 ğŸ” 78% overall use AI in at least one function, up from 55% a year earlier.

 â˜‘ï¸ 27% review all AI outputs; a similar share checks 20% or less, highlighting wide variation in quality control.

 ğŸ‘¤ 28% say their CEO oversees AI governance, while 17% name their board of directors.

 âš ï¸ Many report mitigating inaccuracy, cybersecurity, and IP risksâ€”cited most often as having caused negative impacts.

 âœï¸ 63% of those using generative AI create text, over one-third generate images, and more than one-quarter produce code.

 ğŸ”’ 13% have hired AI compliance specialists, and 6% have hired AI ethics specialists in the past 12 months.

 ğŸ§‘â€ğŸ’» Fewer respondents call AI hiring â€œvery difficultâ€ compared with previous years, but data scientists remain in high demand.

 ğŸ”„ Most organizations have reskilled employees in the past year due to AI; more reskilling is planned over the next three years.

 ğŸ‘¥ 38% expect little net workforce change from generative AI; service operations and supply chain see the largest reduction risk, while IT and product development may expand.

Full report can be accessed at: https://www.mckinsey.com/capabilities/quantumblack/our-insights/the-state-of-ai#/
<div style="display: flex; justify-content: center; align-items: center;">
  <div class="pswp-gallery pswp-gallery--single-column" id="gallery--news" style="display: flex; gap: 10px; flex-wrap: wrap; justify-content: center;">
    <a href="/assets/img/news/AI news/2.jpg"

      target="_blank">
      <img src="/assets/img/news/AI news/2.jpg" 
           alt="Nyx" 
          />
</a>

  </div>
</div>

<hr>
<hr>

<h1>ğŸ–²ï¸ğŸ–²ï¸Visual Geometry Grounded TransformerğŸ–²ï¸ğŸ–²ï¸ </h1>

ğŸ‘‰VGGT by VGG & META (CVPR2025) is a feed-forward neural net. that directly infers all key 3D attributes of a scene, including extrinsic/intrinsic cam-params, point maps, depth maps, and 3D point tracks, from one, a few, or hundreds of its views, within seconds. Code released under NC 4.0ğŸ’™

ğ‡ğ¢ğ ğ¡ğ¥ğ¢ğ ğ¡ğ­ğ¬:

âœ…VGGT: Visual Geometry Grounded Transformer

âœ…Large feed-fwd transformer for 3D attributes

âœ…Intr/extrinsics, points, depth & 3D point tracks

âœ…SOTA, better than methods w/ post-processing


ğŸ‘‰Paper https://arxiv.org/pdf/2503.11651

ğŸ‘‰Project https://vgg-t.github.io/

ğŸ‘‰Code https://github.com/facebookresearch/vggt

<div style="display: flex; justify-content: center; align-items: center;">
  <div class="pswp-gallery pswp-gallery--single-column" id="gallery--news" style="display: flex; gap: 20px; flex-wrap: wrap; justify-content: center;">
    <a href="/assets/img/news/AI news/3.jpg"

      target="_blank">
      <img src="/assets/img/news/AI news/3.jpg" 
           alt="Nyx" 
          />
</a>

  </div>
</div>

<hr>
<hr>

<h1> ğŸš€ **Mistral Small 3.1: A Game-Changing Open-Source LLM** ğŸš€ </h1>

Wow! Mistral just dropped a 24B SOTA Multilingual, Multimodal LLM with 128K context AND Apache 2.0 license ğŸ”¥

In the fast-paced world of AI, Mistral has released **Mistral Small 3.1**, a powerful open-source model that outperforms many industry leaders. At 24B parameters, it runs efficiently on an RTX 4090 or a Mac with 32GB RAM, making it a lightweight yet robust solution for developers and enthusiasts.

ğŸ’¡ **Key Features:**

âœ… **Multimodal & Multilingual:** Seamlessly handles text, images, and 21+ languages.

âœ… **Fast & Efficient:** Processes 150 tokens per second with a 128K context window.

âœ… **Apache 2.0 License:** Fully open-source for use, fine-tuning, and integration.

ğŸ“Š **Performance:**

Mistral Small 3.1 excels in:

- **Math & Logical Reasoning**

- **Programming & Code Generation**

- **Multimodal Image Understanding**

- **Long-Context Retention**

Ideal for AI applications, multimodal projects, or local AI setups, this model stands out in the open-source AI landscape.

HF Access: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503


<div style="display: flex; justify-content: center; align-items: center;">
  <div class="pswp-gallery pswp-gallery--single-column" id="gallery--news" style="display: flex; gap: 20px; flex-wrap: wrap; justify-content: center;">
    <a href="/assets/img/news/AI news/4.jpg"
      target="_blank">
      <img src="/assets/img/news/AI news/4.jpg" 
           alt="ClaudeCode" 
           />
</a>

  </div>
</div>

<hr>
<hr>

<h1> Hugging Face just dropped SmolDocling </h1>

ğŸš€ We just dropped ğ—¦ğ—ºğ—¼ğ—¹ğ——ğ—¼ğ—°ğ—¹ğ—¶ğ—»ğ—´: a ğŸ®ğŸ±ğŸ²ğ—  ğ—¼ğ—½ğ—²ğ—»-ğ˜€ğ—¼ğ˜‚ğ—¿ğ—°ğ—² ğ˜ƒğ—¶ğ˜€ğ—¶ğ—¼ğ—» ğ—Ÿğ—  for complete document OCR! âœ¨

ğŸ“„ ğ—˜ğ—»ğ—±-ğ˜ğ—¼-ğ—²ğ—»ğ—± ğ—±ğ—¼ğ—°ğ˜‚ğ—ºğ—²ğ—»ğ˜ ğ—°ğ—¼ğ—»ğ˜ƒğ—²ğ—¿ğ˜€ğ—¶ğ—¼ğ—»â€”no more complex pipelines, just one tiny model

âš¡ ğ—™ğ—®ğ˜€ğ˜ & ğ—¹ğ—¶ğ—´ğ—µğ˜ğ˜„ğ—²ğ—¶ğ—´ğ—µğ˜â€”processes a page in 0.35 sec on a consumer GPU with <500MB VRAM

ğŸ† ğ—¦ğ—¢ğ—§ğ—” ğ—®ğ—°ğ—°ğ˜‚ğ—¿ğ—®ğ—°ğ˜†â€”outperforms models 27Ã— larger in full-page transcription, layout detection, and code recognition

ğŸ’¾ Efficient large-batch processingâ€”cheap and easy to run in-house

ğŸ“Š Handles all document elementsâ€”tables, charts, code, equations, lists, and more

ğŸ” Full tech report available with release

This is another example that small, optimized models can compete with much larger systemsâ€”making AI more sustainable.

Model: https://huggingface.co/ds4sd/SmolDocling-256M-preview

Paper: https://arxiv.org/abs/2503.11576

Code: https://github.com/docling-project/docling

<div style="display: flex; justify-content: center; align-items: center;">
  <div class="pswp-gallery pswp-gallery--single-column" id="gallery--news" style="display: flex; gap: 20px; flex-wrap: wrap; justify-content: center;">
    <iframe width="560" height="315" src="https://www.youtube.com/embed/7oi5H9M4gfE?si=NXcIkGvE1BKWCVjB" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>


  </div>
</div>

<hr>
<hr>

<h1> New RL Method thats better than GRPO!  </h1>

New RL Method thats better than GRPO! ğŸ¤¯ ByteDance released a new open source RL method that outperforms GRPO ğŸ‘€ DAPO or Decoupled Clip and Dynamic sAmpling Policy Optimization (DAPO) achieves 50 points on the AIME 2024 with 50% fewer training steps.

TL;DR: 

ğŸ† 50% AIME 2024 accuracy (Qwen2.5-32B), surpassing DeepSeek-R1 with 50% fewer steps.

ğŸ¤— Fully open-source: code (based on verl), training dataset (DAPO-Math-17k), and model weights (soon)

ğŸ’¡ Clip-Higher: Asymmetric clipping bounds with higher upper bound to prevent entropy collapse

ğŸ”„ Dynamic Sampling: Filters out prompts with 0% or 100% accuracy

ğŸ” Token-level Policy Gradient Loss: Prevents excessive response lengths and maintains reasoning quality

ğŸ“ Length-aware penalty: Reduce reward noise for truncated, but potentially valid, long responses.

âœ… Uses simple, robust rule-based verifier based on string normalization and matching

ğŸ› ï¸ Compared to GRPO: No KL divergence penalty, token-level loss (vs sample-level), asymmetric clip ranges and filtering.

ğŸ¥‡Qwen2.5-32B DAPO achieves 50 points on AIME vs. GRPO's 30 points

âš ï¸ Release excludes Dynamic Sampling in scripts (44% AIME)

project page: https://dapo-sia.github.io/

code: https://github.com/BytedTsinghua-SIA/DAPO

paper: https://huggingface.co/papers/2503.14476

<div style="display: flex; justify-content: center; align-items: center;">
  <div class="pswp-gallery pswp-gallery--single-column" id="gallery--news" style="display: flex; gap: 20px; flex-wrap: wrap; justify-content: center;">
    <a href="/assets/img/news/AI news/6.jpg"
      target="_blank">
      <img src="/assets/img/news/AI news/6.jpg" 
           alt="BirdSQL" 
           />
</a>

  </div>
</div>

<hr>
<hr>

<h1> Robots finally got real! </h1>

Jensen Huang just introduced Blue (Star Wars droid) and NVIDIA's partnership with DeepMind and Disney ğŸ‘ğŸ¥¹

At Nvidia's GTC 2025, CEO Jensen Huang introduced a droid straight out of your Star Wars dreams ğŸ¤–

Blue isn't just a prop - it's an interactive droid designed to:

- Walk and move fluidly.

- Engage naturally with humans.

- Bring Star Wars to life, right before your eyes.

To achieve that, three giants joined forces:

â†³ NVIDIAâ€™s expertise in AI and computing.

â†³ Google DeepMindâ€™s advancements in machine learning.

â†³ Disneyâ€™s creative storytelling.

They're also launching Newton, an open-source physics engine. It's a digital playground where robots learn real-world skills through lifelike simulations.

The future is here, and it's amazing.

<div style="display: flex; justify-content: center; align-items: center;">
  <div class="pswp-gallery pswp-gallery--single-column" id="gallery--news" style="display: flex; gap: 20px; flex-wrap: wrap; justify-content: center;">
   <iframe width="560" height="315" src="https://www.youtube.com/embed/YxH4Mx6zh6c?si=CwG8J81JEBNGxOae" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

  </div>
</div>
