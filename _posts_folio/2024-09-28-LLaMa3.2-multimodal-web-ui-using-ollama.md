---
layout: post
title: User friendly LLaMa 3.2 Multimodal Web UI using Ollama
description: "User-friendly UI web user interface for recent LLaMa 3.2, a large language model LLM from Meta."
date: 2024-09-28 09:00:00-0400
tags: [large language model, llm, LLaMa, Ollama, Multimodal]
thumbnail: assets/img/blog/ollama_webui_icon.png
---


LLaMa 3.2 Multimodal Web UI is user-friendly interface for interacting with the Ollama platform's LLaMa 3.2 multimodal model. It effortlessly supports both text and image inputs, allowing users to ask questions, submit prompts, and receive responses in text, code, and even visual outputs, making the power of multimodal AI accessible to all.

This code is tested on Ubuntu.


## Features

- **Multimodal Input Support**: Submit text and image inputs to receive context-aware responses.
- **Formatted Responses**: Highlighted code blocks and detailed explanations for code outputs.
- **Copy-to-Clipboard**: Easily copy code snippets directly from the UI for seamless integration into your projects.
- **Responsive UI**: Optimized for both desktop and mobile devices.
- **Interactive Output**: View text, images, and other media types generated by Llama 3.2.

## Installation

To run CodeLlama Multimodal Web UI locally, follow these steps:

1. **Clone the repository**:

    ``
    git clone https://github.com/iamgmujtaba/llama3.2-webUI
    ``

2. **Navigate to the project directory**:

    ``
    cd llama3.2-webUI
    ``

3. **Ensure Ollama are installed** on your machine:
    - Install Ollama [here](https://ollama.com)

4. **Run the application** using the included shell script:

    ``
    bash run.sh
    ``

   This will start the local development server and automatically open the application in your default web browser at `http://localhost:8000`.

## Usage

### 1. Interacting with the UI

Once the interface loads in your browser, you can:

- **Submit Text Inputs**: Type your question or prompt in the input box and hit **Submit**.
- **Upload Images** (Coming soon): You will soon be able to upload images to receive both text and visual-based responses from the model.

### 2. Viewing Responses
- **Formatted Text**: Responses are displayed in a clean, readable format.
- **Highlighted Code**: Any code snippets returned are syntax-highlighted for better readability.
- **Multimodal Responses**: When image inputs are supported, you will also receive generated images, annotations, or detailed content explanations.


## Screenshots

{% include figure.liquid loading="eager" path="assets/img/blog/home_webui.png" class="img-fluid rounded z-depth-1" %}


{% include figure.liquid loading="eager" path="assets/img/blog/home_webui_res.png" class="img-fluid rounded z-depth-1" %}



## Contributing
Contributions are always welcome.


## License

This project is licensed under the MIT License. See the [LICENSE](LICENSE) file for more details.


---

_This project is maintained on GitHub. For more information, visit the [LLaMa 3.2 Multimodal Web UI GitHub repository](https://github.com/iamgmujtaba/llama3.2-webUI)._