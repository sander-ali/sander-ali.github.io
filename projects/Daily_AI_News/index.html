<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Daily AI news | Sunder Ali Khowaja </title> <meta name="author" content="Sunder Ali Khowaja"> <meta name="description" content="This page is dedicated to your daily AI news especially related to Agents, LLMs, and Agentic AI"> <meta name="keywords" content="Khowaja Sunder, Sunder Ali Khowaja, Sander Ali Khowaja, dr.Sunder Ali Khowaja, doctor sunder khowaja, sunder ali, sander ali, dr sunder ali contact number, IEEE Senior Member, sunder ali usindh, sander ali usindh, deep-learning, computer-vision, AI-researcher, machine-learning, model inversion attacks, medical-imaging, private ai, neural-networks, academic-publications, research-portfolio"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img//favicon.png?5db4cf704bc2caffc10b1d53e8fa8ffd"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://sander-ali.github.io/projects/Daily_AI_News/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Sunder Ali Khowaja </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">Blogs </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">Teaching </a> </li> <li class="nav-item "> <a class="nav-link" href="/updates/">News </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Daily AI news</h1> <p class="post-description">This page is dedicated to your daily AI news especially related to Agents, LLMs, and Agentic AI</p> </header> <article> <div style="display: flex; justify-content: center; align-items: center;"> <div class="pswp-gallery pswp-gallery--single-column" id="gallery--news" style="display: flex; gap: 20px; flex-wrap: wrap; justify-content: center;"> <a href="/assets/img/news/logo_SAK_15.PNG" target="_blank"> <img src="/assets/img/news/logo_SAK_15.PNG" alt="Comapny Logo"> </a> </div> </div> <h1> 21st March 2025 </h1> <h1> Every 0.2 seconds on Earth, someone downloads a small language model </h1> <p>15M times small language models were downloaded from Hugging Face in the last month. How different are these models from each other?</p> <p>The recent addition to SLM is Gemma-3 (1B to 27B). The new generation of models has a bigger context of 128K tokens. An increased context window size requires more computing and memory. It was solved by Grouped-Query Attention mechanisms and an optimized KV-cache - the model uses 5 local attention layers for 1 global. Gemma-3 is trained on 14T tokens, including text and images. For text data, the model uses the same tokenizer as Gemini 2.0 with a 256K vocabulary size. The SigLIP encoder encodes images. The encoder segments images into non-overlapping crops and uses the P&amp;S method for different aspect ratios. An interesting solution is to keep the image encoder frozen during training; it saves some resources on pre-training of the model, as we can pre-compute embeddings of images. The authors produced different quantized models focusing on open-source inference engines such as llama.cpp.</p> <p>The Qwen2.5 (0.5B to 72B) model is trained on 18T tokens. It uses GQ attention as Gemma. Vocabulary size is 151K tokens and uses Qwen’s tokenizer. Control tokens were expanded from 3 to 22 to support tool-calling functionality and structure output. The model developers use the scaling law to predict future model performance and determine its optimal parameters, such as model size for a given compute budget. In the Qwen paper, it’s used for Hyper-parameters, such as batch size and learning rate.</p> <p>In Dec 2024, the new version of Phi model was released Phi-4 (14B). Phi models are built focusing on quality synthetic data. Every new generation improves the data creation pipeline. The pipeline includes multi-agent prompting, self-revision workflows, and instruction reversal. Additional efforts were spent on preventing data contamination(leaking of benchmarks into training data). The paper describes a hybrid n-gram algorithm that uses 13- and 7-gram features to detect data from benchmarks. The total size of pre-trained data is 10T tokens. The context length of the model is 16K tokens, which is achieved via extending the default context 4K during midtraining. An interesting aspect of the model post-training is the Pivotal Token Search. The idea is based on probabilities that LLM produce during response generation. We can calculate the difference between before and after one token generation. Tokens which impact the overall probability the most are pivotal or crucial decision-making points. So when we do post-training, we should target such pivotal tokens to train the model to make better decisions.</p> <p>There are more than 70 open-source small language models on the market, and the upper bound of model size is slightly growing. By today’s standards, GPT-2 is SLM. However, a few things stay the same: focus on quality data and inference efficiency.</p> <div style="display: flex; justify-content: center; align-items: center;"> <div class="pswp-gallery pswp-gallery--single-column" id="gallery--news" style="display: flex; gap: 10px; flex-wrap: wrap; justify-content: center;"> <a href="/assets/img/news/AI%20news/1.jpg" target="_blank"> <img src="/assets/img/news/AI%20news/1.jpg" alt="Nyx"> </a> </div> </div> <hr> <hr> <h1> QuantumBlack, AI by McKinsey has released an interesting State of AI in 2025 </h1> <p>Report packed with interesting statistics. I totally agree with their frame “The value of AI comes from rewiring how companies run.” That’s the imperative for this year and beyond.</p> <p>Some of the particularly interesting statistics:</p> <p>💰 Over half of respondents using generative AI report cost reductions in the business units where it’s deployed.</p> <p>📉 More than 80% do not yet see a material enterprise-wide EBIT effect from generative AI, despite functional gains.</p> <p>⚙️ 21% of organizations that have adopted generative AI have fundamentally redesigned some workflows.</p> <p>✅ 71% regularly use generative AI in at least one function, a jump from 65% in early 2024.</p> <p>🔎 78% overall use AI in at least one function, up from 55% a year earlier.</p> <p>☑️ 27% review all AI outputs; a similar share checks 20% or less, highlighting wide variation in quality control.</p> <p>👤 28% say their CEO oversees AI governance, while 17% name their board of directors.</p> <p>⚠️ Many report mitigating inaccuracy, cybersecurity, and IP risks—cited most often as having caused negative impacts.</p> <p>✍️ 63% of those using generative AI create text, over one-third generate images, and more than one-quarter produce code.</p> <p>🔒 13% have hired AI compliance specialists, and 6% have hired AI ethics specialists in the past 12 months.</p> <p>🧑‍💻 Fewer respondents call AI hiring “very difficult” compared with previous years, but data scientists remain in high demand.</p> <p>🔄 Most organizations have reskilled employees in the past year due to AI; more reskilling is planned over the next three years.</p> <p>👥 38% expect little net workforce change from generative AI; service operations and supply chain see the largest reduction risk, while IT and product development may expand.</p> <p>Full report can be accessed at: https://www.mckinsey.com/capabilities/quantumblack/our-insights/the-state-of-ai#/</p> <div style="display: flex; justify-content: center; align-items: center;"> <div class="pswp-gallery pswp-gallery--single-column" id="gallery--news" style="display: flex; gap: 10px; flex-wrap: wrap; justify-content: center;"> <a href="/assets/img/news/AI%20news/2.jpg" target="_blank"> <img src="/assets/img/news/AI%20news/2.jpg" alt="Nyx"> </a> </div> </div> <hr> <hr> <h1>🖲️🖲️Visual Geometry Grounded Transformer🖲️🖲️ </h1> <p>👉VGGT by VGG &amp; META (CVPR2025) is a feed-forward neural net. that directly infers all key 3D attributes of a scene, including extrinsic/intrinsic cam-params, point maps, depth maps, and 3D point tracks, from one, a few, or hundreds of its views, within seconds. Code released under NC 4.0💙</p> <p>𝐇𝐢𝐠𝐡𝐥𝐢𝐠𝐡𝐭𝐬:</p> <p>✅VGGT: Visual Geometry Grounded Transformer</p> <p>✅Large feed-fwd transformer for 3D attributes</p> <p>✅Intr/extrinsics, points, depth &amp; 3D point tracks</p> <p>✅SOTA, better than methods w/ post-processing</p> <p>👉Paper https://arxiv.org/pdf/2503.11651</p> <p>👉Project https://vgg-t.github.io/</p> <p>👉Code https://github.com/facebookresearch/vggt</p> <div style="display: flex; justify-content: center; align-items: center;"> <div class="pswp-gallery pswp-gallery--single-column" id="gallery--news" style="display: flex; gap: 20px; flex-wrap: wrap; justify-content: center;"> <a href="/assets/img/news/AI%20news/3.jpg" target="_blank"> <img src="/assets/img/news/AI%20news/3.jpg" alt="Nyx"> </a> </div> </div> <hr> <hr> <h1> 🚀 **Mistral Small 3.1: A Game-Changing Open-Source LLM** 🚀 </h1> <p>Wow! Mistral just dropped a 24B SOTA Multilingual, Multimodal LLM with 128K context AND Apache 2.0 license 🔥</p> <p>In the fast-paced world of AI, Mistral has released <strong>Mistral Small 3.1</strong>, a powerful open-source model that outperforms many industry leaders. At 24B parameters, it runs efficiently on an RTX 4090 or a Mac with 32GB RAM, making it a lightweight yet robust solution for developers and enthusiasts.</p> <p>💡 <strong>Key Features:</strong></p> <p>✅ <strong>Multimodal &amp; Multilingual:</strong> Seamlessly handles text, images, and 21+ languages.</p> <p>✅ <strong>Fast &amp; Efficient:</strong> Processes 150 tokens per second with a 128K context window.</p> <p>✅ <strong>Apache 2.0 License:</strong> Fully open-source for use, fine-tuning, and integration.</p> <p>📊 <strong>Performance:</strong></p> <p>Mistral Small 3.1 excels in:</p> <ul> <li> <p><strong>Math &amp; Logical Reasoning</strong></p> </li> <li> <p><strong>Programming &amp; Code Generation</strong></p> </li> <li> <p><strong>Multimodal Image Understanding</strong></p> </li> <li> <p><strong>Long-Context Retention</strong></p> </li> </ul> <p>Ideal for AI applications, multimodal projects, or local AI setups, this model stands out in the open-source AI landscape.</p> <p>HF Access: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503</p> <div style="display: flex; justify-content: center; align-items: center;"> <div class="pswp-gallery pswp-gallery--single-column" id="gallery--news" style="display: flex; gap: 20px; flex-wrap: wrap; justify-content: center;"> <a href="/assets/img/news/AI%20news/4.jpg" target="_blank"> <img src="/assets/img/news/AI%20news/4.jpg" alt="ClaudeCode"> </a> </div> </div> <hr> <hr> <h1> Hugging Face just dropped SmolDocling </h1> <p>🚀 We just dropped 𝗦𝗺𝗼𝗹𝗗𝗼𝗰𝗹𝗶𝗻𝗴: a 𝟮𝟱𝟲𝗠 𝗼𝗽𝗲𝗻-𝘀𝗼𝘂𝗿𝗰𝗲 𝘃𝗶𝘀𝗶𝗼𝗻 𝗟𝗠 for complete document OCR! ✨</p> <p>📄 𝗘𝗻𝗱-𝘁𝗼-𝗲𝗻𝗱 𝗱𝗼𝗰𝘂𝗺𝗲𝗻𝘁 𝗰𝗼𝗻𝘃𝗲𝗿𝘀𝗶𝗼𝗻—no more complex pipelines, just one tiny model</p> <p>⚡ 𝗙𝗮𝘀𝘁 &amp; 𝗹𝗶𝗴𝗵𝘁𝘄𝗲𝗶𝗴𝗵𝘁—processes a page in 0.35 sec on a consumer GPU with &lt;500MB VRAM</p> <p>🏆 𝗦𝗢𝗧𝗔 𝗮𝗰𝗰𝘂𝗿𝗮𝗰𝘆—outperforms models 27× larger in full-page transcription, layout detection, and code recognition</p> <p>💾 Efficient large-batch processing—cheap and easy to run in-house</p> <p>📊 Handles all document elements—tables, charts, code, equations, lists, and more</p> <p>🔍 Full tech report available with release</p> <p>This is another example that small, optimized models can compete with much larger systems—making AI more sustainable.</p> <p>Model: https://huggingface.co/ds4sd/SmolDocling-256M-preview</p> <p>Paper: https://arxiv.org/abs/2503.11576</p> <p>Code: https://github.com/docling-project/docling</p> <div style="display: flex; justify-content: center; align-items: center;"> <div class="pswp-gallery pswp-gallery--single-column" id="gallery--news" style="display: flex; gap: 20px; flex-wrap: wrap; justify-content: center;"> <iframe width="560" height="315" src="https://www.youtube.com/embed/7oi5H9M4gfE?si=NXcIkGvE1BKWCVjB" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe> </div> </div> <hr> <hr> <h1> New RL Method thats better than GRPO! </h1> <p>New RL Method thats better than GRPO! 🤯 ByteDance released a new open source RL method that outperforms GRPO 👀 DAPO or Decoupled Clip and Dynamic sAmpling Policy Optimization (DAPO) achieves 50 points on the AIME 2024 with 50% fewer training steps.</p> <p>TL;DR:</p> <p>🏆 50% AIME 2024 accuracy (Qwen2.5-32B), surpassing DeepSeek-R1 with 50% fewer steps.</p> <p>🤗 Fully open-source: code (based on verl), training dataset (DAPO-Math-17k), and model weights (soon)</p> <p>💡 Clip-Higher: Asymmetric clipping bounds with higher upper bound to prevent entropy collapse</p> <p>🔄 Dynamic Sampling: Filters out prompts with 0% or 100% accuracy</p> <p>🔍 Token-level Policy Gradient Loss: Prevents excessive response lengths and maintains reasoning quality</p> <p>📏 Length-aware penalty: Reduce reward noise for truncated, but potentially valid, long responses.</p> <p>✅ Uses simple, robust rule-based verifier based on string normalization and matching</p> <p>🛠️ Compared to GRPO: No KL divergence penalty, token-level loss (vs sample-level), asymmetric clip ranges and filtering.</p> <p>🥇Qwen2.5-32B DAPO achieves 50 points on AIME vs. GRPO’s 30 points</p> <p>⚠️ Release excludes Dynamic Sampling in scripts (44% AIME)</p> <p>project page: https://dapo-sia.github.io/</p> <p>code: https://github.com/BytedTsinghua-SIA/DAPO</p> <p>paper: https://huggingface.co/papers/2503.14476</p> <div style="display: flex; justify-content: center; align-items: center;"> <div class="pswp-gallery pswp-gallery--single-column" id="gallery--news" style="display: flex; gap: 20px; flex-wrap: wrap; justify-content: center;"> <a href="/assets/img/news/AI%20news/6.jpg" target="_blank"> <img src="/assets/img/news/AI%20news/6.jpg" alt="BirdSQL"> </a> </div> </div> <hr> <hr> <h1> Robots finally got real! </h1> <p>Jensen Huang just introduced Blue (Star Wars droid) and NVIDIA’s partnership with DeepMind and Disney 👏🥹</p> <p>At Nvidia’s GTC 2025, CEO Jensen Huang introduced a droid straight out of your Star Wars dreams 🤖</p> <p>Blue isn’t just a prop - it’s an interactive droid designed to:</p> <ul> <li> <p>Walk and move fluidly.</p> </li> <li> <p>Engage naturally with humans.</p> </li> <li> <p>Bring Star Wars to life, right before your eyes.</p> </li> </ul> <p>To achieve that, three giants joined forces:</p> <p>↳ NVIDIA’s expertise in AI and computing.</p> <p>↳ Google DeepMind’s advancements in machine learning.</p> <p>↳ Disney’s creative storytelling.</p> <p>They’re also launching Newton, an open-source physics engine. It’s a digital playground where robots learn real-world skills through lifelike simulations.</p> <p>The future is here, and it’s amazing.</p> <div style="display: flex; justify-content: center; align-items: center;"> <div class="pswp-gallery pswp-gallery--single-column" id="gallery--news" style="display: flex; gap: 20px; flex-wrap: wrap; justify-content: center;"> <iframe width="560" height="315" src="https://www.youtube.com/embed/YxH4Mx6zh6c?si=CwG8J81JEBNGxOae" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe> </div> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0 text-left"> © Copyright 2025 Sunder Ali Khowaja. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=UA-82147914-4"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","UA-82147914-4");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script>for(var wechatModal=document.getElementById("WeChatMod"),wechatBtn=document.querySelectorAll('[id="WeChatBtn"]'),i=0;i<wechatBtn.length;i++)wechatBtn[i].onclick=function(){wechatModal.style.display="block"};window.onclick=function(t){t.target==wechatModal&&(wechatModal.style.display="none")};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"About",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-cv",title:"CV",description:"Below is a simplified version of my resume. You can find a full version in the pdf. \ud83d\udc49",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"nav-publications",title:"Publications",description:"",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-blogs",title:"Blogs",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-teaching",title:"Teaching",description:"Courses and teaching activities at various institutions",section:"Navigation",handler:()=>{window.location.href="/teaching/"}},{id:"nav-news",title:"News",description:"",section:"Navigation",handler:()=>{window.location.href="/updates/"}},{id:"post-what-is-the-technical-debt-of-large-language-models-llms-and-how-does-it-affect-us",title:"What Is the Technical Debt of Large Language Models (LLMs) and How Does...",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/What-Is-the-Technical-Debt-of-Large-Language-Models-(LLMs)-and-How-Does-It-Affect-Us/"}},{id:"post-open-source-ai-taking-forward-leaps",title:"Open Source AI taking forward leaps",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/Open-Source-AI-taking-forward-leaps/"}},{id:"post-summarizing-youtube-videos-using-python-and-online-ai-tools",title:"Summarizing YouTube Videos using Python and Online AI Tools",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2023/Summarizing-YouTube-Videos-using-Python-and-Online-AI-Tools/"}},{id:"post-taking-a-deep-dive-into-yolov8",title:"Taking a Deep Dive into YOLOv8",description:"Details on YOLOv8",section:"Posts",handler:()=>{window.location.href="/blog/2023/Taking-a-Deep-Dive-into-YOLOv8/"}},{id:"post-brief-timeline-of-yolo-models",title:"Brief Timeline of YOLO models",description:"Brief Timeline of YOLO models from v1 to v8",section:"Posts",handler:()=>{window.location.href="/blog/2023/Brief-Timeline-of-YOLO-models-from-v1-to-v8/"}},{id:"post-combating-urban-flooding-with-internet-of-things-and-artificial-intelligence",title:"Combating Urban flooding with Internet of Things and Artificial Intelligence",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2020/Combating-Urban-flooding-with-Internet-of-Things-and-Artificial-Intelligence/"}},{id:"post-where-do-pakistan-stand-in-ai-race",title:"Where do Pakistan stand in AI race?",description:"A discussion on where Pakistan resides in terms of AI race.",section:"Posts",handler:()=>{window.location.href="/blog/2020/Where-do-Pakistan-stand-in-AI-race/"}},{id:"projects-best-paper-award-at-kdbc-2018",title:"Best Paper Award at KDBC 2018",description:"Our paper titled Human Action Recognition with Sequential Convolution and Recurrent Neural Networks Using 3D Skeleton Data, has won the best paper award in Korean Database Conference held at South Korea.",section:"Projects",handler:()=>{window.location.href="/projects/Best_Paper_Award_KDBC/"}},{id:"projects-first-runner-up-at-cvpr-2023-6th-ug2-challenge",title:"First Runner Up at CVPR 2023 6th UG2+ Challenge",description:"First Runner Up at CVPR 2023 6th UG2+ Challenge",section:"Projects",handler:()=>{window.location.href="/projects/UG2_FRU_Cert/"}},{id:"projects-second-runner-up-at-cvpr-2023-6th-ug2-challenge",title:"Second Runner Up at CVPR 2023 6th UG2+ Challenge",description:"Second Runner Up at CVPR 2023 6th UG2+ Challenge",section:"Projects",handler:()=>{window.location.href="/projects/UG2_SRU_Cert/"}},{id:"projects-ieee-tmi-distinguished-reviewer-bronze-level-2022-to-2023",title:"IEEE TMI Distinguished Reviewer Bronze Level 2022 to 2023",description:"IEEE TMI Distinguished Reviewer Bronze Level 2022 to 2023",section:"Projects",handler:()=>{window.location.href="/projects/TMI_reviewer/"}},{id:"projects-joined-as-ieee-consumer-technology-society-39-s-technical-committee-member",title:"Joined as IEEE Consumer Technology Society&#39;s Technical Committee Member",description:"Joined as IEEE Consumer Technology Society&#39;s Technical Committee Member",section:"Projects",handler:()=>{window.location.href="/projects/CTSoc_technical_member/"}},{id:"projects-best-paper-award-at-ieee-wcnc-2024",title:"Best Paper Award at IEEE WCNC 2024",description:"Our paper titled Zero-Trust Attack Framework with Split Learning for Autonomous Vehicles in 6G Networks, has won the best paper award at 25th IEEE Wireless Communications and Networking Conference held at Dubai, UAE",section:"Projects",handler:()=>{window.location.href="/projects/Best_Paper_Award_WCNC/"}},{id:"projects-serving-as-special-session-chair-at-ieee-gem-2024",title:"Serving as Special Session Chair at IEEE GEM 2024",description:"Call for papers in Special Session at IEEE Gaming, Entertainment and Media (GEM) conference, 2024",section:"Projects",handler:()=>{window.location.href="/projects/Chair_special_session_GEM/"}},{id:"projects-top-10-methods-at-cvpr-39-s-ntire-blind-compressed-image-enhancement-challenge",title:"Top 10 Methods at CVPR&#39;s NTIRE Blind Compressed Image Enhancement Challenge",description:"Our submission has been included in the top methods for NTIRE 2024 Blind Compressed Image Enhancement Challenge held at CVPR competitions.",section:"Projects",handler:()=>{window.location.href="/projects/NTIRE_BICE_Challenge/"}},{id:"projects-best-researcher-nomination-certificate",title:"Best Researcher Nomination Certificate",description:"Nomination Certificate for Early Career Researcher Award at Technological University (TU) Dublin, Ireland.",section:"Projects",handler:()=>{window.location.href="/projects/Best_researcher_nomination/"}},{id:"projects-daily-ai-news",title:"Daily AI news",description:"This page is dedicated to your daily AI news especially related to Agents, LLMs, and Agentic AI",section:"Projects",handler:()=>{window.location.href="/projects/Daily_AI_News/"}},{id:"teaching-machine-learning",title:"Machine Learning",description:"2024_Machine_Learning.",section:"Teaching",handler:()=>{window.location.href="/teaching/Machine-Learning/"}},{id:"teaching-object-oriented-programming-java",title:"Object Oriented Programming (Java)",description:"2024_Object_oriented_programming_java.",section:"Teaching",handler:()=>{window.location.href="/teaching/object-oriented-programming-java/"}},{id:"teaching-object-oriented-programming-python",title:"Object Oriented Programming (Python)",description:"2024_Object_oriented_programming_python.",section:"Teaching",handler:()=>{window.location.href="/teaching/object-oriented-programming-python/"}},{id:"teaching-systems-and-network-database-administration",title:"Systems and Network Database Administration",description:"2024_Systems_Database_Administration.",section:"Teaching",handler:()=>{window.location.href="/teaching/systems-database-administration/"}},{id:"teaching-python-for-data-management",title:"Python for Data Management",description:"2024_python_for_data_management.",section:"Teaching",handler:()=>{window.location.href="/teaching/python-for-data-management/"}},{id:"teaching-program-design",title:"Program Design",description:"2024_Program_Design.",section:"Teaching",handler:()=>{window.location.href="/teaching/program-design/"}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}],console.log(ninja.data);</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>