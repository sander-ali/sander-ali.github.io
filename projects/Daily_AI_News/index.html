<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Daily AI news | Sunder Ali Khowaja </title> <meta name="author" content="Sunder Ali Khowaja"> <meta name="description" content="This page is dedicated to your daily AI news especially related to Agents, LLMs, and Agentic AI"> <meta name="keywords" content="Khowaja Sunder, Sunder Ali Khowaja, Sander Ali Khowaja, dr.Sunder Ali Khowaja, doctor sunder khowaja, sunder ali, sander ali, dr sunder ali contact number, IEEE Senior Member, sunder ali usindh, sander ali usindh, deep-learning, computer-vision, AI-researcher, machine-learning, model inversion attacks, medical-imaging, private ai, neural-networks, academic-publications, research-portfolio"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img//favicon.png?5db4cf704bc2caffc10b1d53e8fa8ffd"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://sander-ali.github.io/projects/Daily_AI_News/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Sunder Ali Khowaja </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">Blogs </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">Teaching </a> </li> <li class="nav-item "> <a class="nav-link" href="/updates/">News </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Daily AI news</h1> <p class="post-description">This page is dedicated to your daily AI news especially related to Agents, LLMs, and Agentic AI</p> </header> <article> <div style="display: flex; justify-content: center; align-items: center;"> <div class="pswp-gallery pswp-gallery--single-column" id="gallery--news" style="display: flex; gap: 20px; flex-wrap: wrap; justify-content: center;"> <a href="/assets/img/news/logo_SAK_15.PNG" target="_blank"> <img src="/assets/img/news/logo_SAK_15.PNG" alt="Comapny Logo"> </a> </div> </div> <h1> 24th March 2025 </h1> <h1> ImageRAG: Transforming Satellite Imagery, Medical Imaging, and Climate Monitoring with AI ğŸŒğŸ”¬ğŸŒ¦ï¸ </h1> <p>Ultra-High-Resolution (UHR) images used in satellite imagery often exceeding 100,000 Ã— 100,000 pixels, overwhelm existing AI tools due to computational and memory limits.</p> <p>But we need detail in these images for tasks like:</p> <p>âŠ Semantic Segmentation: Identifying detailed regions.</p> <p>âŠ Object Detection: Pinpointing structures, vehicles, or natural features.</p> <p>âŠ Change Detection: Monitoring landscape evolution.</p> <p>ImageRAG solves these callenges.</p> <p>ï¹‹ï¹‹ï¹‹ï¹‹ï¹‹</p> <p>ã€‹ ğ—§ğ—µğ—² ğ—šğ—®ğ—ºğ—²-ğ—–ğ—µğ—®ğ—»ğ—´ğ—²ğ—¿: ğ—œğ—ºğ—®ğ—´ğ—²ğ—¥ğ—”ğ—š</p> <p>âŠ What is ImageRAG?</p> <ul> <li> <p>A training-free framework designed to tackle UHR image analysis.</p> </li> <li> <p>Integrates Retrieval-Augmented Generation (RAG) to selectively process and analyze crucial image portions.</p> </li> </ul> <p>âŠ Why itâ€™s revolutionary:</p> <ul> <li> <p>Efficiently handles vast image data without sacrificing detail.</p> </li> <li> <p>Focuses on small but critical details, boosting accuracy.</p> </li> <li> <p>Avoids costly model retraining.</p> </li> </ul> <p>ï¹‹ï¹‹ï¹‹ï¹‹ï¹‹ ã€‹ ğ—–ğ—¼ğ—¿ğ—² ğ—™ğ—²ğ—®ğ˜ğ˜‚ğ—¿ğ—²ğ˜€ ğ—¼ğ—³ ğ—œğ—ºğ—®ğ—´ğ—²ğ—¥ğ—”ğ—š</p> <p>âŠ Two-Path Retrieval System:</p> <p>âœ£ Fast Path: Quickly retrieves relevant image patches for straightforward queries.</p> <p>âœ£ Slow Path: Digs deeper into labeled databases when the fast path fails.</p> <p>âŠ Training-Free:</p> <ul> <li> <p>No need for additional annotations or retraining.</p> </li> <li> <p>Ready-to-use for diverse applications.</p> </li> </ul> <p>âŠ Adaptable and Modular:</p> <p>âœ£ Integrates with existing AI models and workflows.</p> <p>ï¹‹ï¹‹ï¹‹ï¹‹ï¹‹ ã€‹ ğ—›ğ—¼ğ˜„ ğ——ğ—¼ğ—²ğ˜€ ğ—œğ—ºğ—®ğ—´ğ—²ğ—¥ğ—”ğ—š ğ—ªğ—¼ğ—¿ğ—¸?</p> <p>âŠ Image Patch Division:</p> <ul> <li> <p>Splits large images into smaller, manageable patches.</p> </li> <li> <p>Retains context for seamless analysis.</p> </li> </ul> <p>âŠ Instruction Analyzing Module:</p> <ul> <li> <p>Extracts key phrases from user queries.</p> </li> <li> <p>Matches text with visual elements.</p> </li> </ul> <p>âŠ Text-Image and Image-Image Retrieval:</p> <ul> <li> <p>Finds the most relevant visual data for any given query.</p> </li> <li> <p>Combines satellite imagery with textual inputs for precise results.</p> </li> </ul> <p>ï¹‹ï¹‹ï¹‹ï¹‹ï¹‹ ã€‹ ğ—”ğ—½ğ—½ğ—¹ğ—¶ğ—°ğ—®ğ˜ğ—¶ğ—¼ğ—»ğ˜€: ğ—•ğ—²ğ˜†ğ—¼ğ—»ğ—± ğ—¦ğ—®ğ˜ğ—²ğ—¹ğ—¹ğ—¶ğ˜ğ—² ğ—œğ—ºğ—®ğ—´ğ—²ğ—¿ğ˜†</p> <p>ImageRAG works for:</p> <p>âŠ Medical Imaging:</p> <p>âœ£ Focusing on anomalies in high-resolution scans.</p> <p>âŠ Retail and Supply Chain:</p> <p>âœ£ Monitoring large-scale operations via aerial imagery.</p> <p>âŠ Climate Monitoring:</p> <p>âœ£ Analyzing environmental changes with precision.</p> <div style="display: flex; justify-content: center; align-items: center;"> <div class="pswp-gallery pswp-gallery--single-column" id="gallery--news" style="display: flex; gap: 10px; flex-wrap: wrap; justify-content: center;"> <a href="/assets/img/news/AI%20news/1.gif" target="_blank"> <img src="/assets/img/news/AI%20news/1.gif" alt="Nyx"> </a> </div> </div> <hr> <hr> <h1> ğ—”ğ—Ÿğ—Ÿğ—®ğ— -ğ—§ğ—µğ—¶ğ—»ğ—¸ğ—¶ğ—»ğ—´ - ğ—®ğ—» ğ—”ğ—¿ğ—®ğ—¯ğ—¶ğ—° ğ—¹ğ—®ğ—»ğ—´ğ˜‚ğ—®ğ—´ğ—² ğ—ºğ—¼ğ—±ğ—²ğ—¹ ğ—¼ğ—½ğ˜ğ—¶ğ—ºğ—¶ğ˜‡ğ—²ğ—± ğ—³ğ—¼ğ—¿ ğ—¿ğ—²ğ—®ğ˜€ğ—¼ğ—»ğ—¶ğ—»ğ—´ ğ—®ğ—»ğ—± ğ—ºğ—®ğ˜ğ—µğ—²ğ—ºğ—®ğ˜ğ—¶ğ—°ğ—®ğ—¹ ğ—½ğ—¿ğ—¼ğ—¯ğ—¹ğ—²ğ—º-ğ˜€ğ—¼ğ—¹ğ˜ƒğ—¶ğ—»ğ—´! </h1> <p>ALLaM-Thinking, a specialized Arabic Large Language Model that excels at step-by-step reasoning. The model is now available on both Ollama and Hugging Face platforms!</p> <p>ğŸ§  ğ—ğ—²ğ˜† ğ—™ğ—²ğ—®ğ˜ğ˜‚ğ—¿ğ—²ğ˜€:</p> <p>â€¢ Arabic-first design built specifically for high-quality Arabic text generation</p> <p>â€¢ Enhanced reasoning capabilities, particularly for mathematical problems</p> <p>â€¢ Step-by-step problem-solving methodology with clear explanations</p> <p>ğŸ“Š ğ—”ğ—Ÿğ—Ÿğ—®ğ— -ğ—§ğ—µğ—¶ğ—»ğ—¸ğ—¶ğ—»ğ—´ ğ—¯ğ—¿ğ—²ğ—®ğ—¸ğ˜€ ğ—±ğ—¼ğ˜„ğ—» ğ—°ğ—¼ğ—ºğ—½ğ—¹ğ—²ğ˜… ğ—½ğ—¿ğ—¼ğ—¯ğ—¹ğ—²ğ—ºğ˜€ ğ—ºğ—²ğ˜ğ—µğ—¼ğ—±ğ—¶ğ—°ğ—®ğ—¹ğ—¹ğ˜†, ğ—±ğ—²ğ—ºğ—¼ğ—»ğ˜€ğ˜ğ—¿ğ—®ğ˜ğ—¶ğ—»ğ—´ ğ˜€ğ˜ğ—¿ğ—¼ğ—»ğ—´ ğ—°ğ—®ğ—½ğ—®ğ—¯ğ—¶ğ—¹ğ—¶ğ˜ğ—¶ğ—²ğ˜€ ğ—¶ğ—»:</p> <p>â€¢ Mathematical reasoning with detailed explanations</p> <p>â€¢ Logical analysis and deduction</p> <p>â€¢ Maintaining coherence in complex technical responses</p> <p>This model is built upon ALLaM-AI/ALLaM-7B-Instruct-preview and implements Group Relative Policy Optimization (GRPO) for improved alignment.</p> <p>âœ… ğ—§ğ—¿ğ˜† ğ—¶ğ˜ ğ˜†ğ—¼ğ˜‚ğ—¿ğ˜€ğ—²ğ—¹ğ—³:</p> <p>â€¢ On Ollama: <code class="language-plaintext highlighter-rouge">ollama pull almaghrabima/ALLaM-Thinking</code> or visit https://ollama.com/almaghrabima/ALLaM-Thinking</p> <p>â€¢ On Hugging Face: https://huggingface.co/almaghrabima/ALLaM-Thinking</p> <div style="display: flex; justify-content: center; align-items: center;"> <div class="pswp-gallery pswp-gallery--single-column" id="gallery--news" style="display: flex; gap: 10px; flex-wrap: wrap; justify-content: center;"> <a href="/assets/img/news/AI%20news/2.jpg" target="_blank"> <img src="/assets/img/news/AI%20news/2.jpg" alt="Nyx"> </a> </div> </div> <hr> <hr> <h1>AI just changed startups forever </h1> <p>Y Combinatorâ€™s newest batch of startups is breaking every record.</p> <p>Hereâ€™s whatâ€™s happening:</p> <p>â†’ 25% of startups wrote 95% of their code using AI.</p> <p>â†’ Teams of fewer than 10 people are hitting $10M+ revenue.</p> <p>â†’ Theyâ€™re growing at 10% per week. Not just one or two - the entire batch.</p> <p>The era of needing 100 engineers and huge funding rounds? Gone.</p> <p>AI-powered â€œvibe codingâ€ means smaller teams, less capital, faster growth.</p> <p>Silicon Valleyâ€™s â€œgrowth at all costsâ€ mindset is officially dead.</p> <p>Profitability and lean teams are the new standard.</p> <p>Forget landing that job at Google, Amazon, Apple or Meta. You might find it easier &amp; more enjoyable to build the next $100M AI startup instead.</p> <p>Because the playing field just leveled.</p> <p>All thanks to AI.</p> <div style="display: flex; justify-content: center; align-items: center;"> <div class="pswp-gallery pswp-gallery--single-column" id="gallery--news" style="display: flex; gap: 20px; flex-wrap: wrap; justify-content: center;"> <a href="/assets/img/news/AI%20news/3.jpg" target="_blank"> <img src="/assets/img/news/AI%20news/3.jpg" alt="Nyx"> </a> </div> </div> <hr> <hr> <h1> Kyutai just released MoshiVis - an end-to-end low-latency Vision Speech Model, CC-BY license ğŸ”¥ </h1> <blockquote> <p>Only adds 206M parameters via lightweight cross-attention (CA) modules to integrate visual inputs from a frozen PaliGemma2-3B-448 vision encoder</p> </blockquote> <blockquote> <p>Uses a learnable gating mechanism in the CA modules allows MoshiVis to â€œturn offâ€ visual input streams when unnecessary, preserving Moshiâ€™s conversational abilities</p> </blockquote> <blockquote> <p>Adds only ~7ms per inference step on a MacMini with M4 Pro Chip, maintaining real-time performance</p> </blockquote> <blockquote> <p>Best part: it keeps the tone, emotion and the prosody of the original Moshi model</p> </blockquote> <blockquote> <p>CC-BY-4.0 licensed weights on the hub, allows commercial use</p> </blockquote> <blockquote> <p>Works with MLX, Candle and PyTorch from day-0</p> </blockquote> <div style="display: flex; justify-content: center; align-items: center;"> <div class="pswp-gallery pswp-gallery--single-column" id="gallery--news" style="display: flex; gap: 20px; flex-wrap: wrap; justify-content: center;"> <iframe width="560" height="315" src="https://www.youtube.com/embed/coroLWOS7II?si=bUbzE0COHvJOlX0H" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe> </div> </div> <hr> <hr> <h1> Langchain announces ğŸ”ğŸ“š Local Deep Research </h1> <p>An AI research assistant that runs locally, leveraging multiple LLMs for deep analysis across academic and web sources. Built with LangChain, it features RAG-powered search and flexible model support.</p> <p>Explore this powerful research tool on GitHub ğŸš€ https://github.com/LearningCircuit/local-deep-research</p> <div style="display: flex; justify-content: center; align-items: center;"> <div class="pswp-gallery pswp-gallery--single-column" id="gallery--news" style="display: flex; gap: 20px; flex-wrap: wrap; justify-content: center;"> <a href="/assets/img/news/AI%20news/5.jpg" target="_blank"> <img src="/assets/img/news/AI%20news/5.jpg" alt="Nyx"> </a> </div> </div> <hr> <hr> <h1> ğŸš€ OpenAI Releases Next-Gen Audio Models </h1> <p>OpenAI has just announced the launch of new speech-to-text and text-to-speech models, marking a significant update in voice AI capabilities.</p> <p>Key Highlights:</p> <p>1ï¸âƒ£Enhanced Accuracy: New speech-to-text models outperform existing solutions, especially in challenging scenarios.</p> <p>2ï¸âƒ£Customizable Voices: Developers can now instruct text-to-speech models on how to speak, enabling tailored experiences.</p> <p>3ï¸âƒ£Multilingual Support: Strong performance across over 100 languages ensures global reach.</p> <p>4ï¸âƒ£Technical Innovations: Advanced distillation techniques and pretraining on authentic audio datasets drive these advancements.</p> <p>These updates empower developers to build more accurate and expressive voice agents, enhancing user experiences.</p> <p>What practical applications do you see for these enhanced audio models in your industry?</p> <p>Video: OpenAI Developers (via 1 Zero)</p> <div style="display: flex; justify-content: center; align-items: center;"> <div class="pswp-gallery pswp-gallery--single-column" id="gallery--news" style="display: flex; gap: 20px; flex-wrap: wrap; justify-content: center;"> <iframe width="560" height="315" src="https://www.youtube.com/embed/sCrM4TBWLig?si=yOSVr7aIM3q8c1Mn" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe> </div> </div> <hr> <hr> <h1> ğŸ¥ğŸ¥ 3D LLM Spatial Understanding ğŸ¥ğŸ¥ </h1> <p>ğŸ‘‰Manycore unveils SpatialLM: novel LLM designed to process 3D point cloud data and generate structured 3D scene understanding outputs. Paper announced, code, model &amp; test data releasedğŸ’™</p> <p>ğ‡ğ¢ğ ğ¡ğ¥ğ¢ğ ğ¡ğ­ğ¬:</p> <p>âœ…Reasoning capabilities in complex scene</p> <p>âœ…Elements like walls, doors, windows</p> <p>âœ…Object BBoxes w/ semantic categories</p> <p>âœ…Point clouds from many diverse sources</p> <p>ğŸ‘‰Project https://manycore-research.github.io/SpatialLM/</p> <p>ğŸ‘‰Code https://github.com/manycore-research/SpatialLM</p> <p>ğŸ¤—Models https://huggingface.co/manycore-research</p> <div style="display: flex; justify-content: center; align-items: center;"> <div class="pswp-gallery pswp-gallery--single-column" id="gallery--news" style="display: flex; gap: 20px; flex-wrap: wrap; justify-content: center;"> <iframe width="560" height="315" src="https://manycore-research-azure.kujiale.com/manycore-research/SpatialLM/teaser.mp4"></iframe> </div> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0 text-left"> Â© Copyright 2025 Sunder Ali Khowaja. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=UA-82147914-4"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","UA-82147914-4");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script>for(var wechatModal=document.getElementById("WeChatMod"),wechatBtn=document.querySelectorAll('[id="WeChatBtn"]'),i=0;i<wechatBtn.length;i++)wechatBtn[i].onclick=function(){wechatModal.style.display="block"};window.onclick=function(t){t.target==wechatModal&&(wechatModal.style.display="none")};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"About",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-cv",title:"CV",description:"Below is a simplified version of my resume. You can find a full version in the pdf. \ud83d\udc49",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"nav-publications",title:"Publications",description:"",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-blogs",title:"Blogs",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-teaching",title:"Teaching",description:"Courses and teaching activities at various institutions",section:"Navigation",handler:()=>{window.location.href="/teaching/"}},{id:"nav-news",title:"News",description:"",section:"Navigation",handler:()=>{window.location.href="/updates/"}},{id:"post-what-is-the-technical-debt-of-large-language-models-llms-and-how-does-it-affect-us",title:"What Is the Technical Debt of Large Language Models (LLMs) and How Does...",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/What-Is-the-Technical-Debt-of-Large-Language-Models-(LLMs)-and-How-Does-It-Affect-Us/"}},{id:"post-open-source-ai-taking-forward-leaps",title:"Open Source AI taking forward leaps",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/Open-Source-AI-taking-forward-leaps/"}},{id:"post-summarizing-youtube-videos-using-python-and-online-ai-tools",title:"Summarizing YouTube Videos using Python and Online AI Tools",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2023/Summarizing-YouTube-Videos-using-Python-and-Online-AI-Tools/"}},{id:"post-taking-a-deep-dive-into-yolov8",title:"Taking a Deep Dive into YOLOv8",description:"Details on YOLOv8",section:"Posts",handler:()=>{window.location.href="/blog/2023/Taking-a-Deep-Dive-into-YOLOv8/"}},{id:"post-brief-timeline-of-yolo-models",title:"Brief Timeline of YOLO models",description:"Brief Timeline of YOLO models from v1 to v8",section:"Posts",handler:()=>{window.location.href="/blog/2023/Brief-Timeline-of-YOLO-models-from-v1-to-v8/"}},{id:"post-combating-urban-flooding-with-internet-of-things-and-artificial-intelligence",title:"Combating Urban flooding with Internet of Things and Artificial Intelligence",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2020/Combating-Urban-flooding-with-Internet-of-Things-and-Artificial-Intelligence/"}},{id:"post-where-do-pakistan-stand-in-ai-race",title:"Where do Pakistan stand in AI race?",description:"A discussion on where Pakistan resides in terms of AI race.",section:"Posts",handler:()=>{window.location.href="/blog/2020/Where-do-Pakistan-stand-in-AI-race/"}},{id:"projects-best-paper-award-at-kdbc-2018",title:"Best Paper Award at KDBC 2018",description:"Our paper titled Human Action Recognition with Sequential Convolution and Recurrent Neural Networks Using 3D Skeleton Data, has won the best paper award in Korean Database Conference held at South Korea.",section:"Projects",handler:()=>{window.location.href="/projects/Best_Paper_Award_KDBC/"}},{id:"projects-first-runner-up-at-cvpr-2023-6th-ug2-challenge",title:"First Runner Up at CVPR 2023 6th UG2+ Challenge",description:"First Runner Up at CVPR 2023 6th UG2+ Challenge",section:"Projects",handler:()=>{window.location.href="/projects/UG2_FRU_Cert/"}},{id:"projects-second-runner-up-at-cvpr-2023-6th-ug2-challenge",title:"Second Runner Up at CVPR 2023 6th UG2+ Challenge",description:"Second Runner Up at CVPR 2023 6th UG2+ Challenge",section:"Projects",handler:()=>{window.location.href="/projects/UG2_SRU_Cert/"}},{id:"projects-ieee-tmi-distinguished-reviewer-bronze-level-2022-to-2023",title:"IEEE TMI Distinguished Reviewer Bronze Level 2022 to 2023",description:"IEEE TMI Distinguished Reviewer Bronze Level 2022 to 2023",section:"Projects",handler:()=>{window.location.href="/projects/TMI_reviewer/"}},{id:"projects-joined-as-ieee-consumer-technology-society-39-s-technical-committee-member",title:"Joined as IEEE Consumer Technology Society&#39;s Technical Committee Member",description:"Joined as IEEE Consumer Technology Society&#39;s Technical Committee Member",section:"Projects",handler:()=>{window.location.href="/projects/CTSoc_technical_member/"}},{id:"projects-best-paper-award-at-ieee-wcnc-2024",title:"Best Paper Award at IEEE WCNC 2024",description:"Our paper titled Zero-Trust Attack Framework with Split Learning for Autonomous Vehicles in 6G Networks, has won the best paper award at 25th IEEE Wireless Communications and Networking Conference held at Dubai, UAE",section:"Projects",handler:()=>{window.location.href="/projects/Best_Paper_Award_WCNC/"}},{id:"projects-serving-as-special-session-chair-at-ieee-gem-2024",title:"Serving as Special Session Chair at IEEE GEM 2024",description:"Call for papers in Special Session at IEEE Gaming, Entertainment and Media (GEM) conference, 2024",section:"Projects",handler:()=>{window.location.href="/projects/Chair_special_session_GEM/"}},{id:"projects-top-10-methods-at-cvpr-39-s-ntire-blind-compressed-image-enhancement-challenge",title:"Top 10 Methods at CVPR&#39;s NTIRE Blind Compressed Image Enhancement Challenge",description:"Our submission has been included in the top methods for NTIRE 2024 Blind Compressed Image Enhancement Challenge held at CVPR competitions.",section:"Projects",handler:()=>{window.location.href="/projects/NTIRE_BICE_Challenge/"}},{id:"projects-best-researcher-nomination-certificate",title:"Best Researcher Nomination Certificate",description:"Nomination Certificate for Early Career Researcher Award at Technological University (TU) Dublin, Ireland.",section:"Projects",handler:()=>{window.location.href="/projects/Best_researcher_nomination/"}},{id:"projects-daily-ai-news",title:"Daily AI news",description:"This page is dedicated to your daily AI news especially related to Agents, LLMs, and Agentic AI",section:"Projects",handler:()=>{window.location.href="/projects/Daily_AI_News/"}},{id:"teaching-machine-learning",title:"Machine Learning",description:"2024_Machine_Learning.",section:"Teaching",handler:()=>{window.location.href="/teaching/Machine-Learning/"}},{id:"teaching-object-oriented-programming-java",title:"Object Oriented Programming (Java)",description:"2024_Object_oriented_programming_java.",section:"Teaching",handler:()=>{window.location.href="/teaching/object-oriented-programming-java/"}},{id:"teaching-object-oriented-programming-python",title:"Object Oriented Programming (Python)",description:"2024_Object_oriented_programming_python.",section:"Teaching",handler:()=>{window.location.href="/teaching/object-oriented-programming-python/"}},{id:"teaching-systems-and-network-database-administration",title:"Systems and Network Database Administration",description:"2024_Systems_Database_Administration.",section:"Teaching",handler:()=>{window.location.href="/teaching/systems-database-administration/"}},{id:"teaching-python-for-data-management",title:"Python for Data Management",description:"2024_python_for_data_management.",section:"Teaching",handler:()=>{window.location.href="/teaching/python-for-data-management/"}},{id:"teaching-program-design",title:"Program Design",description:"2024_Program_Design.",section:"Teaching",handler:()=>{window.location.href="/teaching/program-design/"}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}],console.log(ninja.data);</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>