<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://sander-ali.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://sander-ali.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-04-06T23:48:25+00:00</updated><id>https://sander-ali.github.io/feed.xml</id><title type="html">Sunder Ali Khowaja</title><subtitle>Dr. Sunder Ali Khowaja (Ph.D.) is IEEE Senior Member and a distinguished researcher in the field of Computer Vision, Deep Learning, Privacy Preservation Machine Learning, and Model Inversion Attacks. </subtitle><entry><title type="html">What Is the Technical Debt of Large Language Models (LLMs) and How Does It Affect Us</title><link href="https://sander-ali.github.io/blog/2024/What-Is-the-Technical-Debt-of-Large-Language-Models-(LLMs)-and-How-Does-It-Affect-Us/" rel="alternate" type="text/html" title="What Is the Technical Debt of Large Language Models (LLMs) and How Does It Affect Us"/><published>2024-08-10T13:00:00+00:00</published><updated>2024-08-10T13:00:00+00:00</updated><id>https://sander-ali.github.io/blog/2024/What-Is-the-Technical-Debt-of-Large-Language-Models-(LLMs)-and-How-Does-It-Affect-Us</id><content type="html" xml:base="https://sander-ali.github.io/blog/2024/What-Is-the-Technical-Debt-of-Large-Language-Models-(LLMs)-and-How-Does-It-Affect-Us/"><![CDATA[<figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/LLM_technical_debt-480.webp 480w,/assets/img/blog/LLM_technical_debt-800.webp 800w,/assets/img/blog/LLM_technical_debt-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/blog/LLM_technical_debt.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>As Large Language Models (LLMs) become increasingly central to the field of artificial intelligence, understanding their inherent technical debt is crucial. This debt impacts the maintainability, scalability, and effectiveness of LLMs, posing significant challenges. The efficient management of technical debt is vital for the sustainable development and deployment of AI technologies over the long run.</p> <p>There are several key areas associated with the accumulation of technical debt in LLMs:</p> <h3 id="model-complexity">Model Complexity</h3> <p>LLMs can contain billions of parameters, making both the initial model training process and subsequent model deployment highly complex and resource-intensive undertakings. Additionally, the computational requirements needed for model inference can lead to increased operational costs and issues with latency or response times at scale.</p> <h3 id="data-management">Data Management</h3> <p>Potential issues around data quality, biases present in training data, and the challenges of managing different versions of massive datasets over time can all negatively impact model performance, reproducibility of results, and the introduction of unintended biases.</p> <h3 id="training-and-hyperparameter-tuning">Training and Hyperparameter Tuning</h3> <p>The training of large language models requires vast computational resources, leading to substantial costs and potential environmental impacts. Additionally, the hyperparameter tuning process used to optimize model performance is inherently resource-intensive and time-consuming in nature.</p> <h3 id="deployment-and-scalability">Deployment and Scalability</h3> <p>The deployment of trained LLMs into robust, performant, and scalable production environments poses significant complexity. Ensuring low-latency and high-availability model serving capabilities at scale likely requires advanced distributed systems.</p> <h3 id="model-maintenance">Model Maintenance</h3> <p>Continuously monitoring models for issues like concept or lexical drift and performance degradation over time requires persistent vigilance. Additionally, keeping models up-to-date by retraining on new data introduces technical debt due to the resource demands of such efforts.</p> <h3 id="security-privacy-and-compliance">Security, Privacy, and Compliance</h3> <p>Ensuring data privacy, robustness to adversarial attacks, and overall compliance with regulations is an ongoing challenge. Preventing unauthorized data access and model manipulation is paramount.</p> <h3 id="interpretability-and-explainability">Interpretability and Explainability</h3> <p>The “black box” nature of complex LLMs makes understanding and explaining their decisions difficult, though explanations may be necessary for regulatory reasons or to ensure safe, fair, and transparent use.</p> <h3 id="ethical-considerations">Ethical Considerations</h3> <p>Addressing harms like unintended biases, lack of fairness, and potential for irresponsible or harmful applications requires careful management due to the consequential nature of decisions made by AI systems.</p> <h3 id="legacy-code-and-dependencies">Legacy Code and Dependencies</h3> <p>Technical debt accrues from reliance on outdated libraries and tools. Maintaining compatibility with evolving software ecosystems introduces complexity over time.</p> <p>Addressing these multifaceted areas of technical debt demands proactive, diligent, and well-resourced efforts such as regular audits and testing, ongoing monitoring, continued education of practitioners, and adaptation to emerging best practices. This can help ensure the long-term sustainability, reliability, and ethical deployment of LLMs.</p>]]></content><author><name></name></author><category term="LLM Debt"/><category term="LLM"/><category term="Model Complexity"/><category term="LLM Scalability"/><category term="LLM Complexity"/><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Open Source AI taking forward leaps</title><link href="https://sander-ali.github.io/blog/2024/Open-Source-AI-taking-forward-leaps/" rel="alternate" type="text/html" title="Open Source AI taking forward leaps"/><published>2024-07-22T13:00:00+00:00</published><updated>2024-07-22T13:00:00+00:00</updated><id>https://sander-ali.github.io/blog/2024/Open-Source-AI-taking-forward-leaps</id><content type="html" xml:base="https://sander-ali.github.io/blog/2024/Open-Source-AI-taking-forward-leaps/"><![CDATA[<p>Most progressive week in the history of Open Source AI (yet):</p> <ol> <li>Mistral (in collaboration with Nvidia) dropped Apache 2.0 licensed NeMo 12B LLM, better than L3 8B and Gemma 2 9B. Models are multilingual with 128K context and a highly efficient tokenizer — tekken.</li> </ol> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/Open_models_1-480.webp 480w,/assets/img/blog/Open_models_1-800.webp 800w,/assets/img/blog/Open_models_1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/blog/Open_models_1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ol> <li>Apple released DCLM 7B — truly open source LLM, based on OpenELM, trained on 2.5T tokens with 63.72 MMLU (better than Mistral 7B)</li> </ol> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/Open_models_2-480.webp 480w,/assets/img/blog/Open_models_2-800.webp 800w,/assets/img/blog/Open_models_2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/blog/Open_models_2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ol> <li>HF shared SmolLM — 135M, 360M, &amp; 1.7B Smol LMs capable of running directly in the browser; they beat Qwen 1.5B, Phi 1.5B and more. Trained on just 650B tokens.</li> </ol> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/Open_models_3-480.webp 480w,/assets/img/blog/Open_models_3-800.webp 800w,/assets/img/blog/Open_models_3-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/blog/Open_models_3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ol> <li>Groq put out Llama 3 8B &amp; 70B tool use &amp; function calling model checkpoints — achieves 90.76% accuracy on Berkely Function Calling Leaderboard (BFCL). Excels at API usage &amp; structured data manipulation!</li> </ol> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/Open_models_4-480.webp 480w,/assets/img/blog/Open_models_4-800.webp 800w,/assets/img/blog/Open_models_4-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/blog/Open_models_4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/Open_models_5-480.webp 480w,/assets/img/blog/Open_models_5-800.webp 800w,/assets/img/blog/Open_models_5-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/blog/Open_models_5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ol> <li>Salesforce released xLAM 1.35B &amp; 7B Large Action Models along with 60K instruction fine-tuning dataset. The 7B model scores 88.24% on BFCL &amp; 2B 78.94%</li> </ol> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/Open_models_6-480.webp 480w,/assets/img/blog/Open_models_6-800.webp 800w,/assets/img/blog/Open_models_6-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/blog/Open_models_6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/Open_models_7-480.webp 480w,/assets/img/blog/Open_models_7-800.webp 800w,/assets/img/blog/Open_models_7-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/blog/Open_models_7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/Open_models_8-480.webp 480w,/assets/img/blog/Open_models_8-800.webp 800w,/assets/img/blog/Open_models_8-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/blog/Open_models_8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/Open_models_9-480.webp 480w,/assets/img/blog/Open_models_9-800.webp 800w,/assets/img/blog/Open_models_9-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/blog/Open_models_9.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/Open_models_10-480.webp 480w,/assets/img/blog/Open_models_10-800.webp 800w,/assets/img/blog/Open_models_10-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/blog/Open_models_10.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ol> <li>Deepseek changed the game with v2 chat 0628 — The best open LLM on LYMSYS arena right now — 236B parameter model with 21B active parameters. It also excels at coding (rank #3) and arena hard problems (rank #3)</li> </ol> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/Open_models_11-480.webp 480w,/assets/img/blog/Open_models_11-800.webp 800w,/assets/img/blog/Open_models_11-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/blog/Open_models_11.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/Open_models_12-480.webp 480w,/assets/img/blog/Open_models_12-800.webp 800w,/assets/img/blog/Open_models_12-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/blog/Open_models_12.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/Open_models_13-480.webp 480w,/assets/img/blog/Open_models_13-800.webp 800w,/assets/img/blog/Open_models_13-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/blog/Open_models_13.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>There’s a lot more; Arcee (mergekit) released a series of LLMs, each better than the other, and Numina and HF Numina 72B (based on Qwen 2) and Math datasets, Mixbread with embedding models (english + german) and a lot more!</p>]]></content><author><name></name></author><category term="Open Source AI"/><category term="LLM"/><category term="Mistral AI"/><category term="Apple AI"/><category term="Salesforce"/><summary type="html"><![CDATA[Most progressive week in the history of Open Source AI (yet):]]></summary></entry><entry><title type="html">Summarizing YouTube Videos using Python and Online AI Tools</title><link href="https://sander-ali.github.io/blog/2023/Summarizing-YouTube-Videos-using-Python-and-Online-AI-Tools/" rel="alternate" type="text/html" title="Summarizing YouTube Videos using Python and Online AI Tools"/><published>2023-08-06T13:00:00+00:00</published><updated>2023-08-06T13:00:00+00:00</updated><id>https://sander-ali.github.io/blog/2023/Summarizing-YouTube-Videos-using-Python-and-Online-AI-Tools</id><content type="html" xml:base="https://sander-ali.github.io/blog/2023/Summarizing-YouTube-Videos-using-Python-and-Online-AI-Tools/"><![CDATA[<p>If you have a lot of videos to transcribe and summarize, using speech-to-text technology can be a time-consuming and tedious process. OpenAI’s Whisper technology has made it easier to transcribe your videos into an audio file, and then use ChatGPT’s advanced summarization algorithm to create concise summaries. Let’s take a look!</p> <h2 id="introduction-to-summarization-of-youtube-videos-using-google-chrome-extension-and-chatgpt">Introduction to Summarization of YouTube Videos using Google Chrome Extension and ChatGPT</h2> <p>ChatGPT is an impressive and powerful AI chatbot that is capable of effectively summarizing Youtube videos. The summarization process is quick and straightforward when utilizing ChatGPT. It is worth noting that the summarization process is even more efficient when a transcript of the video is available. However, the absence of a video transcript poses a challenge for ChatGPT.</p> <p>The Youtube Summary with ChatGPT Chrome extension is a valuable tool for summarizing Youtube videos. With the assistance of the AI chatbot ChatGPT, the extension can even transcribe any Youtube video. The extension simplifies the summarization process, allowing users to summarize any video with just a click of a button. By utilizing this extension, users can stay informed and up-to-date with current events. The extension saves time that would otherwise be spent watching videos, enhancing the overall Youtube experience.</p> <p>To install Youtube Summary with ChatGPT, follow these steps:</p> <ul> <li> <p>Install the Youtube Summary with ChatGPT code from GitHub.</p> </li> <li> <p>Unzip the file and open it in an IDE of your choice.</p> </li> <li> <p>In the terminal, run “npm install”.</p> </li> <li> <p>Run webpack to generate the ‘dist’ folder by running “npm run build” or “npm run build-release”.</p> </li> <li> <p>Open Chrome extensions and enable developer mode.</p> </li> <li> <p>Click on “Load unpacked” and select the “dist directory”.</p> </li> <li> <p>You can now install and run Youtube Summary with ChatGPT on your device.</p> </li> </ul> <h2 id="summarization-of-youtube-video-with-python-and-chatgpt">Summarization of YouTube video with Python and ChatGPT</h2> <p>If you find yourself fatigued from viewing numerous videos on YouTube and attempting to retain information from each one, utilizing ChatGPT to condense the content is the optimal solution for you. Following is a quick way to summarize YouTube Videos with speech-to-text API and ChatGPT technology.</p> <p>First, identify the video of your choice on Youtube.</p> <p>Create a python project to begin, you can use your own choice of IDE, it can be visual studio code, PyCharm or any other. We need to install a python package pytube [https://pytube.io/en/latest/]. Create a virtual environment and run the following command:</p> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip <span class="nb">install </span>pytube 
</code></pre></div></div> <p>In order to download the video of your choice from youtube, copy-paste the following code snippet and run it, accordingly.</p> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from pytube import YouTube

def ytvd<span class="o">(</span>url_s<span class="o">)</span>:
    YTObj <span class="o">=</span> YouTube<span class="o">(</span>url_s<span class="o">)</span>
    YTObj <span class="o">=</span> YTObj.streams.get_highest_resolution<span class="o">()</span>
    try:
        YTObj.download<span class="o">()</span>
    except:
        print<span class="o">(</span><span class="s2">"There was an error while downloading"</span><span class="o">)</span>
    print<span class="o">(</span><span class="s2">"Download is completed successfully"</span><span class="o">)</span>


url_s <span class="o">=</span> <span class="s2">"Paste your YouTube video link here"</span>
ytvd<span class="o">(</span>url_s<span class="o">)</span>
</code></pre></div></div> <p>The video will be now downloaded and saved to your device.</p> <h2 id="convert-video-to-audio">Convert Video to Audio</h2> <p>In order to perform video to audio conversion, we need another python package, i.e. moviepy.</p> <p>Install the package by running the following command.</p> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip <span class="nb">install </span>moviepy
</code></pre></div></div> <p>After installing, run the following code snippet to extract audio from the video.</p> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from moviepy.editor import <span class="k">*</span>
video_name <span class="o">=</span> <span class="s2">"Write the name of the video you have download"</span>
vid <span class="o">=</span> VideoFileClip<span class="o">(</span>f<span class="s2">"{video_name}.mp4"</span><span class="o">)</span>
vid.audio.write_audiofile<span class="o">(</span>f<span class="s2">"{video_name}.mp3"</span><span class="o">)</span>
</code></pre></div></div> <p>An audio file of the same name will be saved to your device.</p> <h2 id="convert-audio-to-text">Convert Audio to Text</h2> <p>With the progression of Generative AI, several tools are now available that can convert the audio file to its textual counterpart. However, in this article, we consider the use of Whisper API from OpenAI.</p> <p>In order to utilize Whisper API you need to visit the <a href="https://openai.com/index/openai-api/">website</a> and create an account. Whisper API provides $18 worth of credit to every new account as they have a Free Tier. For this demo, the number of credits obtained with the free account are enough, however, to be safe, you can check the usage of your credits on your personal page. The option for usage statistics will be available on the top right corner.</p> <p>You will require an API key to access Whisper API from python IDE. You can navigate to “API Keys” page from “User” tab and press generate a new key. From the settings section, you also need to retrieve the organization ID.</p> <p>Once you are done with the logistics of the keys and information available to you, you can proceed further.</p> <p>First, you need to install the OpenAI package by using the following command:</p> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip <span class="nb">install </span>openai
</code></pre></div></div> <p>Its recommended to keep the information acquired from OpenAI account to a json file, i.e. key.json, as shown below:</p> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">{</span>
    <span class="s2">"key"</span>: <span class="s2">"YOUR-PERSONAL-KEY-HERE"</span>,
    <span class="s2">"organization_id"</span> : <span class="s2">"YOUR-PERSONAL-ORGANIZATION-ID-HERE"</span>
<span class="o">}</span>
</code></pre></div></div> <p>The following code snippet reads the local mp3 file and calls OpenAI service to convert audio file into text. It should be noted that Whisper can directly convert video files to text as well, so you can try that feature as well.</p> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import openai
import json

aud_name <span class="o">=</span> <span class="s2">"The name of the audio file.mp3"</span>
<span class="nv">aud_file</span><span class="o">=</span> open<span class="o">(</span>aud_name, <span class="s2">"rb"</span><span class="o">)</span>

<span class="c"># Reading keys</span>
api_info_path <span class="o">=</span> <span class="s2">"key.json"</span>
with open<span class="o">(</span>api_info_path, <span class="s2">"r"</span><span class="o">)</span> as f:
    keys <span class="o">=</span> json.load<span class="o">(</span>f<span class="o">)</span>
whisp_key <span class="o">=</span> keys[<span class="s2">"key"</span><span class="o">]</span> 
org_id <span class="o">=</span> keys[<span class="s2">"organization_id"</span><span class="o">]</span> 

<span class="c"># Settings keys to the API module</span>
openai.organization <span class="o">=</span> org_id
openai.api_key <span class="o">=</span> whisp_key

print<span class="o">(</span><span class="s2">"Be Patient ... Converting your audio to text"</span><span class="o">)</span>
tp <span class="o">=</span> openai.Audio.transcribe<span class="o">(</span><span class="s2">"whisper-1"</span>, aud_file<span class="o">)</span>
txt <span class="o">=</span> tp[<span class="s2">"text"</span><span class="o">]</span>
print<span class="o">(</span>txt<span class="o">)</span>
</code></pre></div></div> <p>Once the text is available, it is easy to summarize it using ChatGPT, all you need to do is to run the following set of commands.</p> <h2 id="summarizing-the-text">Summarizing the Text</h2> <p>Copy the text you obtained from the previous step and paste it in place of {text}</p> <p>Code Snippet</p> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>print<span class="o">(</span><span class="s2">"Be Patient ... ChatGPT is summarizing your text"</span><span class="o">)</span>
response <span class="o">=</span> openai.ChatCompletion.create<span class="o">(</span>
<span class="nv">model</span><span class="o">=</span><span class="s2">"gpt-3.5-turbo"</span>,
<span class="nv">messages</span><span class="o">=[</span>
        <span class="o">{</span><span class="s2">"role"</span>: <span class="s2">"system"</span>, <span class="s2">"content"</span>: <span class="s2">"You are ChatGPT, a large language model trained by OpenAI. Answer as concisely as possible."</span><span class="o">}</span>,
        <span class="o">{</span><span class="s2">"role"</span>: <span class="s2">"user"</span>, <span class="s2">"content"</span>: f<span class="s2">"Summarize the following text: {text}"</span><span class="o">}</span>
    <span class="o">]</span>
<span class="o">)</span>

print<span class="o">(</span>response[<span class="s1">'choices'</span><span class="o">][</span>0][<span class="s1">'message'</span><span class="o">][</span><span class="s1">'content'</span><span class="o">])</span>
</code></pre></div></div> <p>and Voila ! the summarized text is ready.</p> <h2 id="text-summarization-tools">Text Summarization Tools</h2> <p>The last step uses ChatGPT to summarize the text, however, if you would like to use other AI tools that can summarize the text obtained from the second-last step, you can use the following but not limited to:</p> <h4 id="quill-bot">Quill Bot</h4> <p>Quilllbot is a summarizing tool that gives you creative freedom with how much or little of your writing you need to summarize or paraphrase. It can be used within the Quillbot platform or integrated into Word and Google Docs. Quilllbot uses machine learning to scan text, suggesting synonyms, and checking for plagiarism. This is helpful when you’re writing content for a client or if you are writing college-level essays.</p> <h4 id="genei">Genei</h4> <p>Genei is an artificial intelligence summary tool that can extract keywords from any long article or research paper and summarize it into easily digestible content. It’s used by college students when conducting research, as well as people who write longer content and aren’t quite sure what to emphasize with a bullet list in their own summaries. Genei is trusted by thought leaders and experts such as Berkeley university, Harvard, Oxford, Stanford and more.</p> <h4 id="study-crumb">Study Crumb</h4> <p>Study Crumb is an AI tool specifically designed for students who need help writing academic works. From book reports to term papers, Study Crumb lets you order essays and different reports based on what you need. The AI program matches you with a writer who’s an expert at the writing you need to be completed.</p> <h4 id="jasper">Jasper</h4> <p>Jasper is an extremely useful writing tool, because it provides a large number of features for writers. For example, Jasper has a text summarizer that allows you to pick out the most important lines of text.</p> <h4 id="paraphraserio">Paraphraser.io</h4> <p>Paraphraser.io strives to provide high-quality paraphrasing, with a human feel, despite being an AI tool. The company’s program runs faster than other AI-based summary generators because of its grammatically correct and more readable content.</p> <p>and many many many more…</p> <h2 id="summarizing-youtube-videos-using-websites">Summarizing YouTube Videos using Websites</h2> <p>The article already discussed one extension that allows the user to summarize YouTube Videos. The implications for such summarization can be extended to student learning process, teachers and researchers for expanding their knowledge base, and Content Creators.</p> <p>This article presents three more extensions that can be used to summarize the YouTube videos among many that are available.</p> <h4 id="glasp">Glasp</h4> <p>Glasp extension is extremely useful in summarizing any video. It uses AI technology and advanced machine learning techniques first to generate a transcript, which it then uses to create a summary that includes certain phrases from the Youtube video.</p> <h4 id="glairty">Glairty</h4> <p>Introducing Glairty, a Chrome extension designed to enhance your Youtube experience. This innovative tool automatically generates video summaries as soon as you start playing a video. Additionally, you have the flexibility to tailor the prompts sent to ChatGPT for more personalized video summaries.</p> <p>With Glairty, you are in control. You can easily toggle between automatic and manual summary generation for any video, allowing you to choose the method that suits your preferences. Whether you prefer to utilize your ChatGPT account or OpenAI’s API key, Glairty provides a seamless integration for a customized summarization experience.</p> <h4 id="youtube-digest">YouTube Digest</h4> <p>This extension offers a plethora of impressive features, making it a standout option among the available choices. One notable feature is its ability to summarize videos in various formats.</p> <p>Users can obtain a summarized version of the video in bulleted form, as well as in the form of articles, paragraphs, or sections with headings. Additionally, the extension provides a time-stamped summary, facilitating easier navigation and enabling users to watch specific parts of interest.</p> <p>Furthermore, the summary can be exported to a document or PDF format. Finally, the text-to-speech feature saves time by allowing users to listen to the summaries instead of reading them, similar to podcasts. Notably, this extension is completely free to use.</p>]]></content><author><name></name></author><category term="Text Summarization"/><category term="Video Summarization"/><category term="ChatGPT"/><category term="Whisper Api"/><category term="Python"/><summary type="html"><![CDATA[If you have a lot of videos to transcribe and summarize, using speech-to-text technology can be a time-consuming and tedious process. OpenAI’s Whisper technology has made it easier to transcribe your videos into an audio file, and then use ChatGPT’s advanced summarization algorithm to create concise summaries. Let’s take a look!]]></summary></entry><entry><title type="html">Taking a Deep Dive into YOLOv8</title><link href="https://sander-ali.github.io/blog/2023/Taking-a-Deep-Dive-into-YOLOv8/" rel="alternate" type="text/html" title="Taking a Deep Dive into YOLOv8"/><published>2023-08-03T13:00:00+00:00</published><updated>2023-08-03T13:00:00+00:00</updated><id>https://sander-ali.github.io/blog/2023/Taking-a-Deep-Dive-into-YOLOv8</id><content type="html" xml:base="https://sander-ali.github.io/blog/2023/Taking-a-Deep-Dive-into-YOLOv8/"><![CDATA[<p>Ultralytics unveiled YOLOv8 on January 10, 2023, which has garnered over one million downloads since its debut. This blog post aims to delve into the intricacies of YOLOv8.</p> <h2 id="what-is-yolov8">What is YOLOv8</h2> <p>YOLOv8 is the latest cutting-edge YOLO model, designed for object detection, image classification, and instance segmentation tasks. Developed by Ultralytics, the creators of the influential YOLOv5 model, YOLOv8 incorporates several architectural and developer experience enhancements compared to its predecessor. Currently, YOLOv8 is in active development as Ultralytics continues to introduce new features and address community feedback. It is worth noting that Ultralytics provides long-term support for their models, collaborating with the community to optimize the model’s performance.</p> <h2 id="how-yolo-grew-into-yolov8">HOW YOLO Grew Into YOLOv8</h2> <p>The YOLO (You Only Look Once) series of models has gained significant recognition in the field of computer vision. YOLO’s popularity stems from its impressive accuracy despite having a compact model size. This attribute allows YOLO models to be trained efficiently on a single GPU, making it accessible to a diverse community of developers. Machine learning practitioners can deploy YOLO models at a low cost, either on edge hardware or in the cloud. For a concise overview of the timeline of YOLO models, please refer to my other article.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/YOLOv8_dive_2023-480.webp 480w,/assets/img/blog/YOLOv8_dive_2023-800.webp 800w,/assets/img/blog/YOLOv8_dive_2023-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/blog/YOLOv8_dive_2023.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="why-should-you-use-yolov8">Why Should you use YOLOv8</h2> <p>There are several compelling reasons to consider using YOLOv8 for your next computer vision project:</p> <ol> <li> <p>YOLOv8 offers improved accuracy compared to previous YOLO models.</p> </li> <li> <p>The latest YOLOv8 implementation includes a range of new features, such as a user-friendly CLI and a GitHub repository.</p> </li> <li> <p>YOLOv8 supports various tasks, including object detection, instance segmentation, and image classification.</p> </li> <li> <p>The YOLO community is highly active and supportive, with numerous tutorials, videos, and articles available. You can also find assistance in communities like MLOps Community and DCAI.</p> </li> </ol> <p>It’s worth noting that training YOLOv8 is likely to be faster than other two-stage object detection models. However, there is one limitation to consider: YOLOv8 does not currently support models trained at a resolution of 1280 pixels. Therefore, if you require high-resolution inference, it is not recommended to use YOLOv8.</p> <p>In addition to its performance, YOLOv8 offers significant developer-convenience features. Unlike other models that spread tasks across multiple Python files, YOLOv8 provides a CLI that simplifies the model training process. Moreover, the Python package accompanying YOLOv8 ensures a smoother coding experience compared to previous models.</p> <p>The YOLO community’s expertise and extensive online resources also make it an attractive choice. Many computer vision experts are familiar with YOLO and can provide guidance. Although YOLOv8 is relatively new, there is already a wealth of online guides available to assist users.</p> <h2 id="how-does-yolov8-compare-to-previousmodels">How does YOLOv8 compare to previous models?</h2> <p>The Ultralytics team has recently conducted a benchmark of YOLOv8 using the COCO dataset, yielding impressive results when compared to previous versions of YOLO across all five model sizes. In evaluating the performance of different YOLO lineages and model sizes on the COCO dataset, they used various metrics for the assessment. These include performance (mean average precision or mAP), speed of inference (measured in frames per second or fps), and compute cost (represented by the size of the model in terms of FLOPs and parameters).</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/Dive_YOLOv8_1-480.webp 480w,/assets/img/blog/Dive_YOLOv8_1-800.webp 800w,/assets/img/blog/Dive_YOLOv8_1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/blog/Dive_YOLOv8_1.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>In the object detection comparison of the five model sizes, the YOLOv8m model achieved an mAP of 50.2 % on the COCO dataset, while the largest model, YOLOv8x, achieved a higher mAP of 53.9 % with more than double the number of parameters.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/Dive_YOLOv8_2-480.webp 480w,/assets/img/blog/Dive_YOLOv8_2-800.webp 800w,/assets/img/blog/Dive_YOLOv8_2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/blog/Dive_YOLOv8_2.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Overall, YOLOv8 demonstrates exceptional accuracy and performance, positioning it as a compelling choice for your next computer vision project. Whether you are considering implementing object detection in a commercial product or exploring the latest advancements in computer vision, YOLOv8 stands as a cutting-edge model worth considering. For a brief tutorial of YOLOv8 by Ultralytics, we invite you to check out their colab tutorial [https://medium.com/r/?url=https%3A%2F%2Fcolab.research.google.com%2Fgithub%2Fultralytics%2Fultralytics%2Fblob%2Fmain%2Fexamples%2Ftutorial.ipynb].</p> <h2 id="yolov8-architecture-a-deepdive">YOLOv8 Architecture: A Deep Dive</h2> <p>The layout presented by RangeKing on GitHub [https://medium.com/r/?url=https%3A%2F%2Fgithub.com%2Fultralytics%2Fultralytics%2Fissues%2F189] is an excellent visualization of the architecture. Although YOLOv8 does not currently have a published paper, which limits our direct understanding of the research methodology and ablation studies conducted during its development, we can still analyze the code and repository to gather information about the model. By documenting the distinctions between YOLOv8 and other architectures, we can gain insights into its unique features.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/dive_YOLOv8_3-480.webp 480w,/assets/img/blog/dive_YOLOv8_3-800.webp 800w,/assets/img/blog/dive_YOLOv8_3-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/blog/dive_YOLOv8_3.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="new-convolutions-inyolov8">New convolutions in YOLOv8</h2> <p>According to the introductory post from Ultralytics, the YOLOv8 architecture has undergone a series of updates and new modifications. Here are the key changes:</p> <blockquote> <blockquote> <p>The backbone of the system has been altered by replacing C3 [https://github.com/ultralytics/yolov5/blob/cdd804d39ff84b413bde36a84006f51769b6043b/models/common.py#L157] with C2f [https://github.com/ultralytics/ultralytics/blob/dba3f178849692a13f3c43e81572255b1ece7da9/ultralytics/nn/modules.py#L196]. In the stem, the first 6x6 convolution has been replaced with a 3x3 convolution. In C2f, the outputs from the Bottleneck (which consists of two 3x3 convolutions with residual connections) are combined, whereas in C3, only the output from the last Bottleneck was utilized.</p> </blockquote> </blockquote> <blockquote> <blockquote> <p>Two convolutions (#10 and #14 in the YOLOv5 configuration) have been eliminated.</p> </blockquote> </blockquote> <blockquote> <blockquote> <p>The Bottleneck in YOLOv8 remains the same as in YOLOv5, except for the change in the kernel size of the first convolution from 1x1 to 3x3. This adjustment indicates a shift towards the ResNet block defined in 2015.</p> </blockquote> </blockquote> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/dive_YOLOv8_4-480.webp 480w,/assets/img/blog/dive_YOLOv8_4-800.webp 800w,/assets/img/blog/dive_YOLOv8_4-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/blog/dive_YOLOv8_4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="anchor-free-detection">Anchor Free Detection</h2> <p>YOLOv8 is a model that does not rely on anchor boxes. Instead, it directly predicts the center of an object without considering the offset from a pre-defined anchor box.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/dive_YOLOv8_5-480.webp 480w,/assets/img/blog/dive_YOLOv8_5-800.webp 800w,/assets/img/blog/dive_YOLOv8_5-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/blog/dive_YOLOv8_5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="anchor-boxes">Anchor Boxes</h2> <p>In previous versions of YOLO, anchor boxes posed a challenge as they may not accurately represent the distribution of object boxes in a custom dataset, but rather the distribution of object boxes in a general benchmark dataset. </p> <p>By adopting an anchor-free approach, the number of box predictions is reduced, resulting in faster Non-Maximum Suppression (NMS). NMS is a complex post-processing step that filters through candidate detections after the inference process.</p> <h2 id="closing-the-mosaic-augmentation">Closing the Mosaic Augmentation</h2> <p>Deep learning research often prioritizes the development of model architecture; however, in the case of YOLOv5 and YOLOv8, the training routine plays a crucial role in their success. During training, YOLOv8 employs online augmentation, which involves presenting the model with slightly different variations of the provided images at each epoch. One of these augmentations is known as mosaic augmentation, where four images are stitched together. This forces the model to learn about objects in new locations, partial occlusion, and against varying surrounding pixels.</p> <p>Nevertheless, it has been empirically observed that if mosaic augmentation is applied throughout the entire training routine, it can lead to a decline in performance. Thus, it is advisable to disable this augmentation for the last ten training epochs. This modification exemplifies the meticulous attention given to YOLO modeling over time in both the YOLOv5 repository and the YOLOv8 research.</p> <h2 id="implementing-yolov8">Implementing YOLOv8</h2> <p>Let’s examine the utilization and implementation of YOLOv8 in your workflows. YOLOv8 includes pre-trained models that can be readily employed in your computer vision projects to enhance model performance. These models consist of instance segmentation models trained on the COCO segmentation dataset with an image resolution of 640, image classification models pre-trained on the ImageNet dataset with an image resolution of 224, and object detection models trained on the COCO detection dataset with an image resolution of 640.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/dive_YOLOv8_6-480.webp 480w,/assets/img/blog/dive_YOLOv8_6-800.webp 800w,/assets/img/blog/dive_YOLOv8_6-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/blog/dive_YOLOv8_6.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="how-do-i-use-the-yolov8cli">How do I use the YOLOv8 CLI?</h2> <p>YOLOv8 can be accessed easily via the CLI and used on any type of dataset.</p> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">!</span>yolo <span class="nv">task</span><span class="o">=</span>detect <span class="se">\ </span><span class="nv">mode</span><span class="o">=</span>predict <span class="se">\ </span><span class="nv">model</span><span class="o">=</span>yolov8n.pt <span class="se">\ </span><span class="nb">source</span><span class="o">=</span><span class="s2">"image.jpg"</span>
</code></pre></div></div> <p>To use it simply insert the following commands:</p> <ul> <li> <p>task in [detect, classify, segment]</p> </li> <li> <p>mode in [train, predict, val, export]</p> </li> <li> <p>model as an uninitialized .yaml or as a previously trained .pt file</p> </li> </ul> <p>Source as the path/to/data</p> <h2 id="can-i-pip-install-yolov8">Can I pip install YOLOv8?</h2> <p>In addition to the CLI, YOLOv8 is available as a PIP package [https://pypi.org/project/yolov8/], making it suitable for Python environments. While this may present some challenges for local development, it provides the flexibility to seamlessly integrate YOLOv8 into your Python code, opening up a wide range of possibilities.</p> <p>You can clone it from GitHub:</p> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone https://github.com/ultralytics/ultralytics.git
</code></pre></div></div> <p>or pip install from pip:</p> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip <span class="nb">install </span>ultralytics
</code></pre></div></div> <p>After you have installed the package, you can import a model and use it in your choice of python environment:</p> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from ultralytics import YOLO 

<span class="c"># Load a model</span>
model <span class="o">=</span> YOLO<span class="o">(</span><span class="s2">"yolov8n.pt"</span><span class="o">)</span>  <span class="c"># load a pretrained model</span>

<span class="c"># Use the model</span>
results <span class="o">=</span> model.train<span class="o">(</span><span class="nv">data</span><span class="o">=</span><span class="s2">"coco128.yaml"</span>, <span class="nv">epochs</span><span class="o">=</span>5<span class="o">)</span>  <span class="c"># train the model</span>
results <span class="o">=</span> model.val<span class="o">()</span>  <span class="c"># evaluate model performance on the validation data set</span>
results <span class="o">=</span> model<span class="o">(</span><span class="s2">"https://ultralytics.com/images/cat.jpg"</span><span class="o">)</span>  <span class="c"># predict on an image</span>
success <span class="o">=</span> YOLO<span class="o">(</span><span class="s2">"yolov8n.pt"</span><span class="o">)</span>.export<span class="o">(</span><span class="nv">format</span><span class="o">=</span><span class="s2">"onnx"</span><span class="o">)</span>  <span class="c"># export a model to ONNX</span>
</code></pre></div></div> <p>You can also access YOLO models via TensorFlow 2, Keras API, Ultralytics Google Colab Notebook [https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb], and OpenCV [https://opencv-tutorial.readthedocs.io/en/latest/yolo/yolo.html].</p> <h2 id="what-is-the-annotation-format-of-yolov8">What is the Annotation Format of YOLOv8?</h2> <p>YOLOv8 utilizes a straightforward annotation format that aligns with the YOLOv5 PyTorch TXT annotation format. This format is a modified version of the Darknet annotation format.</p> <p>For each image sample, there is a corresponding .txt file that contains one line per bounding box. The format for each row is presented as follows:</p> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>class_id center_x center_y width height
</code></pre></div></div> <p>The coordinates are normalized from 0 to 1 and each field is space delimited. An example of annotation format is shown below:</p> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1:  1 0.317 0.30354206008 0.114 0.173819742489
2:  1 0.694 0.33726094420 0.156 0.23605150214
3:  1 0.395 0.32257467811 0.13 0.195278969957
</code></pre></div></div> <p>The model uses the information concerning image location along with class ids and labels from data.yaml folder. The structure of the folder is shown below:</p> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>train: ../train/images
<span class="nb">test</span>: ../test/images
val: ../valid/images

nc: 5
names: <span class="o">[</span><span class="s1">'fish'</span>, <span class="s1">'cat'</span>, <span class="s1">'person'</span>, <span class="s1">'dog'</span>, <span class="s1">'shark'</span><span class="o">]</span>
</code></pre></div></div> <h2 id="conclusion">Conclusion</h2> <p>To begin implementing YOLOv8 for your own specific use case, please refer to the article/blog [https://learnopencv.com/train-yolov8-on-custom-dataset/] on training YOLOv8 with a custom dataset.</p> <p>For additional insights and ideas, you can explore Github repositories [https://github.com/keremberke/awesome-yolov8-models], where you will find various YOLOv8 models, datasets, and sources of inspiration.</p> <p>If you are a practitioner who is actively deploying their model and employing active learning strategies to continuously enhance its performance, many articles/blogs introduce a pathway that allows you to deploy your YOLOv8 model [https://pub.towardsai.net/step-by-step-guide-on-deploying-yolo-model-on-fast-api-fcc6b60f5c26]. This will enable you to utilize our inference engines and benefit from label assist functionality on your dataset.</p> <p>I wish you success in your training endeavors and productive inference sessions!</p>]]></content><author><name></name></author><category term="YOLOv8"/><category term="Python"/><category term="Github"/><category term="Object Detection"/><category term="Object Segmentation"/><summary type="html"><![CDATA[Details on YOLOv8]]></summary></entry><entry><title type="html">Brief Timeline of YOLO models</title><link href="https://sander-ali.github.io/blog/2023/Brief-Timeline-of-YOLO-models-from-v1-to-v8/" rel="alternate" type="text/html" title="Brief Timeline of YOLO models"/><published>2023-08-01T13:00:00+00:00</published><updated>2023-08-01T13:00:00+00:00</updated><id>https://sander-ali.github.io/blog/2023/Brief-Timeline-of-YOLO-models-from-v1-to-v8</id><content type="html" xml:base="https://sander-ali.github.io/blog/2023/Brief-Timeline-of-YOLO-models-from-v1-to-v8/"><![CDATA[<p>The YOLO framework has undergone several iterations since its initial release in 2015 by Joseph Redmon and a team of collaborators (https://arxiv.org/pdf/1506.02640.pdf). It revolutionized real-time object detection capabilities. Redmon and Ali Farhadi subsequently developed YOLO V2 in 2016 (https://arxiv.org/pdf/1612.08242.pdf) and YOLO V3 in 2018 (https://arxiv.org/pdf/1804.02767.pdf), introducing advancements such as anchor boxes, the Darknet-19 architecture, and fully convolutional predictions in V2, and the Darknet-53 architecture and multi-scale predictions in V3.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/YOLO_timeline_2023-480.webp 480w,/assets/img/blog/YOLO_timeline_2023-800.webp 800w,/assets/img/blog/YOLO_timeline_2023-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/blog/YOLO_timeline_2023.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>In 2020, Redmon announced his discontinuation of computer vision research due to concerns about military applications. Subsequently, other teams took over the development of the YOLO framework, resulting in more accessible articles. Alexey Bochkovskiy et al. published the V4 paper in 2020 (https://arxiv.org/pdf/2004.10934.pdf), focusing on optimizing network hyperparameters and introducing an IOU-based loss function. Soon after, Ultralytics entered the scene with YOLOv5, which featured an improved algorithm for anchor finding (https://github.com/ultralytics/yolov5). Ultralytics had previously implemented their own version of YOLO V3. The same team behind V4 published YOLOR in 2021 (https://arxiv.org/pdf/2105.04206.pdf), emphasizing multi-task learning for classification, detection, and pose estimation. Later in 2021, Megvii Technology developed YOLOX, reintroducing an anchor-free process (https://arxiv.org/pdf/2107.08430.pdf).</p> <p>Interestingly, V7 was released in 2022 before V6 by the team responsible for V4. V7 focused on small optimizations, while V6, introduced by Meituan in 2022 (https://arxiv.org/pdf/2209.02976.pdf), emphasized model compression strategies such as quantization and distillation. Alibaba developed DAMO-YOLO later that year using neural architecture search (https://arxiv.org/pdf/2211.15444.pdf). Ultralytics improved upon the framework with the release of YOLOv8 earlier this year (https://github.com/ultralytics/ultralytics).</p> <p>Meanwhile other teams have also created models labeled as “YOLO,” the discussion on other YOLO models and further details can be accessed in: (https://arxiv.org/pdf/2304.00501.pdf).</p> <p>In summary, the YOLO framework has evolved through multiple iterations, addressing limitations and enhancing performance. This blog provides a very brief timeline of the development from original YOLO v1 to the latest YOLO v8, highlighting the key innovations, differences, and improvements made. Each iteration of the YOLO framework has brought various enhancements, ranging from network design to input resolution scaling.</p>]]></content><author><name></name></author><category term="Artificial Intelligence"/><category term="Object Detection"/><category term="YOLO"/><category term="Training"/><category term="YOLO"/><category term="Object Detection"/><category term="Deep Learning"/><category term="Anchor Boxes"/><category term="Network Architecture"/><summary type="html"><![CDATA[Brief Timeline of YOLO models from v1 to v8]]></summary></entry><entry><title type="html">Combating Urban flooding with Internet of Things and Artificial Intelligence</title><link href="https://sander-ali.github.io/blog/2020/Combating-Urban-flooding-with-Internet-of-Things-and-Artificial-Intelligence/" rel="alternate" type="text/html" title="Combating Urban flooding with Internet of Things and Artificial Intelligence"/><published>2020-09-05T13:00:00+00:00</published><updated>2020-09-05T13:00:00+00:00</updated><id>https://sander-ali.github.io/blog/2020/Combating-Urban-flooding-with-Internet-of-Things-and-Artificial-Intelligence</id><content type="html" xml:base="https://sander-ali.github.io/blog/2020/Combating-Urban-flooding-with-Internet-of-Things-and-Artificial-Intelligence/"><![CDATA[<p>It’s no denying that we have been witnessing unprecedented leaps in natural disasters across the world, let it be wildfires, COVID pandemic, floods, drought, extreme weather, and so forth. Every country, developed or developing, takes note of it and be ready for the next cycle of occurrence. However, Pakistan is an exception and it is evident by the current situation of urban floods and power outages. It’s a long debate as to why precautions are not being taken or why it lacks coping strategies when it’s apparent that the floods pose a problem every year. This article deviates towards the short/long term solutions for the current situation of urban floods with the help of Artificial Intelligence. I’ll deal with the most pressing concern, i.e. urban floods due to heavy rainfall.</p> <p>Heavy rainfall has caused urban flooding due to which cities like Karachi and Hyderabad have become either a swimming pool or a sewer depending on the area you live in. The worst part is that weather advisory has issued a warning for more thunderstorms while we are experiencing a situation that can best be described by a scene in the Jumanji movie. Well, let’s talk about solutions. There are two approaches one could follow to solve the current situation. The first is to use an already developed system “CENTAUR” acronym for Cost-Effective Neural Technique to Alleviate Urban Flood” which uses artificial intelligence techniques to manage the flow of water in sewers. However, it requires the installation of gates in the sewer to control the flow. It’s a kind of a load balancing mechanism such that the gate can hold the water in sewers for one area if it is not flooded already to its maximum extent and divert it to other parts of the city. Technically, the method uses remote sensors and Fuzzy logic to control the gates. The pilot installation of the CENTAUR system has shown promising results in Toulouse, France, and Coimbra, Portugal. Although it is considered to be a low-cost solution still expecting our governments to spend thousands of dollars is like waiting for a miracle to happen. Moreover, this solution is good for small floods but during larger floods, the solution may tremble a bit. Concerning the current situation, the containment strategies cannot be implemented straight away (we have way surpassed the time for damage control), however, the solution for the next year would be to use smart manholes as adapted by the Japanese and Chinese governments. To be honest, it is a quite rough sketch of the system in realization but it gives a certain understanding of how the system would be.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/urban_flooding_2020-480.webp 480w,/assets/img/blog/urban_flooding_2020-800.webp 800w,/assets/img/blog/urban_flooding_2020-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/blog/urban_flooding_2020.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>The depicted system can monitor the water levels in sewers on a real-time basis. Furthermore, it can also help to monitor the flow of the water, leakage in pipes, or any other anomalies. The system with smart manholes can be implemented with existing infrastructure with a very low-cost in comparison to the former solution. The sensor deployment and its communication will be the main issue in the manholes but the benefit of doing so is that the administration will have all the information needed at their disposal along with some predictions regarding the rise of water-levels in sewers, rainfall, and alert system, accordingly. With the help of AI, massive accumulated data collected from hundreds if not thousands of smart manholes will make it possible to rapidly forecast the water-levels, the areas where sewerage systems need repairing instantly, effective utilization of equipment, and the anomalies in underground wiring and cables. The system initially would take some costs for sure but afterwards, it can reduce the costs by efficiently improving the sewer maintenance system. Finally, in terms of the current situation, the sensors can sense if a cold-blooded animal is floating around in the sewers.</p> <p>In summary, the former solution would provide a kind of long-term solution but it needs infrastructure to be installed before the system could be put at work and the later solution can be implemented straight away with sensors, communication equipment, and battery installations. In both cases, the AI component is crucial as both the system heavily relies on the prediction system and data analytics based on the acquired data. Nevertheless, it will be the government/administration at the end who have to take necessary actions but at least they would be notified two to three hours earlier before the mishap happens. We need to learn how to incorporate and leverage AI techniques to improve the quality of life and at the same time improve the ecosystem. I hope this article could be considered as a cornerstone for adopting technology to solve our societal problems.</p>]]></content><author><name></name></author><category term="Internet of Things"/><category term="Artificial Intelligence"/><category term="Urban Flooding"/><category term="Karachi"/><category term="Pakistan"/><summary type="html"><![CDATA[It’s no denying that we have been witnessing unprecedented leaps in natural disasters across the world, let it be wildfires, COVID pandemic, floods, drought, extreme weather, and so forth. Every country, developed or developing, takes note of it and be ready for the next cycle of occurrence. However, Pakistan is an exception and it is evident by the current situation of urban floods and power outages. It’s a long debate as to why precautions are not being taken or why it lacks coping strategies when it’s apparent that the floods pose a problem every year. This article deviates towards the short/long term solutions for the current situation of urban floods with the help of Artificial Intelligence. I’ll deal with the most pressing concern, i.e. urban floods due to heavy rainfall.]]></summary></entry><entry><title type="html">Where do Pakistan stand in AI race?</title><link href="https://sander-ali.github.io/blog/2020/Where-do-Pakistan-stand-in-AI-race/" rel="alternate" type="text/html" title="Where do Pakistan stand in AI race?"/><published>2020-08-18T13:00:00+00:00</published><updated>2020-08-18T13:00:00+00:00</updated><id>https://sander-ali.github.io/blog/2020/Where-do-Pakistan-stand-in-AI-race</id><content type="html" xml:base="https://sander-ali.github.io/blog/2020/Where-do-Pakistan-stand-in-AI-race/"><![CDATA[<p>The emergence of Artificial Intelligence (AI) has left a positive influence on the world. Let it be the complex task of image analysis, text translation, speech recognition, and so forth. The aspect which makes AI relatable and interesting is its proof of work which is mostly not observed in the field of Cloud/Edge computing and Blockchain. A wide range of industries have adopted and transformed themselves with the use of AI, let it be Amazon, Netflix, Google, Microsoft, and many more. I can write about the feats which AI has achieved in recent years from beating humans in games to autonomous self-driving cars but that’s not the point of this article. This column aims to let people know where we as a nation stand in the race of adopting AI technologies and on which of its aspects our government needs to focus. Let’s explore a few areas where AI can help in improving the country’s economy.</p> <h2 id="tourism">Tourism</h2> <p>Have you heard about “AirBnB” [1] or similar applications? If not, then it’s time to make our own. Our northern areas are the hub of tourist spots both for the nationals and foreigners but do the locals get enough benefit? Can’t we just make a list of the local peoples sharing their rooms, food, and utensils for a couple of days in exchange for a reasonable rent? Most of the foreigners when visiting a country like to experience the traditions, culture, and hospitality along with the spot which they want to visit. This will not only propagate a positive image of our country which has been represented by some Instagram travel influencers visiting Pakistan recently but also benefit the locals and in turn the economic cycle (in the form of taxes).</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/PakistanAI_race1-480.webp 480w,/assets/img/blog/PakistanAI_race1-800.webp 800w,/assets/img/blog/PakistanAI_race1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/blog/PakistanAI_race1.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 2 Tourism empowered by AI, Web and Android technologies</figcaption> </figure> <p>Now how can AI help? Considering we have a list of locals wanting to share their place along with some attributes like location, nearby tourist spot, food, availability of internet, room size, and so forth, AI can recommend these residential places to the national and international tourists planning to visit in advance. Now the complexity of the recommendation system varies with the preferences, like availability of windows showing scenic views, sharing of a kitchen, availability of Internet connection, and so forth. There are two main benefits of using AI for tourism in Pakistan. The first is of course the financial benefit which can be observed through tax returns and the second is the sustainability of the ecosystem which is caused due to the continuous construction of hotels and buildings in the beautiful northern areas. We have seen what the constructors have done to the city of Murree in the last 20 years. Now let’s move to the next area which is:</p> <h2 id="farming">Farming</h2> <p>It is known globally that Pakistan is an agricultural country and how many measures have been taken to improve the farming process through the use of technology? Well, this question is enough to let the government think about what has been done so far. I assume that everyone in our country takes the agricultural aspect for granted. With the rising population of our country, it would be a challenge for the farmers to feed the population with the available land. Furthermore, this challenge is stacked by weather conditions, and threats like pests, and weeds. These challenges heavily affect the quantity and quality of food. It has been proven by the analysts at the Swiss Federal Institute of Technology (EPFL) and Penn State that the smart machines can help farmers to overcome some of the challenges with consistency, efficiency, and precision [2]. For instance, EPFL gathered 53k images of unhealthy and healthy plants to construct an autonomous differentiating system between infectious and non-infectious plants. The system developed was quite accurate and differentiated the plants with a precision rate of almost 99%. Similarly, Blue River Technology [3], developed a system with help of AI integrated cameras and robots to drive and intelligently spray through fields such that it only targets the weeds while keeping the crops intact. With a similar stance, a Berlin-based startup developed a system (Plantix) [4], that can find nutrient deficiencies and potential defects in the soil through image analysis.</p> <h2 id="github-flow">GitHub Flow</h2> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/PakistanAI_race2-480.webp 480w,/assets/img/blog/PakistanAI_race2-800.webp 800w,/assets/img/blog/PakistanAI_race2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/blog/PakistanAI_race2.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 3 Plantix app for detecting diseases in Plants [4]</figcaption> </figure> <p>Canadian based SkySquirrel technologies [5] use Drones to perform crop analysis and a Colorado-based company [6] uses AI to evaluate farms, analyze crop sustainability in relation to the weather prediction. Being an agricultural country, shouldn’t we be focusing on and developing such systems while integrating AI? That is the question which the government needs to ask for now as agriculture is one of the sectors which can help retain economic development.</p> <h2 id="healthcare">Healthcare</h2> <p>With the spread of COVID-19 infection, it just not wreaked the economic industry but also tested the healthcare facilities and affected the population with psychological diseases to a great extent. One of the setbacks in terms of handling COVID-19 infection is the scale at which tests were conducted. It has been proven widely that RT-PCR testing has a sensitivity of 91% [7], yet we have solely relied on the PCR testing so far. The world has moved towards the potential triage methods for increasing the scale of tests such as CT-scans, chest X-rays, Antigen tests, and so forth. Considering the lack of experts, AI can be used extensively with medical images to separate the potential COVID-19 patients from normal or bacterial pneumonia affected ones through analyzing CT-Scans and chest X-rays. However, our government had not explored any options other than the RT-PCR. Moreover, with social distancing and isolation, many patients have developed symptoms of stress, depression, anxiety, and so forth. Smartphones and smartwatches have been the primary accessories people wear and use to perform communication. Exploring the capabilities of these wearable devices beyond their communication capabilities not only can help researchers and doctors to identify stress and depression but also help them to analyze their behavior patterns which elicit such stressful conditions [8–10]. Even further, these wearables can also help in finding physical activity anomalies for sick and elders living independently [8].</p> <h2 id="github-flow-1">GitHub Flow</h2> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/PakistanAI_race3-480.webp 480w,/assets/img/blog/PakistanAI_race3-800.webp 800w,/assets/img/blog/PakistanAI_race3-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/blog/PakistanAI_race3.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 4 Healthcare Internet of Things Framework using Wearables [8]</figcaption> </figure> <p>These wearables consisting of galvanic skin response and heart rate variability can be further used for detecting emotions and suicidal intentions as suggested in many of the research studies [9], [11]. Another area that has not been explored with reference to COVID-19 is the identification of viral strains. A recent study from King’s College London [12] suggested that there are six distinct types of strains that can be distinguished and assigned to a cluster of symptoms. The analysis of these strains can also help predict how unwell a patient becomes. Unfortunately, neither our government has focused on analyzing viral strains nor adapted any AI system to cluster a particular patient. So far, I have only discussed the AI techniques in relation to the COVID pandemic but moving past from this AI can be used in a variety of medical applications such as Diabetic retinopathy, health monitoring through wearable as well as infrastructure-based sensors, visual monitoring of patients, and many more.</p> <h2 id="furthermore">Furthermore</h2> <p>I can go on with the applications of AI in stock markets, surveillance industry, Education, personal fitness, entertainment, trade analysis, corruption control, anomaly detection in government processes, and so forth, but it will transform this article in a short book of a sort. I think will limit my applications for now to three domains.</p> <h2 id="references">References</h2> <p>[1] Airbnb.com</p> <p>[2] [https://news.psu.edu/story/429727/2016/10/04/research/artificial-intelligence-could-help-farmers-diagnose-crop-diseases]</p> <p>[3] [http://www.bluerivertechnology.com/]</p> <p>[4] [https://www.eu-startups.com/2019/11/berlin-based-plantix-raises-e6-6-million-in-funding-to-further-improve-its-agricultural-app/]</p> <p>[5] [https://www.f6s.com/skysquirreltechnologiesinc]</p> <p>[6] [https://www.awhere.com/]</p> <p>[7] Xie, X., Zhong, Z., Zhao, W., Zheng, C., Wang, F. and Liu, J., 2020. Chest CT for typical 2019-nCoV pneumonia: relationship to negative RT-PCR testing. Radiology, p.200343.</p> <p>[8] Khowaja, S.A., Prabono, A.G., Setiawan, F., Yahya, B.N. and Lee, S.L., 2018. Contextual activity based Healthcare Internet of Things, Services, and People (HIoTSP): An architectural framework for healthcare monitoring using wearable sensors. Computer Networks, 145, pp.190–206.</p> <p>[9] Prabono, A.G., Setiawan, F., Khowaja, S.A., Yahya, B.N. and Lee, S.L., 2017. ETL Workflow Design for Analyzing Relationship of Empathy and Social Awareness using Socio-Emotion Data. 한국정보과학회 학술발표논문집, pp.257–259.</p> <p>[10] Khowaja, S.A., Prabono, A.G., Setiawan, F., Yahya, B.N. and Lee, S.L., 2018. Towards Anomaly Detection System using Wearable Sensors. Korea DataBase Conference, pp. 1–4.</p> <p>[11] Setiawan, F., Khowaja, S.A., Prabono, A.G., Yahya, B.N. and Lee, S.L., 2018, July. A framework for real time emotion recognition based on human ans using pervasive device. In 2018 IEEE 42nd Annual Computer Software and Applications Conference (COMPSAC) (Vol. 1, pp. 805–806). IEEE.</p> <p>[12] [https://www.weforum.org/agenda/2020/07/covid-19-study-6-strains-severity-kings-college-london/]</p>]]></content><author><name></name></author><category term="AI Race"/><category term="AI in Pakistan"/><category term="Pakistan and AI"/><category term="Ministry of Science and Technology"/><category term="PIAIC"/><summary type="html"><![CDATA[A discussion on where Pakistan resides in terms of AI race.]]></summary></entry></feed>